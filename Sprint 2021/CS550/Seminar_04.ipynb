{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Date: **11 February 2021**\n",
    "<br>\n",
    "\n",
    "\n",
    "Title: **Seminar 4**\n",
    "\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "\n",
    "Bibliography: \n",
    "<br>\n",
    "[1] Jeremy Howard & Sylvain Gugger, Deep Learning for Coders with fastai & PyTorch, O'Reilly Media, Inc., 2020\n",
    "<br>\n",
    "[2] <a href = \"https://course.fast.ai/videos/?lesson=4\">FastAI: Lesson 4</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">The MNIST Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seven steps, are the key to the training of all deep learning models:\n",
    "\n",
    "1. Initialize the **weights**: $w$, $b$.\n",
    "\n",
    "2. For each instance, use these weights to **predict** the values of interes.\n",
    "\n",
    "3. Based on these predictions, **calculate the loss**, i.e how good the model is.\n",
    "\n",
    "4. **Calculate the gradient**, which measures for each weight, how changing that weight would change the loss.\n",
    "\n",
    "5. **Change all the weights** based on that calculation.\n",
    "\n",
    "6. **Go to the step 2**, and repeat the process.\n",
    "\n",
    "7. **Iterate until you decide to stop the training process**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "\n",
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We already have our independent variables `x`—these are the images themselves. \n",
    "\n",
    "\n",
    "- We'll concatenate them all into a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). \n",
    "\n",
    "\n",
    "- We can do this using `view`, which is a PyTorch method that changes the shape of a tensor without changing its contents. `-1` is a special parameter to `view` that means \"make this axis as big as necessary to fit all the data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need a label for each image. \n",
    "\n",
    "\n",
    "- We'll use `1` for 3s and `0` for 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `Dataset` in PyTorch is required to return a tuple of `(x,y)` when indexed. \n",
    "\n",
    "\n",
    "- Python provides a `zip` function which, when combined with `list`, provides a simple way to get this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = list(zip(train_x,train_y))\n",
    "x,y = dset[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we need an (initially random) weight for every pixel (this is the *initialize* step in our seven-step process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function `weights*pixels` won't be flexible enough — it is always equal to $0$ when the pixels are equal to 0 (i.e.,its *intercept* is 0). \n",
    "\n",
    "\n",
    "- You might remember from high school math that the formula for a line is `y=w*x+b`; we still need the `b`. \n",
    "\n",
    "\n",
    "- We'll initialize it to a random number too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In neural networks, the `w` in the equation `y=w*x+b` is called the *weights*, and the `b` is called the *bias*. Together, the weights and bias make up the *parameters*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters**: The _weights_ and _biases_ of a model. The weights are the $w$ in the equation $w*x+b$, and the biases are the $b$ in that equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now calculate a prediction for one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.2330], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x[0]*weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we could use a Python `for` loop to calculate the prediction for each image, that would be very slow. \n",
    "\n",
    "  Because Python loops don't run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions.\n",
    "\n",
    "\n",
    "- In this case, there's an extremely convenient mathematical operation that calculates $\\mathrm{w}*\\mathrm{x}$ for every row of a matrix. It's called **matrix multiplication**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Matrix multiplication\" width=\"400\" caption=\"Matrix multiplication\" src=\"images/S4_Matrix_Mult.svg\" id=\"matmul\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n",
      "Wall time: 23.4 ms\n",
      "Wall time: 971 µs\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dot_prod_1(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        for k in range(mx2.shape[1]):\n",
    "            if verbose: print(f'\\tk = {k}')\n",
    "            for j in range(mx1.shape[1]):\n",
    "                if verbose: print(f'\\t\\tj = {j}')\n",
    "                s = mx1[i, j] + mx2[j, k]\n",
    "            mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "def dot_prod_2(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        for k in range(mx2.shape[1]):\n",
    "            mx3[i, k] = mx1[i, :] @ mx2[:, k]\n",
    "            if verbose: \n",
    "                print(f'\\tk = {mx1[i:]}') \n",
    "                print(f'\\tk = {mx2[:, k]}')\n",
    "            \n",
    "            #mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "def dot_prod_3(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        mx3[i, :] = mx1[i, :] @ mx2\n",
    "        if verbose: \n",
    "            print(f'\\tk = {mx2[:, k]}')\n",
    "            \n",
    "            #mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "A = np.random.randn(100, 1000)\n",
    "B = np.random.randn(1000, 100)\n",
    "\n",
    "%time Z_loop = dot_prod_1(A, B)\n",
    "%time Z_loop = dot_prod_2(A, B)\n",
    "%time Z_loop = dot_prod_3(A, B)\n",
    "%time Z_loop = A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.2330],\n",
       "        [-10.6388],\n",
       "        [-20.8865],\n",
       "        ...,\n",
       "        [-15.9176],\n",
       "        [ -1.6866],\n",
       "        [-11.3568]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(xb): return xb@weights + bias\n",
    "preds = linear1(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first element is the same as we calculated before, as we'd expect. \n",
    "\n",
    "- This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network (the other one is the *activation function*, which we'll see in a moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check our accuracy. \n",
    "\n",
    "- To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0, so our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = (preds>0.0).float() == train_y\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379961133003235"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's see what the change in accuracy is for a small change in one of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0] *= 1.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379961133003235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(train_x)\n",
    "((preds>0.0).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we've seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some **loss function** that represents how good our model is. \n",
    "\n",
    "  That is because the gradients are a measure of how that loss function changes with small tweaks to the weights.\n",
    "\n",
    "\n",
    "- So, we need to choose a loss function. The obvious approach would be to use accuracy, which is our metric, as our loss function as well. \n",
    "\n",
    "  In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy, and then calculate the gradients of each weight with respect to that overall accuracy.\n",
    "\n",
    "\n",
    "- Unfortunately, we have a significant technical problem here. The gradient of a function is its *slope*, or its steepness, which can be defined as *rise over run*—that is, how much the value of the function goes up or down, divided by how much we changed the input. \n",
    "\n",
    "  We can write this in mathematically as: `(y_new - y_old) / (x_new - x_old)`. \n",
    "\n",
    "  This gives us a good approximation of the gradient when `x_new` is very similar to `x_old`, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. The problem is that a small change in weights from `x_old` to `x_new` isn't likely to cause any prediction to change, so `(y_new - y_old)` will almost always be 0. In other words, the gradient is 0 almost everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A very small change in the value of a weight will often not actually change the accuracy at all. \n",
    "\n",
    "  This means it is not useful to use accuracy as a loss function—if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.\n",
    "\n",
    "\n",
    "- In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.\n",
    "\n",
    "\n",
    "- Instead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. \n",
    "\n",
    "  So what does a \"slightly better prediction\" look like, exactly? Well, in this case, it means that if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.\n",
    "\n",
    "\n",
    "- Let's write such a function now. What form does it take?\n",
    "\n",
    "\n",
    "- The loss function receives not the images themselves, but the predictions from the model. \n",
    "\n",
    "  Let's make one argument, `prds`, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor), indexed over the images.\n",
    "\n",
    "\n",
    "- The purpose of the loss function is to measure the difference between predicted values and the true values — that is, the targets (aka labels). \n",
    "\n",
    "  Let's make another argument, `trgts`, with values of 0 or 1 which tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor), indexed over the images.\n",
    "\n",
    "\n",
    "- So, for instance, suppose we had three images which we knew were a 3, a 7, and a 3. \n",
    "\n",
    "  And suppose our model predicted with high confidence (`0.9`) that the first was a 3, with slight confidence (`0.4`) that the second was a 7, and with fair confidence (`0.2`), but incorrectly, that the last was a 7. \n",
    "  \n",
    "  This would mean our loss function would receive these values as its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgts  = tensor([1,0,1])\n",
    "prds   = tensor([0.9, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's a first try at a loss function that measures the distance between `predictions` and `targets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're using a new function, `torch.where(a,b,c)`. \n",
    "\n",
    "\n",
    "- This is the same as running the list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))]`, except it works on tensors, at C/CUDA speed. \n",
    "\n",
    "\n",
    "- In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try it on our `prds` and `trgts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.4000, 0.8000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(trgts==1, 1-prds, prds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident. \n",
    "\n",
    "\n",
    "- In PyTorch, we always assume that a lower value of a loss function is better. \n",
    "\n",
    "\n",
    "- Since we need a scalar for the final loss, `mnist_loss` takes the mean of the previous tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4333)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(prds,trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For instance, if we change our prediction for the one \"false\" target from `0.2` to `0.8` the loss will go down, indicating that this is a better prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(tensor([0.9, 0.4, 0.8]),trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One problem with `mnist_loss` as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! \n",
    "\n",
    "\n",
    "- As it happens, there is a function that does exactly that—let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Sigmoid Function</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `sigmoid` function always outputs a number between 0 and 1. It's defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch defines an accelerated version for us, so we don’t really need our own. \n",
    "\n",
    "\n",
    "- This is an important function in deep learning, since we often want to ensure values are between 0 and 1. \n",
    "\n",
    "\n",
    "- This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEMCAYAAAA/Jfb8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlXUlEQVR4nO3deXyU1dn/8c8FBBISErYQ9h1kU0AiKG51q0vr0qLVqrhhLah1a/urrY+7XbSLffSxKk9RFNz3rdpWrVbUKvsS9jVsIYFA9j3X748JPjEmMECSe2byfb9e85K558xwOcx8c3Luc59j7o6IiMSWVkEXICIijU/hLiISgxTuIiIxSOEuIhKDFO4iIjFI4S4iEoMU7hJzzOwuM1sbdB17mdlMM3t/P22uMLPK5qpJYp/CXaKKmSWY2b1mtsbMSsxsl5nNNbMbajX7A3B0UDXW40bggqCLkJalTdAFiBygR4GTCAXmYiAZGAv03dvA3QuBwkCqq4e75wVdg7Q86rlLtDkP+L27v+7uG9x9sbvPdPd79jaob1jGzG4ysy1mVmxmfzezyWbmZta75vErzKzSzE4ys6U1vxV8bGY9zewEM1toZkVm9r6Z9arz2peb2XIzK6v5O+4zsza1Hv/asIyF3Gtm2WZWaGbPA52a6P2SFkrhLtFmO3CGmXUO9wlm9n1CQzW/B0YDzwH319O0FXAncDVwLNATeAG4B5gGHAf0Bv5U67W/AzwBzAIOB34KXFfzOg25AbgF+DlwJLBgP+1FDpy766Zb1NwIhe4moApYAkwHzgWsVpu7gLW17n8KzKrzOr8DHOhdc/+KmvtjarX5ec2xcbWO3QzsrHX/E+DFOq99I1ACtK25PxN4v9bjW4Bf13nOy0Bl0O+vbrFzU89dooq7fwoMAo4HngLSgFeAN83MGnjaCOA/dY59Xt/LA0tr3c+q+e+SOse6mFnrmvsjgX/XeZ2PgfiaOr/GzJKBXsBndR6a00DtIgdF4S5Rx90r3f0zd/+ju59LqNf9XeCEfT0tjJeudvequs9x94p6XsfqOUadx+r7O/f1mEijUbhLLFhR899uDTy+HDimzrHGmiqZAZxY59gJhIZl1tdt7KGZM1sJDS/VVve+yCHRVEiJKmb2MaETovOAHGAw8BtgD/CvBp72R+AFM/sSeBeYCFxW89ih9qB/C7xlZrcCrwJjCI35/9Hdy/dRz71mtpLQcNE5wKmHWIfI16jnLtHmXeAS4G/AKuBJYA1wrLvvrO8J7v4q8P+AWwmNqV8C3F3zcOmhFOPufwOuAi4HlgEPAn+p9fr1+W/goZq2iwj9VnHPPtqLHDBz19CftDxmdgdwo7t3CboWkaagYRmJeWYWR2j++d+AIkJXuP4ceCTIukSaknruEvNqrhZ9GxgHdAA2AE8TutJVi3VJTFK4i4jEIJ1QFRGJQREx5t61a1fv379/0GWIiESV+fPn73T31Poei4hw79+/P/PmzQu6DBGRqGJmmxp6TMMyIiIxKKxwN7PrzWxezXrVM/fT9mYzyzKzPDN7wszaNUqlIiIStnB77tuA+witW90gMzud0FWApwD9gYHs+0o9ERFpAmGFu7u/6u6vA7v20/RyYIa7Z7j7buBeQiv2iYhIM2rsMfeRhPa13GsxkGZmusRbRKQZNXa4JwG1NwPe++cOdRua2TU14/jzcnJyGrkMEZGWrbHDvZDQbvR77f1zQd2G7j7d3dPdPT01td5pmiIicpAae557BqENiF+suT8a2OHu+xurFxGJae5OblE5WfmlZOeXkV1Qyo78Msb27cjxQxq/gxtWuNcsvNQGaA20NrN4Qpv51l106Wlgppk9Q2iX+v8itDmwiEhMK6+sZuueErbsLmbL7hK27i5h254Stu4pYXteKVn5pZRXVn/jedO+NSi4cCcU0nfWun8pcLeZPUFoC7MR7p7p7u+Z2QOEdsRJILRx8Z3feDURkShUUVVNZm4x63OK2LCzkA07i9m4s4jM3GK255VQXWsdxtatjO7J8fTsGM+YPh3p0TGe7smhW7fkeLp1aEdqh3bEx7Vu+C88BBGxKmR6erpr+QERiRRV1c6GnYWszCpg9Y5C1uwoYE12IZt2FVFR9X+Z2al9HP27JtKvc3v6dkmkb+f29OmUQO/O7Unr0I42rZt2EQAzm+/u6fU9FhFry4iIBKW0oopVWQUs3ZpHxrY8MrblsyqrgLKaIZRWBv26JDK4WxKnjUhjcGoSA1MTGdg1iZT2cQFX3zCFu4i0GO5OZm4x8zftZmHmHhZv2cOK7flf9cZTEuIY2TOZyUf3Y3iPZIb16MCg1KQmGzppSgp3EYlZ1dXOiqx8vlify5cbcpm3aTc7C8sASGzbmiN6d+Tq4wdyRK8URvVKoXenBMws4Kobh8JdRGLKxp1FzFm7k0/X7uSzdbvIK6kAoHenBI4f0pVx/TqR3r8TQ7p1oHWr2Ajy+ijcRSSqlVZU8fm6XXy0KpuPVuewaVcxAD1T4vn2iDSOGdSFCQO70KtjQsCVNi+Fu4hEnT3F5fxz+Q7eX7GDf6/eSUlFFfFxrZg4qCtTjhvA8UNS6d+lfcwMsRwMhbuIRIXdReW8l5HF35Zu5/N1u6isdnqkxHP+uN6cMrwbRw/sEpUnPpuKwl1EIlZJeRX/WJ7Fm4u28fHqHCqrnX5d2vOjEwZy5qjuHN4rpUX3zvdF4S4iEcXdWZC5m5fnb+HtxdspKKuke3I8Vx03gHNG92Rkz2QFehgU7iISEfKKK3hlwRae/TKTtdmFJMS15qzDezBpXC+OHtCFVjE8s6UpKNxFJFDLt+Uz87MNvLFoG2WV1Yzu05EHJh3BWUf0IKmdIupg6Z0TkWZXXe28v2IHM+Zs4IsNucTHteL7R/bm0qP7MrJnStDlxQSFu4g0m7LKKl5fuJXH/72e9TlF9OqYwK/OGsaF6X0jep2WaKRwF5EmV1pRxfNfZvLYx+vJyi9lZM9kHvrhWM4a1b3JV05sqRTuItJkSiuqeOaLTB77eB05BWWMH9CZ319wBMcN7qoZL01M4S4ija6yqpqX52/hvz9Yw/a8UiYO6sLDPxzL0QO7BF1ai6FwF5FG4+68vyKb3767gvU5RYzp05E/XjCaiYO7Bl1ai6NwF5FGsWxrHve+vZwvNuQyMDWR6ZPHcdqINA2/BEThLiKHJLeonN//fRXPz82kU/u23HvuSC4a35c4nSgNlMJdRA5KdbXz7JeZ/P7vqygsq+TKiQO46bQhJMdrSmMkULiLyAFbmZXPL19dysLMPRwzsAt3nzuSoWkdgi5LalG4i0jYSiuqeOiDNUz/93qSE+J48MLRnDeml8bVI5DCXUTCsjBzNz9/eQlrsws5f1xvbjtrOJ0S2wZdljRA4S4i+1RWWcWD/1zD9H+vIy05nqeuGs+JQ1ODLkv2Q+EuIg1avaOAG59fxIrt+VyY3ofbvjtcJ0yjhMJdRL7B3Xnqs4389t2VJLVrw18vS+fUEWlBlyUHQOEuIl+zp7icn720hPdX7OCkw1J54PzRpHZoF3RZcoAU7iLylfmbdnPDcwvJLijl9u+O4Kpj+2smTJRSuIsI7s6Tn27kN39bQY+O8bw8dSKj+3QMuiw5BAp3kRauuLySX766lDcWbePU4Wn88QejSUnQSdNop3AXacEydxVzzax5rNpRwM9PP4xpJw7SRtQxIqyVfcyss5m9ZmZFZrbJzC5uoJ2Z2X1mttXM8szsIzMb2bgli0hj+HzdLs59ZA7b80qZeeV4rjtpsII9hoS7bNsjQDmQBlwCPNpAaF8AXAUcD3QGPgdmNUKdItKInv0ik8kzvqBLUjveuO5YXZQUg/Yb7maWCEwCbnf3QnefA7wJTK6n+QBgjruvd/cqYDYwojELFpGDV1Xt3Pv2cn712lKOG9KVV6+dSP+uiUGXJU0gnJ77UKDK3VfXOrYYqK/n/jww2MyGmlkccDnw3qGXKSKHqqS8imufmc+MORu4YmJ/Zlx+lK42jWHhnFBNAvLqHMsD6lvfczvwCbAKqAI2AyfX96Jmdg1wDUDfvn3DLFdEDsauwjKmPDWPxVv2cMd3R3DVcQOCLkmaWDg990Iguc6xZKCgnrZ3AkcBfYB44G7gQzNrX7ehu09393R3T09N1XifSFPZnFvM+Y99zsqsfB67dJyCvYUIJ9xXA23MbEitY6OBjHrajgZecPct7l7p7jOBTmjcXSQQK7bnM+nRz8gtKueZqydw+sjuQZckzWS/4e7uRcCrwD1mlmhmxwLnUv8smLnABWaWZmatzGwyEAesbcyiRWT/5m7M5QePf04rM16aegzj+nUOuiRpRuFexHQt8ASQDewCprl7hpn1BZYDI9w9E7gf6AYsAhIJhfokd9/TyHWLyD58siaHHz09j54pCTw9ZTy9O31jZFRiXFjh7u65wHn1HM8kdMJ17/1S4Lqam4gE4B8ZWVz/7EIGpiYya8oErejYQmn5AZEY8tbibdz0wiJG9UrhqSuPomN7bYPXUincRWLEG4u2cvMLi0jv15kZV6TTQXPYWzSFu0gMeH3hVm55cRFH9e/ME1ccRWI7fbVbOn0CRKLc3mAfPyAU7O3b6mstCneRqPbOku3c8uIiJgzowhNXHEVC29ZBlyQRItxVIUUkwvwjI4sbn1/IkX07MeOKdAW7fI3CXSQKfbw6h+ufXcjIXik8eaWGYuSbFO4iUWbexlx+PGseg7ol8fSV4zUrRuqlcBeJIsu35XPlzLn0TElg1pTxpLRXsEv9FO4iUWLDziIue+JLktq1YdbVE+iapCtPpWEKd5EokJ1fyuQZX1DtzqwpE+jVMSHokiTCKdxFIlx+aQWXPzmX3KJyZl55FIO7Je3/SdLiKdxFIlhZZRVTZ81nzY4CHrt0HEf07hh0SRIlNH9KJEJVVzs/e2kJn63bxYMXjuaEodqxTMKnnrtIhLr/7yt5a/E2bj1zGN8b2zvociTKKNxFItDs/2zi8Y/Xc+nRffnxCQODLkeikMJdJMJ8uHIHd7yxjJOHdeOus0diZkGXJFFI4S4SQZZvy+f6ZxcyomcyD/9wLG1a6ysqB0efHJEIkZ1fytVPzSUlIY4Zl2tNdjk0+vSIRICS8ip+9PQ89pRU8NLUY0hLjg+6JIlyCneRgIWmPC5mydY8Hr90HCN7pgRdksQADcuIBOyhD9fwztLt3HrGML49snvQ5UiMULiLBOjdpdv58/trmHRkb67RlEdpRAp3kYBkbMvjlhcXM7ZvR379vVGa8iiNSuEuEoCdhWVc8/R8OraP4/HJ44iP0xZ50rh0QlWkmVVUVXPdMwvYWVjGy1Mn0q2DZsZI41O4izSzX7+zgi825PLghaM5vLdmxkjT0LCMSDN6ad5mZn62kSnHDdBiYNKkFO4izWTJlj3c9voyJg7qwi/PHBZ0ORLjFO4izWBXYRlTZ80nNakd/3PxkVozRpqcxtxFmlhlVTU3PL+QnUXlvDJ1Ip0T2wZdkrQAYXUfzKyzmb1mZkVmtsnMLt5H24Fm9raZFZjZTjN7oPHKFYk+f/jHaj5du4v7zhulE6jSbML93fARoBxIAy4BHjWzkXUbmVlb4J/Ah0B3oDcwu3FKFYk+7y3L4rGP13HxhL78IL1P0OVIC7LfcDezRGAScLu7F7r7HOBNYHI9za8Atrn7n9y9yN1L3X1Jo1YsEiXW5xTys5cWM7pPR+48e0TQ5UgLE07PfShQ5e6rax1bDHyj5w4cDWw0s3drhmQ+MrPDG6NQkWhSXF7JtNkLiGtt/OWSI2nXRlegSvMKJ9yTgLw6x/KADvW07Q1cBDwE9ATeAd6oGa75GjO7xszmmdm8nJycA6taJIK5O7e9tozV2QX8+aKx9OqYEHRJ0gKFE+6FQHKdY8lAQT1tS4A57v6uu5cDfwC6AMPrNnT36e6e7u7pqampB1i2SOR65otMXlu4lZtOGcqJQ/XZlmCEE+6rgTZmNqTWsdFARj1tlwDeGIWJRKOlW/K4563lnDA0lZ+cPDjocqQF22+4u3sR8Cpwj5klmtmxwLnArHqazwaONrNTzaw1cBOwE1jReCWLRKa84gqufXY+XZLa8ucLx9CqlZbwleCEOxXyWiAByAaeA6a5e4aZ9TWzQjPrC+Duq4BLgceA3YR+CJxTM0QjErPcnZ+9vJjte0r5n4uP1IVKEriwrlB191zgvHqOZxI64Vr72KuEevoiLcZfP9nAP5fv4PbvjmBcv05BlyOitWVEDtX8Tbu5/72VnDGyO1cd2z/ockQAhbvIIdldVM5Pnl1Aj47x3H/+EdoqTyKGFg4TOUjV1c5PX1rMzsJyXpk2kZSEuKBLEvmKeu4iB+l/P1nPhyuzue07w7UgmEQchbvIQZi/KZcH/r6Ksw7vzmXH9Au6HJFvULiLHKDQOPtCenVM4HeTNM4ukUlj7iIHwN35Wa1x9uR4jbNLZFLPXeQA/PWTDXywMptfnTVM4+wS0RTuImFamBmaz376yDQun9g/6HJE9knhLhKGvOIKrn92Id1T4nng/NEaZ5eIpzF3kf1wd37xyhJ25Jfy0tRjNJ9dooJ67iL78fTnm3gvI4tfnDGMsX21boxEB4W7yD4s25rHr99ZwcnDunH18QOCLkckbAp3kQYUlFZw/bML6JLUlj9eoHF2iS4acxeph7vzq9eWsXl3Cc9fczSdtD67RBn13EXq8cLczby1eBu3nDaUo/p3DrockQOmcBepY1VWAXe+mcFxg7sy7cRBQZcjclAU7iK1FJdXct2zC+gQH8eD2gdVopjG3EVqueONDNblFDJ7ygRSO7QLuhyRg6aeu0iNV+Zv4eX5W/jJyUM4dnDXoMsROSQKdxFgbXYB//X6MsYP6MyNpwwJuhyRQ6ZwlxavpLyK655ZSELb1jx00Vhaa5xdYoDG3KXFu+vNDFbtKOCpq8bTPSU+6HJEGoV67tKivb5wKy/M28y13xrEiUNTgy5HpNEo3KXFWptdyK9eW8pR/Ttxy2lDgy5HpFEp3KVFCo2zLyA+rjUP/XAsbVrrqyCxRWPu0iLtHWefeeVR9EhJCLockUan7oq0OK8u2MIL8zZz3UmD+NZh3YIuR6RJKNylRVmzo4DbXgvNZ7/5VI2zS+xSuEuLUVRWybRnFpDYrjX/o3F2iXEac5cWwd257bWlrK9ZN6ZbsuazS2wLq+tiZp3N7DUzKzKzTWZ2cRjP+dDM3Mz0A0QC9+yXmby+aBs3nzqUiVo3RlqAcIP3EaAcSAPGAO+Y2WJ3z6ivsZldcgCvLdKklmzZw91vLufEoalcd9LgoMsRaRb77bmbWSIwCbjd3QvdfQ7wJjC5gfYpwJ3A/2vMQkUOxp7icqbNXkBqh3Zan11alHCGZYYCVe6+utaxxcDIBtr/BngUyDrE2kQOSXW1c9MLi8gpKOMvlxxJZ+2DKi1IOOGeBOTVOZYHdKjb0MzSgWOBh/f3omZ2jZnNM7N5OTk54dQqckAe/nAtH63K4Y6zRzC6T8egyxFpVuGEeyGQXOdYMlBQ+4CZtQL+Atzo7pX7e1F3n+7u6e6enpqqBZukcX20Kps/f7Ca743txSUT+gZdjkizCyfcVwNtzKz2DgajgbonU5OBdOAFM8sC5tYc32Jmxx9ypSJhytxVzI3PL+KwtA785nuHY6Zxdml59jujxd2LzOxV4B4zu5rQbJlzgYl1muYBPWvd7wN8CYwDNO4izaKkvIqps+fj7jw+eRwJbVsHXZJIIMK9RO9aIAHIBp4Dprl7hpn1NbNCM+vrIVl7b/xfoO9w9/ImqF3ka9yd215fyoqsfP77orH065IYdEkigQlrLrq75wLn1XM8k9AJ1/qesxHQ78PSbJ7+fBOvLtjKTacO4aRhWhBMWjYtriEx4fN1u7jn7eWcOjyNG07WBtciCneJelv3lHDdswvo36U9D144WhcqiaBwlyhXWlHFj2fNo6KymumXpdMhPi7okkQigtZ/kajl7vz85SVkbMvnr5elMyi13tM/Ii2Seu4Stf7y0TreWryNn59+GKcMTwu6HJGIonCXqPSPjCx+//dVnDumJ9NOHBR0OSIRR+EuUWdlVj43v7CI0b1TuH/SEboCVaQeCneJKjkFZUyZOY+k+DY8Pjmd+DhdgSpSH51QlahRWlHFNbPmkVtUzktTj6F7irbKE2mIwl2iwt6ZMQsz9/DYpeMY1Ssl6JJEIpqGZSQqPPjP1by1eBu/OGMYZ4zqHnQ5IhFP4S4R78W5m3now7VcmN6HqScODLockaigcJeI9smaHH712lJOGJrKfd8bpZkxImFSuEvEWrE9n2mzFzC4WxKPXDyWuNb6uIqES98WiUhbdhdzxZNfktSuDU9eeZTWjBE5QAp3iTi7i8q57IkvKSmv4ukp4+mRkhB0SSJRR1MhJaKUlFdx1VNz2bK7hNlTJjA0rUPQJYlEJfXcJWKUV1Zz7TPzWbx5Dw9dNJbxAzoHXZJI1FLPXSJCVbXz05cW869VOfz2+4drLrvIIVLPXQLn7tzxxjLeWryNW88cxg/H9w26JJGop3CXQLk7D/x9Fc98kcnUEwcxVcv3ijQKhbsE6qEP1vLoR+u4eEJffnHGYUGXIxIzFO4SmMc/XseD76/m/HG9ue9cXX0q0pgU7hKIJz/dwG/fXcnZo3ty/6QjaNVKwS7SmDRbRprdE3M2cM/byzljZHf+9IPRtFawizQ6hbs0q79+sp773lnBGSO787DWixFpMvpmSbPZG+xnjlKwizQ19dylybk7D3+4lj/9czXfObwHf75ojIJdpIkp3KVJuTu/e28lj3+8nklH9ub+SYfTRsEu0uQU7tJkqqqdO99cxuz/ZDL56H7cfc5IzYoRaSYKd2kSZZVV3PLCYt5Zup0fnziQW88YpnnsIs0orN+Pzayzmb1mZkVmtsnMLm6g3eVmNt/M8s1si5k9YGb6AdLCFJZVctXMubyzdDu3nTWcX545XMEu0szCHfx8BCgH0oBLgEfNbGQ97doDNwFdgQnAKcDPDr1MiRbZ+aVcNP1z/rM+lz9eMJofnaANrUWCsN9etZklApOAUe5eCMwxszeBycCttdu6+6O17m41s2eAkxqxXolgq3cUcOWTc9ldXM5fL0vnpGHdgi5JpMUKZ8hkKFDl7qtrHVsMnBjGc08AMg6mMIkun67dydRZ84lv25oXf3wMo3qlBF2SSIsWTrgnAXl1juUB+9z/zMyuBNKBqxt4/BrgGoC+fbV+dzR75otN3PlGBgNTE3nyyvH06qg9T0WCFk64FwLJdY4lAwUNPcHMzgN+B5zq7jvra+Pu04HpAOnp6R5OsRJZKququfft5Tz1+SZOHJrKwxePJTk+LuiyRITwwn010MbMhrj7mppjo2lguMXMzgD+F/iOuy9tnDIl0uQWlXPDcwuZs3YnPzp+ALeeOVwLgIlEkP2Gu7sXmdmrwD1mdjUwBjgXmFi3rZmdDDwDfM/dv2zkWiVCLN2Sx9TZ88kpLOOB84/gB+l9gi5JROoIdyrktUACkA08B0xz9wwz62tmhWa2d9D8diAF+FvN8UIze7fxy5agvDhvM5Me+wyAl6ceo2AXiVBhXWDk7rnAefUczyR0wnXvfU17jFHF5ZXc8UYGL8/fwnGDu/LQD8fSObFt0GWJSAN09ajs16qsAq57dgHrcgq54ZQh3HjKEI2vi0Q4hbs0yN2Z/UUmv35nOUnt4pg9ZQLHDu4adFkiEgaFu9Qrp6CMX7yyhA9XZnPC0FT+cMERdOsQH3RZIhImhbt8w3vLsrjttaUUlFVy19kjuOyY/lqqVyTKKNzlK7lF5dz5ZgZvLd7GyJ7JPHfhGIam7fNCZBGJUAp3wd15Z+l27nozg7ySCm45bSjTvjVIW+GJRDGFewu3ObeYO95Yxr9W5XB4rxRmTZnA8B51V5sQkWijcG+hyiqrmDFnAw9/sBYzuP27I7j8mH7a31QkRijcW6CPVmVz91vL2bCziNNGpHHXOSO1kqNIjFG4tyBrdhTw23dX8uHKbAZ2TeSpq8Zz4tDUoMsSkSagcG8BcgrK+PP7q3l+7mbat23NL88cxpXHDqBtGw3BiMQqhXsMyyuuYPon63hizkYqqqqZfHQ/bjhliNaEEWkBFO4xKL+0gqc+3cj/frKe/NJKzhndk5tPG8qArolBlyYizUThHkP2FJfz5KcbeeLTDRSUVnLq8G7cctphjOipqY0iLY3CPQZs3VPCjE828PzcTIrLqzh9ZBo/OXmINqkWacEU7lFs0eY9PPnpBt5esh0Dzh7dk2tOGKiLkERE4R5tSiuqeG9ZFjM/28iizXtIateGy4/pz5TjB2iuuoh8ReEeJdbnFPLcl5m8PH8Lu4srGNA1kbvOHsH56X1Iaqd/RhH5OqVCBMsvreCdJdt5ef4W5m/aTZtWxmkj0rhkQj8mDuqiZXhFpEEK9whTWlHFR6tyeHPxVj5YkU1ZZTWDuyVx65nD+P7YXnRL1oYZIrJ/CvcIUFpRxb9X5/DusizeX7GDgtJKuia15aKj+nDe2F6M6dMRM/XSRSR8CveA5BaV86+V2by/Ygf/Xp1DUXkVKQlxnD6yO+eM7snEQV20QqOIHDSFezOpqnaWbc3jo1U5fLw6m0Wb91DtkJbcjnPG9OLMUd05ZlAXbZAhIo1C4d5E3J11OUX8Z/0uPl27k8/W7SKvpAIzOKJXCtefPIRTh3djVM8UnRgVkUancG8kFVXVrNiez7yNu5m3KZcvN+Sys7AcgJ4p8Xx7RBrHDenKcYO70iWpXcDVikisU7gfBHdny+4Slm7NY9HmPSzavIelW/IoqagCQmF+/JBUJgzozISBXejfpb1OiIpIs1K470d5ZTXrcgpZmZXPiu0FLN+Wz7JteewprgCgbetWjOiZzIVH9SG9fyeO7NuJnrpSVEQCpnCvUVpRxYadRazLKWRtdiFrsgtZnVXAhp1FVFY7AG3btGJoWhJnjurOqF4pjOqZwvAeydr0QkQiTosK97ySCrbsLmZzbjGbdhWTmVvMxl1FbNxZzLa8EjyU4ZhBn07tGZqWxKkj0hjWvQPDeyQzsGuipieKSFSImXAvKqsku6CM7Xkl7MgvJSuvjG17StieV8LWPaVs2V1MQWnl157TsX0c/bskMn5AZ/p3SWRgaiKDuyUxoGsi8XGtA/o/ERE5dFEd7v9amc09by8nO7+UovKqbzyekhBHz44J9EyJZ3z/TvTu1J5enRLo27k9fTq3JyUhLoCqRUSaXljhbmadgRnAt4GdwC/d/dkG2t4M/AJIAF4Bprl7WeOU+3Ud28cxokcy3zoslW4d4unWoR09UuLpXnNr3zaqf3aJiBy0cNPvEaAcSAPGAO+Y2WJ3z6jdyMxOB24FTga2Aa8Bd9cca3Rj+3bikUs6NcVLi4hEtf2eHTSzRGAScLu7F7r7HOBNYHI9zS8HZrh7hrvvBu4FrmjEekVEJAzhTP0YClS5++paxxYDI+tpO7Lmsdrt0sysy8GXKCIiByqccE8C8uocywM6hNF275+/0dbMrjGzeWY2LycnJ5xaRUQkTOGEeyFQd8flZKAgjLZ7//yNtu4+3d3T3T09NTU1nFpFRCRM4YT7aqCNmQ2pdWw0kFFP24yax2q32+Huuw6+RBEROVD7DXd3LwJeBe4xs0QzOxY4F5hVT/OngSlmNsLMOgH/BcxsxHpFRCQM4V5Lfy2heevZwHOE5q5nmFlfMys0s74A7v4e8ADwL2BTze3Oxi9bRET2Jax57u6eC5xXz/FMQidRax/7E/CnxihOREQOjvne1bKCLMIsh1Av/2B0JXTVbKSJ1LogcmtTXQdGdR2YWKyrn7vXOyMlIsL9UJjZPHdPD7qOuiK1Lojc2lTXgVFdB6al1aX1a0VEYpDCXUQkBsVCuE8PuoAGRGpdELm1qa4Do7oOTIuqK+rH3EVE5JtioecuIiJ1KNxFRGKQwl1EJAbFXLib2RAzKzWz2UHXAmBms81su5nlm9lqM7s6AmpqZ2YzzGyTmRWY2UIzOzPougDM7PqapaDLzGxmwLV0NrPXzKyo5r26OMh6amqKmPentgj/TEXcd7C2psqsWNxk9BFgbtBF1PJbYIq7l5nZMOAjM1vo7vMDrKkNsBk4EcgEzgJeNLPD3X1jgHVBaHvG+4DTCa1nFKSwtpdsZpH0/tQWyZ+pSPwO1tYkmRVTPXczuwjYA3wQcClfqdlycO8G4V5zGxRgSbh7kbvf5e4b3b3a3d8GNgDjgqyrprZX3f11INBlog9we8lmEynvT10R/pmKuO/gXk2ZWTET7maWDNwD/DToWuoys7+YWTGwEtgO/C3gkr7GzNIIbacYZI800hzI9pJSR6R9piLxO9jUmRUz4U5oM+4Z7r456ELqcvdrCW01eDyhtfHL9v2M5mNmccAzwFPuvjLoeiLIgWwvKbVE4mcqQr+DTZpZURHuZvaRmXkDtzlmNgY4FXgwkuqq3dbdq2p+te8NTIuEusysFaFNV8qB65uypgOpK0IcyPaSUqO5P1MHojm/g/vTHJkVFSdU3f1b+3rczG4C+gOZZgahXldrMxvh7kcGVVcD2tDE433h1GWhN2oGoZOFZ7l7RVPWFG5dEeSr7SXdfU3NsYa2lxSC+UwdpCb/DobhWzRxZkVFzz0M0wn9Y42puT0GvENoRkFgzKybmV1kZklm1trMTgd+CHwYZF01HgWGA2e7e0nQxexlZm3MLB5oTejDHm9mzd4JOcDtJZtNpLw/DYi4z1QEfwebPrPcPeZuwF3A7AioIxX4mNDZ8HxgKfCjCKirH6EZA6WEhh/23i6JgNru4v9mNOy93RVQLZ2B14EiQtP7Ltb7E12fqUj9Djbw79qomaWFw0REYlCsDMuIiEgtCncRkRikcBcRiUEKdxGRGKRwFxGJQQp3EZEYpHAXEYlBCncRkRj0/wEgFjgxXr3rVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. \n",
    "\n",
    "\n",
    "- It's also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. \n",
    "\n",
    "\n",
    "- Let's update `mnist_loss` to first apply `sigmoid` to the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can be confident our loss function will work, even if the predictions are not between 0 and 1. \n",
    " \n",
    "  All that is required is that a higher prediction corresponds to higher confidence an image is a 3.\n",
    "\n",
    "\n",
    "- Having defined a loss function, now is a good moment to recapitulate why we did this. After all, we already had a metric, which was overall accuracy. So why did we define a loss?\n",
    "\n",
    "\n",
    "- The key difference is that the metric is to drive human understanding and the loss is to drive automated learning. \n",
    "\n",
    "  To drive automated learning, the loss must be a function that has a meaningful derivative. \n",
    "  \n",
    "   It can't have big flat sections and large jumps, but instead must be reasonably smooth. \n",
    "   \n",
    "   This is why we designed a loss function that would respond to small changes in confidence level. \n",
    "   \n",
    "   This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal, and a function that can be optimized using its gradient. \n",
    "   \n",
    "   The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch.\n",
    "\n",
    "\n",
    "- Metrics, on the other hand, are the numbers that we really care about. \n",
    "\n",
    "  These are the values that are printed at the end of each epoch that tell us how our model is really doing. \n",
    "  \n",
    "  It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">SGD and Mini-Batch (Revisited)</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function that is suitable for driving SGD, we can consider some of the details involved in the next phase of the learning process, which is to change or update the weights based on the gradients. This is called an *optimization step*.\n",
    "\n",
    "In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n",
    "\n",
    "So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a *mini-batch*. The number of data items in the mini-batch is called the *batch size*. A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this course.\n",
    "\n",
    "Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\n",
    "\n",
    "As we saw in our discussion of data augmentation in <<chapter_production>>, we get better generalization if we can vary things during training. One simple and effective thing we can vary is what data items we put in each mini-batch. Rather than simply enumerating our dataset in order for every epoch, instead what we normally do is randomly shuffle it on every epoch, before we create mini-batches. PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called `DataLoader`.\n",
    "\n",
    "A `DataLoader` can take any Python collection and turn it into an iterator over many batches, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3, 12,  8, 10,  2]),\n",
       " tensor([ 9,  4,  7, 14,  5]),\n",
       " tensor([ 1, 13,  0,  6, 11])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = range(15)\n",
    "dl = DataLoader(coll, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a model, we don't just want any Python collection, but a collection containing independent and dependent variables (that is, the inputs and targets of the model). A collection that contains tuples of independent and dependent variables is known in PyTorch as a `Dataset`. Here's an example of an extremely simple `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = L(enumerate(string.ascii_lowercase))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we pass a `Dataset` to a `DataLoader` we will get back many batches which are themselves tuples of tensors representing batches of independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n",
       " (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n",
       " (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n",
       " (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n",
       " (tensor([2, 4]), ('c', 'e'))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now ready to write our first training loop for a model using SGD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Putting All Together</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's time to implement the process we saw in <<gradient_descent>>. In code, our process will be implemented something like this for each epoch:\n",
    "\n",
    "```python\n",
    "for x,y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, let's re-initialize our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `DataLoader` can be created from a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll do the same for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a mini-batch of size 4 for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_x[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.6180],\n",
       "        [ 9.0489],\n",
       "        [-2.4524],\n",
       "        [-2.5197]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4616, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds, train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0057), tensor([-0.0355]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's put that all in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0113), tensor([-0.0710]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch, train_y[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But look what happens if we call it twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0170), tensor([-0.1065]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch, train_y[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradients have changed! The reason for this is that `loss.backward` actually *adds* the gradients of `loss` to any gradients that are currently stored. So, we have to set the current gradients to 0 first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.grad.zero_()\n",
    "bias.grad.zero_();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects _in place_. For instance, `bias.zero_()` sets all elements of the tensor `bias` to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step. Here's our basic training loop for an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to check how we're doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds>0.0).float() == train_y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us this function to calculate our validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(batch), train_y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then put the batches together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.549"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That's our starting point. \n",
    "\n",
    "\n",
    "- Let's train for one epoch, and see if the accuracy improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6294"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1.\n",
    "params = weights,bias\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then do a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8129 0.913 0.9443 0.956 0.9624 0.9668 0.9692 0.9707 0.9721 0.9741 0.9746 0.9751 0.976 0.9765 0.9765 0.977 0.9775 0.9775 0.9775 0.9775 "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking good! We're already about at the same accuracy as our \"pixel similarity\" approach, and we've created a general-purpose foundation we can build on. \n",
    "\n",
    "\n",
    "- Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it's called an **optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Creating an Optimizer</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first thing we can do is replace our `linear1` function with PyTorch's `nn.Linear` module. A *module* is an object of a class that inherits from the PyTorch `nn.Module` class. Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model.\n",
    "\n",
    "`nn.Linear` does the same thing as our `init_params` and `linear` together. It contains both the *weights* and *biases* in a single class. Here's how we replicate our model from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PyTorch module knows what parameters it has that can be trained; they are available through the `parameters` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to create an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our optimizer by passing in the model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop can now be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation function doesn't need to change at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3567"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our little training loop in a function, to make things simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.77 0.856 0.9175 0.9355 0.9492 0.9551 0.9634 0.9658 0.9682 0.9697 0.9726 0.9741 0.9746 0.9761 0.9761 0.9775 0.978 0.978 0.9785 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai provides the `SGD` class which, by default, does the same thing as our `BasicOptim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8149 0.854 0.917 0.9351 0.9497 0.9555 0.9629 0.9658 0.9682 0.9697 0.9717 0.9746 0.9751 0.9761 0.977 0.978 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai also provides `Learner.fit`, which we can use instead of `train_model`. To create a `Learner` we first need to create a `DataLoaders`, by passing in our training and validation `DataLoader`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Learner` without using an application (such as `cnn_learner`) we need to pass in all the elements that we've created in this chapter: the `DataLoaders`, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636801</td>\n",
       "      <td>0.503659</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595294</td>\n",
       "      <td>0.167055</td>\n",
       "      <td>0.868008</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216102</td>\n",
       "      <td>0.194360</td>\n",
       "      <td>0.820903</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092935</td>\n",
       "      <td>0.109724</td>\n",
       "      <td>0.909715</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.078979</td>\n",
       "      <td>0.932287</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>0.062828</td>\n",
       "      <td>0.946025</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.052928</td>\n",
       "      <td>0.955348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.962218</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.041868</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.038526</td>\n",
       "      <td>0.967615</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's nothing magic about the PyTorch and fastai classes. They are just convenient pre-packaged pieces that make your life a bit easier! (They also provide a lot of extra functionality we'll be using in future chapters.)\n",
    "\n",
    "With these classes, we can now replace our linear model with a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Adding a Nonlinearity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers—this is what gives us a neural network.\n",
    "\n",
    "Here is the entire definition of a basic neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! All we have in `simple_net` is two linear classifiers with a `max` function between them.\n",
    "\n",
    "Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are bias tensors; that is, parameters that are initially randomly initialized, just like we did in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point about this is that `w1` has 30 output activations (which means that `w2` must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that `30` to anything you like, to make the model more or less complex.\n",
    "\n",
    "That little function `res.max(tensor(0.0))` is called a *rectified linear unit*, also known as *ReLU*. We think we can all agree that *rectified linear unit* sounds pretty fancy and complicated... But actually, there's nothing more to it than `res.max(tensor(0.0))`—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as `F.relu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: There is an enormous amount of jargon in deep learning, including terms like _rectified linear unit_. The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example. The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible. One of the ways that they do that is to introduce jargon. Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn't mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven't seen before, it will almost certainly turn out to be referring to a very simple concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there's no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\n",
    "\n",
    "But if we put a nonlinear function between them, such as `max`, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The `max` function is particularly interesting, because it operates as a simple `if` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for `w1` and `w2` and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the *universal approximation theorem*. The three lines of code that we have here are known as *layers*. The first and third are known as *linear layers*, and the second line of code is known variously as a *nonlinearity*, or *activation function*.\n",
    "\n",
    "Just like in the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` creates a module that will call each of the listed layers or functions in turn.\n",
    "\n",
    "`nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu` function. Most functions that can appear in a model also have identical forms that are modules. Generally, it's just a case of replacing `F` with `nn` and changing the capitalization. When using `nn.Sequential`, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example. \n",
    "\n",
    "Because `nn.Sequential` is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let's try it out! As this is a deeper model, we'll use a lower learning rate and a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.319949</td>\n",
       "      <td>0.414147</td>\n",
       "      <td>0.504416</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149887</td>\n",
       "      <td>0.226945</td>\n",
       "      <td>0.809127</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>0.113654</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.077005</td>\n",
       "      <td>0.940628</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.957311</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.050843</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.044894</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027485</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.967615</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025698</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023194</td>\n",
       "      <td>0.033826</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.032359</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021463</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.973994</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.028313</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>0.026919</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.026321</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.025779</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.024828</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016808</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016330</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015711</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015183</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015023</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.021225</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.021057</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014319</td>\n",
       "      <td>0.020748</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_output\n",
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not showing the 40 lines of output here to save room; the training process is recorded in `learn.recorder`, with the table of output stored in the `values` attribute, so we can plot the accuracy over training as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982826292514801"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have something that is rather magical:\n",
    "\n",
    "1. A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\n",
    "1. A way to find the best set of parameters for any function (stochastic gradient descent)\n",
    "\n",
    "This is why deep learning can do things which seem rather magical, such fantastic things. Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take. It seems too good to be true—surely things should be more difficult and complicated than this? Our recommendation: try it out! We just tried it on the MNIST dataset and you have seen the results. And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Going Deeper</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to stop at just two linear layers. We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers. As you will learn, however, the deeper the model gets, the harder it is to optimize the parameters in practice.\n",
    "\n",
    "We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.\n",
    "\n",
    "That means that we can train the model more quickly, and it will take up less memory. In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice. Eventually, theoretical results were developed which showed why this happens. Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity.\n",
    "\n",
    "Here what happens when we train an 18-layer model using the same approach we saw in <<chapter_intro>>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/193 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 7824, 7536, 13132, 6320, 13792, 8224, 10500, 12632, 708, 8412, 9708, 13612, 8548, 2752, 624, 7848) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-d6c34ac6673f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m learn = cnn_learner(dls, resnet18, pretrained=False,\n\u001b[0;32m      3\u001b[0m                     loss_func=F.cross_entropy, metrics=accuracy)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\callback\\schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[0;32m    110\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[0;32m    111\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;31m# Cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\fastai\\data\\load.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__idxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_idxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# called in context of main process (not workers/subprocesses)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 7824, 7536, 13132, 6320, 13792, 8224, 10500, 12632, 708, 8412, 9708, 13612, 8548, 2752, 624, 7848) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">End of Seminar</h1>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
