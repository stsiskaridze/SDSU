{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Date: **9 February 2021**\n",
    "<br>\n",
    "\n",
    "\n",
    "Title: **Seminar 3**\n",
    "\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "\n",
    "Bibliography: \n",
    "<br>\n",
    "[1] Jeremy Howard & Sylvain Gugger, Deep Learning for Coders with fastai & PyTorch, O'Reilly Media, Inc., 2020\n",
    "<br>\n",
    "[2] <a href = \"https://course.fast.ai/videos/?lesson=4\">FastAI: Lesson 3</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">SGD from Scrath</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Derivatives</h3>\n",
    "\n",
    "- Let $U \\subset \\mathbb{R}$ be a open subset and $f:U \\subset \\mathbb{R} \\to \\mathbb{R} $:\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. Function has a **derivative** at point $x_0 \\in \\mathbb{R}$ if for every $h \\in \\mathbb{R}$ there exists the limit of:\n",
    "$$\\lim_{h \\to 0}\\frac{f(x_0 + h) - f(x_0)}{h}.$$\n",
    "This limit point is called **derivate at point** $x_0$ and denoted by $f'(x_0)$.\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. A function is **differentiable** if it has derivative at any point $x\\in U$.\n",
    "\n",
    "<center><img src=\"images/S4_Derivative.gif\" width=\"550\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Partial Derivatives</h3>\n",
    "\n",
    "- Let $\\overrightarrow{f}:\\mathbb{R}^n \\to \\mathbb{R}^m$ be a vector function between two normed vector spaces.\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. The **partial derivative** of an function $\\overrightarrow{f} = f(x_1, \\dots, x_m$) in the direction $x_i$ at the point ($a_1, \\dots, a_n$) is defined as:\n",
    "\n",
    "  $$\n",
    "\\frac{\\partial f}{\\partial x_i}(a_1, \\ldots, a_n) = \\lim_{h \\to 0}\\frac{f(a_1, \\ldots, a_i+h,\\ldots,a_n) - f(a_1,\\ldots, a_i, \\dots,a_n)}{h}\n",
    "$$\n",
    "\n",
    "  All the variables are fixed except $x_i$. That choice of fixed values determines a function of one variable\n",
    "\n",
    "  $$f_{a_1,\\ldots,a_{i-1},a_{i+1},\\ldots,a_n}(x_i) = f(a_1,\\ldots,a_{i-1},x_i,a_{i+1},\\ldots,a_n)$$\n",
    "\n",
    "  and by definition,\n",
    "\n",
    "  $$\\frac{df_{a_1,\\ldots,a_{i-1},a_{i+1},\\ldots,a_n}}{dx_i}(a_i) = \\frac{\\partial f}{\\partial x_i}(a_1,\\ldots,a_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Jacobian Matrix</h3>\n",
    "\n",
    "- Let $\\overrightarrow{f} = \\{f_1,  ..., f_m\\}:\\mathbb{R}^n \\to \\mathbb{R}^m$ be a function such that each of its partial derivatives exist on $\\mathbb{R}^n$.\n",
    "<br>\n",
    "This function takes a vector $\\overrightarrow{x}\\in \\mathbb{R}^n$ as input and produces the vector $\\overrightarrow{f}(\\overrightarrow{x})\\in \\mathbb{R}^m$ as an output.\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. The Jacobian matrix of $\\overrightarrow{f}$ is defined to be an $m \\times n$ matrix, denoted by $\\mathbf{J}$, whose $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{ \\partial x_j}$:\n",
    "\n",
    "  $$\\mathbf{J} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\overrightarrow{f}}{\\partial x_1} & \\cdots  &  \\frac{\\partial \\overrightarrow{f}}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \n",
    "\\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Gradient</h3>\n",
    "\n",
    "- Let $f :\\mathbb{R}^n \\to \\mathbb{R}$ be a function such that each of its partial derivatives exist on $\\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. The gradient of the function $f$, denoted by $\\nabla f$, is a vector of it's Jacobian matrix:\n",
    "\n",
    "  $$ \\nabla f = \\mathbf{J} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\cdots  &  \\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Geometrical Interpretation of Gradient</h3>\n",
    "\n",
    "- Gradient points to the steepest slope and it's magnitude gives the rate of increase in that direction:\n",
    "\n",
    "  $$\\big(\\nabla f(x)\\big)\\cdot \\mathbf{v} = \\nabla_{\\mathbf v}f(x)$$\n",
    "\n",
    "<center><img src=\"images/S4_Gradient.gif\" width=\"400\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Extrema</h3>\n",
    "\n",
    "- $\\textbf{Definition}$. A funciton $f$ on a metrix space $X$ has a **global**, or **absolute**, **maximum** at point $x\\in X$, if:\n",
    "\n",
    "  $$f(x) \\geq f({x}') \\text{ for any } {x}' \\in X.$$\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. A funciton $f$ on a metrix space $X$ has a **global**, or **absolute**, **minimum** at point $x\\in X$, if:\n",
    "\n",
    "  $$f(x) \\leq f({x}') \\text{ for any } {x}' \\in X.$$\n",
    "  \n",
    "\n",
    "- $\\textbf{Definition}$. A funciton $f$ on a metrix space $X$ has a **local maximum** at point $x\\in X$, if there exists an open neighborhood of $x$, call it $U_x$, such that:\n",
    "\n",
    "  $$f(x) \\geq f({x}') \\text{ for any } {x}' \\in U.$$\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. A funciton $f$ on a metrix space $X$ has a **local minimum** at point $x\\in X$, if there exists an open neighborhood of $x$, call it $U_x$, such that:\n",
    "\n",
    "  $$f(x) \\leq f({x}') \\text{ for any } {x}' \\in U.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Fermat's Theorem</h3>\n",
    "\n",
    "- Let $U$ be an open subset of $\\mathbb{R}^n$ and suppose that function $f: U \\to \\mathbb{R}$ is **differentiable** at point $x \\in U$. \n",
    "\n",
    "\n",
    "- $\\textbf{Fermat's theorem}$. If $f$ has a **local extremum** (**maximum** or **minimum**) in $x \\in U$, then: \n",
    "\n",
    "  $$\\nabla f(x) = 0.$$\n",
    "\n",
    "<center><img src=\"images/S4_Extrema.jpg\" width=\"1500\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Hessian  Matrix</h3>\n",
    "\n",
    "- Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be a function such that all **second partial derivatives** exist on $\\mathbb{R}$.\n",
    "\n",
    "  This function takes a vector $\\overrightarrow{x}\\in \\mathbb{R}^n$ as input and produces a scalar $f(\\overrightarrow{x})\\in \\mathbb{R}$ as an output.\n",
    "\n",
    "\n",
    "- $\\textbf{Definition}$. The Hessian matrix of $f$ is a square $n \\times n$ matrix, denoted by $\\mathbf{H}$, whose $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{ \\partial x_i \\partial x_j}$:\n",
    "\n",
    "  $$\\mathbf{H} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "- If the **Hessian matrix**:\n",
    "  \n",
    "  - is **positive definite** at $x$, then $f$ attains an isolated **local minimum** at $x$\n",
    "\n",
    "  - is **negative definite** at $x$, then $f$ attains an isolated **local maximum** at $x$. \n",
    "\n",
    "  - has **both positive and negative eigenvalues** then $x$ is a **saddle point** for $f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Gradient Descent Method</h3>\n",
    "\n",
    "- **Gradient descent** is:\n",
    "\n",
    "  - a first-order iterative optimization algorithm for finding the local minimum of a differentiable function.\n",
    "\n",
    "  - based on the observation that $f(x)$ decreases **fastest** if one goes in the direction of a **negative gradient.**\n",
    "\n",
    "\n",
    "- It **starts with a guess** $x_0$  for a local minumum of $f$, and consider the **sequence** $x_0, x_1, x_2 ...$  such that:\n",
    "\n",
    "  $$x_{n+1} = x_n  - \\gamma \\nabla f(x_n),$$\n",
    "\n",
    "  where $\\gamma$ is a **step size** and is allowed to change at every iteration.  \n",
    "\n",
    "<center><img src=\"images/S4_Gradient_Descent_2.png\" width=\"600\"alt=\"Example\" /></center>\n",
    "\n",
    "<center><img src=\"images/S4_Gradient_Descent.gif\" width=\"600\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Gradient Descent Algorithm Flow</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seven steps, are the key to the training of all deep learning models:\n",
    "\n",
    "1. Initialize the **weights**: $w$, $b$.\n",
    "\n",
    "2. For each instance, use these weights to **predict** the values of interes.\n",
    "\n",
    "3. Based on these predictions, **calculate the loss**, i.e how good the model is.\n",
    "\n",
    "4. **Calculate the gradient**, which measures for each weight, how changing that weight would change the loss.\n",
    "\n",
    "5. **Change all the weights** based on that calculation.\n",
    "\n",
    "6. **Go to the step 2**, and repeat the process.\n",
    "\n",
    "7. **Iterate until you decide to stop the training process**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Learning Rate</h3>\n",
    "\n",
    "- **Learning rate** $\\alpha$ defines the size of steps we take to reach the minimum.\n",
    "\n",
    "\n",
    "- If learning rate is **very small**, it would take **long time to converge** and become computationally expensive.\n",
    "\n",
    "\n",
    "- If learning rate is **large**, it may **fail to converge** and overshoot the minimum.\n",
    "\n",
    "\n",
    "- Therefore, plot the cost function against different values of $\\alpha$ and pick the value of $\\alpha$ that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges:\n",
    "\n",
    "  <img src=\"images/S4_LR.png\"  width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The most commonly used rates are : $0.001, 0.003, 0.01, 0.03, 0.1, 0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Learning Rate</h3>\n",
    "\n",
    "- Make sure to scale the data if it’s on a very different scales. \n",
    "\n",
    "\n",
    "- If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge:\n",
    "\n",
    "  <img src=\"images/S4_Normalization.png\"  width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Batch Gradient Descent</h3>\n",
    "\n",
    "- **Batch Gradient Descent** is when we **sum up over all examples** on each iteration when performing the updates to the parameters. \n",
    "\n",
    "\n",
    "- Therefore, for each update, we have to sum over all examples.\n",
    "\n",
    "\n",
    "- The main **advantages**:\n",
    "\n",
    "  - We can use **fixed learning rate** during training without worrying about learning rate decay.\n",
    "    \n",
    "  - It has straight trajectory towards the minimum and it is **guaranteed to converge** in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex.\n",
    "    \n",
    "  - It has **unbiased estimate of gradients**. The more the examples, the lower the standard error.\n",
    "\n",
    "\n",
    "- The main **disadvantages**:\n",
    "\n",
    "  - Even though we can use vectorized implementation, it may still be **slow to go over all examples** especially **when we have large datasets**.\n",
    "  \n",
    "  - Each step of learning happens after going over all examples where **some examples may be redundant** and don’t contribute much to the update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Mini-batch Gradient Descent</h3>\n",
    "\n",
    "- Applied machine learning is hardly experiment base and needs many experiments while achieving the stable result. \n",
    "\n",
    "  On the other hand, DL models need \"big\" data for training and full batch processing is almost never possible.\n",
    "\n",
    "  For instance $100000$ examples or even more than $1000000$ can't be feat in to the one batch in GPU memory and iteratively calculation is too slow.\n",
    "\n",
    "\n",
    "- **Mini-batch Gradient Descent** sums up over **lower number of examples** based on the **batch size**. \n",
    "\n",
    "\n",
    "- Therefore, learning happens on each mini-batch of $b$ examples:\n",
    "\n",
    "  - Shuffle the training data set to avoid pre-existing order of examples.\n",
    "    \n",
    "  - Partition the training data set into $b$ mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.\n",
    "\n",
    "\n",
    "- The **batch size** is something we can tune, i.e. it's a **hyperparameter**.\n",
    "\n",
    "  It is usually chosen as **power of** $2$ such as $32$, $64$, $128$, $256$, $512$, etc. \n",
    "  \n",
    "  The reason behind it is because some hardware such as GPUs achieve better run time with common batch sizes such as power of $2$.\n",
    "  \n",
    "\n",
    "- The main **advantages**:\n",
    "\n",
    "  - **Faster than Batch version** because it goes through a lot less examples than Batch (all examples).\n",
    "  \n",
    "  - **Randomly selecting examples will help avoid redundant examples** or examples that are very similar that don’t contribute much to the learning.\n",
    "  \n",
    "  - With batch size $<$ size of training set, it adds noise to the learning process that **helps improving generalization error**.\n",
    "  \n",
    "  - Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.\n",
    "  \n",
    "  \n",
    "- The main **disadvantages**:\n",
    "\n",
    "  - **It won’t converge**. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.\n",
    "  \n",
    "  - Due to the noise, the learning steps **have more oscillations** and **requires adding learning-decay to decrease the learning rate** as we become closer to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"images/S4_Batches.png\"  width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Stochastic Gradient Descent</h3>\n",
    "\n",
    "- Instead of going through all examples, **Stochastic Gradient Descent (SGD)** performs the parameters **update on each example**. \n",
    "\n",
    "\n",
    "- Therefore, **learning happens on every example**:\n",
    "\n",
    "  - Shuffle the training data set to avoid pre-existing order of examples.\n",
    "    \n",
    "  - Partition the training data set into $m$ examples.\n",
    "\n",
    "\n",
    "- It shares most of the advantages and the disadvantages with mini-batch version. \n",
    "\n",
    "  Below are the ones that are specific to SGD:\n",
    "\n",
    "  - It **adds even more noise to the learning process** than mini-batch that **helps improving generalization error**. However, this would **increase the run time**.\n",
    "    \n",
    "  - We can’t utilize vectorization over 1 example and **becomes very slow**. Also, the variance becomes large since we only use 1 example for each learning step.\n",
    "\n",
    "\n",
    "- Below is a graph that shows the gradient descent’s variants and their direction towards the minimum:\n",
    "\n",
    "  <img src=\"images/S4_Comparison.png\"  width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Stepping With a Learning Rate</h3>\n",
    "\n",
    "- From the different point of view, gradients are pretty similar at each epoch and if we stack in **local extrema** or **saddle point**, we can stay there for long:\n",
    "\n",
    "<img src=\"images/S4_Saddle.png\"  width=\"600\" />\n",
    "\n",
    "\n",
    "- On early stages of deep learning, people thought that local optimum was the main problem of optimization. \n",
    "\n",
    "\n",
    "- But for local optima we need that each:\n",
    "\n",
    "  $$\\frac{\\partial C}{\\partial W^l_{i,j}}$$ \n",
    "\n",
    "  direction to be the same which for instance of $100000000$ or even $200000000$ parameters might have a $2^{-100000000}$ or $2^{-200000000}$ probability:\n",
    "  \n",
    "  <img src=\"images/S4_NG.png\"  width=\"800\" />\n",
    "  \n",
    "\n",
    "- Therefor:\n",
    "\n",
    "  - The probability of local optima is low\n",
    "   \n",
    "  - But the probability of saddle points is high\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Stepping With a Learning Rate</h3>\n",
    "\n",
    "- From the different point of view, gradients are pretty similar at each epoch and if we stack in **local extrema** or **saddle point**, we can stay there for long:\n",
    "\n",
    "<img src=\"images/S4_Saddle.png\"  width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Different Optimizers</h3>\n",
    "  \n",
    "  <img src=\"images/S4_Optimizers.gif\"  width=\"500\" />\n",
    "\n",
    "  <img src=\"images/S4_Optimizers2.gif\"  width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">The MNIST Dataset Example</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -Uqq fastbook\n",
    "#import fastbook\n",
    "#fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "\n",
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We already have our independent variables `x`—these are the images themselves. \n",
    "\n",
    "\n",
    "- We'll concatenate them all into a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). \n",
    "\n",
    "\n",
    "- We can do this using `view`, which is a PyTorch method that changes the shape of a tensor without changing its contents. `-1` is a special parameter to `view` that means \"make this axis as big as necessary to fit all the data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need a label for each image. \n",
    "\n",
    "\n",
    "- We'll use `1` for 3s and `0` for 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `Dataset` in PyTorch is required to return a tuple of `(x,y)` when indexed. \n",
    "\n",
    "\n",
    "- Python provides a `zip` function which, when combined with `list`, provides a simple way to get this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = list(zip(train_x,train_y))\n",
    "x,y = dset[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we need an (initially random) weight for every pixel (this is the *initialize* step in our seven-step process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function `weights*pixels` won't be flexible enough — it is always equal to $0$ when the pixels are equal to 0 (i.e.,its *intercept* is 0). \n",
    "\n",
    "\n",
    "- You might remember from high school math that the formula for a line is `y=w*x+b`; we still need the `b`. \n",
    "\n",
    "\n",
    "- We'll initialize it to a random number too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In neural networks, the `w` in the equation `y=w*x+b` is called the *weights*, and the `b` is called the *bias*. Together, the weights and bias make up the *parameters*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters**: The _weights_ and _biases_ of a model. The weights are the $w$ in the equation $w*x+b$, and the biases are the $b$ in that equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now calculate a prediction for one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.2330], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x[0]*weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we could use a Python `for` loop to calculate the prediction for each image, that would be very slow. \n",
    "\n",
    "  Because Python loops don't run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions.\n",
    "\n",
    "\n",
    "- In this case, there's an extremely convenient mathematical operation that calculates $\\mathrm{w}*\\mathrm{x}$ for every row of a matrix. It's called **matrix multiplication**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Matrix multiplication\" width=\"400\" caption=\"Matrix multiplication\" src=\"images/S4_Matrix_Mult.svg\" id=\"matmul\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 s, sys: 0 ns, total: 2.71 s\n",
      "Wall time: 2.71 s\n",
      "CPU times: user 20.1 ms, sys: 0 ns, total: 20.1 ms\n",
      "Wall time: 20.1 ms\n",
      "CPU times: user 3.38 ms, sys: 0 ns, total: 3.38 ms\n",
      "Wall time: 1.47 ms\n",
      "CPU times: user 280 µs, sys: 0 ns, total: 280 µs\n",
      "Wall time: 152 µs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dot_prod_1(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        for k in range(mx2.shape[1]):\n",
    "            if verbose: print(f'\\tk = {k}')\n",
    "            for j in range(mx1.shape[1]):\n",
    "                if verbose: print(f'\\t\\tj = {j}')\n",
    "                s = mx1[i, j] + mx2[j, k]\n",
    "            mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "#მატრიცების ვექტორ-ელემენტ გადამრავლება\n",
    "def dot_prod_2(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        for k in range(mx2.shape[1]):\n",
    "            mx3[i, k] = mx1[i, :] @ mx2[:, k]\n",
    "            if verbose: \n",
    "                print(f'\\tk = {mx1[i:]}') \n",
    "                print(f'\\tk = {mx2[:, k]}')\n",
    "            \n",
    "            #mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "#მატრიცების ვექტორ-ვექტორ გადამრავლება\n",
    "def dot_prod_3(mx1, mx2, verbose = False):\n",
    "    mx3 = np.zeros((mx1.shape[0], mx2.shape[1]))\n",
    "    if verbose: print(f'mx3.shape = {mx3.shape}')\n",
    "    for i in range(mx1.shape[0]):\n",
    "        if verbose: print(f'i = {i}')\n",
    "        mx3[i, :] = mx1[i, :] @ mx2\n",
    "        if verbose: \n",
    "            print(f'\\tk = {mx2[:, k]}')\n",
    "            \n",
    "            #mx3[i, k] = s\n",
    "    \n",
    "    return mx3\n",
    "\n",
    "A = np.random.randn(100, 1000)\n",
    "B = np.random.randn(1000, 100)\n",
    "\n",
    "%time Z_loop = dot_prod_1(A, B)\n",
    "%time Z_loop = dot_prod_2(A, B)\n",
    "%time Z_loop = dot_prod_3(A, B)\n",
    "%time Z_loop = A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.2330],\n",
       "        [-10.6388],\n",
       "        [-20.8865],\n",
       "        ...,\n",
       "        [-15.9176],\n",
       "        [ -1.6866],\n",
       "        [-11.3568]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(xb): return xb@weights + bias\n",
    "preds = linear1(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first element is the same as we calculated before, as we'd expect. \n",
    "\n",
    "- This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network (the other one is the *activation function*, which we'll see in a moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check our accuracy. \n",
    "\n",
    "- To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0, so our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = (preds>0.0).float() == train_y\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379961133003235"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's see what the change in accuracy is for a small change in one of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0] *= 1.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379961133003235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(train_x)\n",
    "((preds>0.0).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we've seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some **loss function** that represents how good our model is. \n",
    "\n",
    "  That is because the gradients are a measure of how that loss function changes with small tweaks to the weights.\n",
    "\n",
    "\n",
    "- So, we need to choose a loss function. The obvious approach would be to use accuracy, which is our metric, as our loss function as well. \n",
    "\n",
    "  In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy, and then calculate the gradients of each weight with respect to that overall accuracy.\n",
    "\n",
    "\n",
    "- Unfortunately, we have a significant technical problem here. The gradient of a function is its *slope*, or its steepness, which can be defined as *rise over run*—that is, how much the value of the function goes up or down, divided by how much we changed the input. \n",
    "\n",
    "  We can write this in mathematically as: `(y_new - y_old) / (x_new - x_old)`. \n",
    "\n",
    "  This gives us a good approximation of the gradient when `x_new` is very similar to `x_old`, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. The problem is that a small change in weights from `x_old` to `x_new` isn't likely to cause any prediction to change, so `(y_new - y_old)` will almost always be 0. In other words, the gradient is 0 almost everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A very small change in the value of a weight will often not actually change the accuracy at all. \n",
    "\n",
    "  This means it is not useful to use accuracy as a loss function—if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.\n",
    "\n",
    "\n",
    "- In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.\n",
    "\n",
    "\n",
    "- Instead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. \n",
    "\n",
    "  So what does a \"slightly better prediction\" look like, exactly? Well, in this case, it means that if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.\n",
    "\n",
    "\n",
    "- Let's write such a function now. What form does it take?\n",
    "\n",
    "\n",
    "- The loss function receives not the images themselves, but the predictions from the model. \n",
    "\n",
    "  Let's make one argument, `prds`, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor), indexed over the images.\n",
    "\n",
    "\n",
    "- The purpose of the loss function is to measure the difference between predicted values and the true values — that is, the targets (aka labels). \n",
    "\n",
    "  Let's make another argument, `trgts`, with values of 0 or 1 which tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor), indexed over the images.\n",
    "\n",
    "\n",
    "- So, for instance, suppose we had three images which we knew were a 3, a 7, and a 3. \n",
    "\n",
    "  And suppose our model predicted with high confidence (`0.9`) that the first was a 3, with slight confidence (`0.4`) that the second was a 7, and with fair confidence (`0.2`), but incorrectly, that the last was a 7. \n",
    "  \n",
    "  This would mean our loss function would receive these values as its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgts  = tensor([1,0,1])\n",
    "prds   = tensor([0.9, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's a first try at a loss function that measures the distance between `predictions` and `targets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're using a new function, `torch.where(a,b,c)`. \n",
    "\n",
    "\n",
    "- This is the same as running the list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))]`, except it works on tensors, at C/CUDA speed. \n",
    "\n",
    "\n",
    "- In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try it on our `prds` and `trgts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.4000, 0.8000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(trgts==1, 1-prds, prds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident. \n",
    "\n",
    "\n",
    "- In PyTorch, we always assume that a lower value of a loss function is better. \n",
    "\n",
    "\n",
    "- Since we need a scalar for the final loss, `mnist_loss` takes the mean of the previous tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4333)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(prds,trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For instance, if we change our prediction for the one \"false\" target from `0.2` to `0.8` the loss will go down, indicating that this is a better prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(tensor([0.9, 0.4, 0.8]),trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One problem with `mnist_loss` as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! \n",
    "\n",
    "\n",
    "- As it happens, there is a function that does exactly that—let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Sigmoid Function</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `sigmoid` function always outputs a number between 0 and 1. It's defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch defines an accelerated version for us, so we don’t really need our own. \n",
    "\n",
    "\n",
    "- This is an important function in deep learning, since we often want to ensure values are between 0 and 1. \n",
    "\n",
    "\n",
    "- This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. \n",
    "\n",
    "\n",
    "- It's also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. \n",
    "\n",
    "\n",
    "- Let's update `mnist_loss` to first apply `sigmoid` to the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can be confident our loss function will work, even if the predictions are not between 0 and 1. \n",
    " \n",
    "  All that is required is that a higher prediction corresponds to higher confidence an image is a 3.\n",
    "\n",
    "\n",
    "- Having defined a loss function, now is a good moment to recapitulate why we did this. After all, we already had a metric, which was overall accuracy. So why did we define a loss?\n",
    "\n",
    "\n",
    "- The key difference is that the metric is to drive human understanding and the loss is to drive automated learning. \n",
    "\n",
    "  To drive automated learning, the loss must be a function that has a meaningful derivative. \n",
    "  \n",
    "   It can't have big flat sections and large jumps, but instead must be reasonably smooth. \n",
    "   \n",
    "   This is why we designed a loss function that would respond to small changes in confidence level. \n",
    "   \n",
    "   This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal, and a function that can be optimized using its gradient. \n",
    "   \n",
    "   The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch.\n",
    "\n",
    "\n",
    "- Metrics, on the other hand, are the numbers that we really care about. \n",
    "\n",
    "  These are the values that are printed at the end of each epoch that tell us how our model is really doing. \n",
    "  \n",
    "  It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">SGD and Mini-Batch (Revisited)</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function that is suitable for driving SGD, we can consider some of the details involved in the next phase of the learning process, which is to change or update the weights based on the gradients. This is called an *optimization step*.\n",
    "\n",
    "In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n",
    "\n",
    "So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a *mini-batch*. The number of data items in the mini-batch is called the *batch size*. A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this course.\n",
    "\n",
    "Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\n",
    "\n",
    "As we saw in our discussion of data augmentation in <<chapter_production>>, we get better generalization if we can vary things during training. One simple and effective thing we can vary is what data items we put in each mini-batch. Rather than simply enumerating our dataset in order for every epoch, instead what we normally do is randomly shuffle it on every epoch, before we create mini-batches. PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called `DataLoader`.\n",
    "\n",
    "A `DataLoader` can take any Python collection and turn it into an iterator over many batches, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3, 12,  8, 10,  2]),\n",
       " tensor([ 9,  4,  7, 14,  5]),\n",
       " tensor([ 1, 13,  0,  6, 11])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = range(15)\n",
    "dl = DataLoader(coll, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a model, we don't just want any Python collection, but a collection containing independent and dependent variables (that is, the inputs and targets of the model). A collection that contains tuples of independent and dependent variables is known in PyTorch as a `Dataset`. Here's an example of an extremely simple `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = L(enumerate(string.ascii_lowercase))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we pass a `Dataset` to a `DataLoader` we will get back many batches which are themselves tuples of tensors representing batches of independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n",
       " (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n",
       " (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n",
       " (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n",
       " (tensor([2, 4]), ('c', 'e'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now ready to write our first training loop for a model using SGD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Putting All Together</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's time to implement the process we saw in <<gradient_descent>>. In code, our process will be implemented something like this for each epoch:\n",
    "\n",
    "```python\n",
    "for x,y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, let's re-initialize our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `DataLoader` can be created from a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll do the same for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a mini-batch of size 4 for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_x[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.6180],\n",
       "        [ 9.0489],\n",
       "        [-2.4524],\n",
       "        [-2.5197]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4616, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds, train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0057), tensor([-0.0355]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's put that all in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0113), tensor([-0.0710]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch, train_y[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But look what happens if we call it twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0170), tensor([-0.1065]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch, train_y[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradients have changed! The reason for this is that `loss.backward` actually *adds* the gradients of `loss` to any gradients that are currently stored. So, we have to set the current gradients to 0 first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.grad.zero_()\n",
    "bias.grad.zero_();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects _in place_. For instance, `bias.zero_()` sets all elements of the tensor `bias` to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step. Here's our basic training loop for an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to check how we're doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds>0.0).float() == train_y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us this function to calculate our validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(batch), train_y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then put the batches together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5483"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That's our starting point. \n",
    "\n",
    "\n",
    "- Let's train for one epoch, and see if the accuracy improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6293"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1.\n",
    "params = weights,bias\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then do a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8129 0.913 0.9442 0.956 0.9623 0.9667 0.9691 0.9706 0.9721 0.974 0.9745 0.975 0.976 0.9765 0.9765 0.977 0.9775 0.9775 0.9775 0.9775 "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking good! We're already about at the same accuracy as our \"pixel similarity\" approach, and we've created a general-purpose foundation we can build on. \n",
    "\n",
    "\n",
    "- Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it's called an **optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Creating an Optimizer</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first thing we can do is replace our `linear1` function with PyTorch's `nn.Linear` module. A *module* is an object of a class that inherits from the PyTorch `nn.Module` class. Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model.\n",
    "\n",
    "`nn.Linear` does the same thing as our `init_params` and `linear` together. It contains both the *weights* and *biases* in a single class. Here's how we replicate our model from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PyTorch module knows what parameters it has that can be trained; they are available through the `parameters` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to create an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our optimizer by passing in the model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop can now be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation function doesn't need to change at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3565"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our little training loop in a function, to make things simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.77 0.8559 0.9175 0.9355 0.9492 0.9551 0.9634 0.9658 0.9682 0.9697 0.9726 0.9741 0.9746 0.9761 0.9761 0.9775 0.978 0.978 0.9785 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai provides the `SGD` class which, by default, does the same thing as our `BasicOptim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8149 0.854 0.917 0.935 0.9497 0.9555 0.9629 0.9658 0.9682 0.9697 0.9717 0.9746 0.9751 0.9761 0.977 0.978 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai also provides `Learner.fit`, which we can use instead of `train_model`. To create a `Learner` we first need to create a `DataLoaders`, by passing in our training and validation `DataLoader`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Learner` without using an application (such as `cnn_learner`) we need to pass in all the elements that we've created in this chapter: the `DataLoaders`, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636801</td>\n",
       "      <td>0.503659</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595294</td>\n",
       "      <td>0.167055</td>\n",
       "      <td>0.868008</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216102</td>\n",
       "      <td>0.194360</td>\n",
       "      <td>0.820903</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092935</td>\n",
       "      <td>0.109724</td>\n",
       "      <td>0.909715</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.078979</td>\n",
       "      <td>0.932287</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>0.062828</td>\n",
       "      <td>0.946025</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.052928</td>\n",
       "      <td>0.955348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.962218</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.041868</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.038526</td>\n",
       "      <td>0.967615</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's nothing magic about the PyTorch and fastai classes. They are just convenient pre-packaged pieces that make your life a bit easier! (They also provide a lot of extra functionality we'll be using in future chapters.)\n",
    "\n",
    "With these classes, we can now replace our linear model with a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Adding a Nonlinearity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers—this is what gives us a neural network.\n",
    "\n",
    "Here is the entire definition of a basic neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! All we have in `simple_net` is two linear classifiers with a `max` function between them.\n",
    "\n",
    "Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are bias tensors; that is, parameters that are initially randomly initialized, just like we did in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point about this is that `w1` has 30 output activations (which means that `w2` must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that `30` to anything you like, to make the model more or less complex.\n",
    "\n",
    "That little function `res.max(tensor(0.0))` is called a *rectified linear unit*, also known as *ReLU*. We think we can all agree that *rectified linear unit* sounds pretty fancy and complicated... But actually, there's nothing more to it than `res.max(tensor(0.0))`—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as `F.relu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: There is an enormous amount of jargon in deep learning, including terms like _rectified linear unit_. The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example. The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible. One of the ways that they do that is to introduce jargon. Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn't mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven't seen before, it will almost certainly turn out to be referring to a very simple concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there's no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\n",
    "\n",
    "But if we put a nonlinear function between them, such as `max`, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The `max` function is particularly interesting, because it operates as a simple `if` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for `w1` and `w2` and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the *universal approximation theorem*. The three lines of code that we have here are known as *layers*. The first and third are known as *linear layers*, and the second line of code is known variously as a *nonlinearity*, or *activation function*.\n",
    "\n",
    "Just like in the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` creates a module that will call each of the listed layers or functions in turn.\n",
    "\n",
    "`nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu` function. Most functions that can appear in a model also have identical forms that are modules. Generally, it's just a case of replacing `F` with `nn` and changing the capitalization. When using `nn.Sequential`, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example. \n",
    "\n",
    "Because `nn.Sequential` is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let's try it out! As this is a deeper model, we'll use a lower learning rate and a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.319949</td>\n",
       "      <td>0.414147</td>\n",
       "      <td>0.504416</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149887</td>\n",
       "      <td>0.226945</td>\n",
       "      <td>0.809127</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>0.113654</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.077005</td>\n",
       "      <td>0.940628</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.957311</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.050843</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.044894</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027485</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.967615</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025698</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023194</td>\n",
       "      <td>0.033826</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.032359</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021463</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.973994</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.028313</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>0.026919</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.026321</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.025779</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.024828</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016808</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016330</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016112</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015711</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015183</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015023</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.021225</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.021057</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014319</td>\n",
       "      <td>0.020748</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_output\n",
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not showing the 40 lines of output here to save room; the training process is recorded in `learn.recorder`, with the table of output stored in the `values` attribute, so we can plot the accuracy over training as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982826292514801"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have something that is rather magical:\n",
    "\n",
    "1. A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\n",
    "1. A way to find the best set of parameters for any function (stochastic gradient descent)\n",
    "\n",
    "This is why deep learning can do things which seem rather magical, such fantastic things. Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take. It seems too good to be true—surely things should be more difficult and complicated than this? Our recommendation: try it out! We just tried it on the MNIST dataset and you have seen the results. And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Going Deeper</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to stop at just two linear layers. We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers. As you will learn, however, the deeper the model gets, the harder it is to optimize the parameters in practice.\n",
    "\n",
    "We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.\n",
    "\n",
    "That means that we can train the model more quickly, and it will take up less memory. In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice. Eventually, theoretical results were developed which showed why this happens. Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity.\n",
    "\n",
    "Here what happens when we train an 18-layer model using the same approach we saw in <<chapter_intro>>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.139514</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>0.995093</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">End of Seminar</h1>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
