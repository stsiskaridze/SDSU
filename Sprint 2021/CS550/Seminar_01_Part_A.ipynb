{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Date: **26 January 2020**\n",
    "<br>\n",
    "\n",
    "\n",
    "Title: **Seminar 1 - Part A**\n",
    "\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "\n",
    "Bibliography: \n",
    "<br>\n",
    "[1] Jeremy Howard & Sylvain Gugger, Deep Learning for Coders with fastai & PyTorch, O'Reilly Media, Inc., 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Running Your First Notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Deep Leaning Jargon</h2>\n",
    "\n",
    "Samuel was working in the 1960s, and since then terminology has changed. \n",
    "\n",
    "\n",
    "Here is the modern deep learning terminology for all the pieces we have discussed:\n",
    "\n",
    "- The functional form of the model is called its **architecture**.\n",
    "  \n",
    "- The **weights** are called **parameters**.\n",
    "  \n",
    "- The **predictions** are calculated from the **independent variable**, which is the **data** not including the **labels**.\n",
    "  \n",
    "- The **results** of the model are called **predictions**.\n",
    "  \n",
    "- The measure of **performance** is called the **loss**.\n",
    "\n",
    "- The loss depends not only on the predictions, but also on the correct **labels** (also known as **targets** or the **dependent variable**); e.g., “dog” or “cat.”\n",
    "\n",
    "\n",
    "\n",
    "After making these changes, the **detailed training loop** looks like this:\n",
    "\n",
    "  <center><img src=\"images/L1_Architecture.png\" width=\"820\"  alt=\"Example\" /></center>\n",
    "\n",
    "<br>\n",
    "  <center><img src=\"images/S1_Jargon.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">The Software: PyTorch, FastAI and Jupyter</h3>\n",
    "\n",
    "  <center><img src=\"images/L1_FastAI.png\" width=\"600\"  alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.172150</td>\n",
       "      <td>0.020986</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>0.022971</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224), num_workers = 0)\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.163848</td>\n",
       "      <td>0.018788</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.041324</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.039417</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006691</td>\n",
       "      <td>0.013764</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224), num_workers = 0)\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/Equalibria/.fastai/data/oxford-iiit-pet/images')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <center><img src=\"images/S1_Path.png\" width=\"1500\"  alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <center><img src=\"images/S1_Classification_and_Regression.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <center><img src=\"images/S1_Overfitting.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <center><img src=\"images/S1_Validation_Set.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">CNN Architecture Overview</h3>\n",
    "\n",
    "- **Convolutional Neural Networks** take advantage of the fact that the **input consists of images** and they constrain the architecture in a more sensible way.\n",
    "\n",
    "\n",
    "- The **Layers** of a CNN have neurons arranged in **3 dimensions**: **width, height, depth**.\n",
    "\n",
    "\n",
    "<img src=\"images/L8_CNN3.jpeg\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "- The **neurons in a layer** will only be **connected** to a **small region** of the **layer before** it, instead of all of the neurons in a fully-connected manner.\n",
    "\n",
    "\n",
    "- The **final output layer** would have (for example) dimensions $1 \\times 1 \\times 10$, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. \n",
    "\n",
    "\n",
    "<img src=\"images/L8_CNN.jpeg\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Transfer Learning</h3>\n",
    "\n",
    "<img src=\"images/S1_Transfer_Learning.png\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/S1_Fine_Tunning.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An **epoch** is one complete pass through the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">4 Pre-Trained Models for Computer Vision</h3>\n",
    "\n",
    "Here are the four pre-trained networks you can use for computer vision tasks such as ranging from image generation, neural style transfer, image classification, image captioning, anomaly detection, and so on:\n",
    "\n",
    "- VGG19\n",
    "\n",
    "- Inceptionv3 (GoogLeNet)\n",
    "\n",
    "- ResNet50\n",
    "\n",
    "- EfficientNet\n",
    "\n",
    "Source: https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">What is a metric?</h3>\n",
    "\n",
    "  A metric is a function that measures the **quality of the model’s predictions** using the validation set, and will be printed at the end of each epoch. \n",
    "  \n",
    "  In this case, we’re using error_rate, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly. \n",
    "  \n",
    "  Another common metric for classification is accuracy (which is just 1.0 - error_rate). \n",
    "  \n",
    "  fastai provides many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">What Our Image Recognizer Learned</h2>\n",
    "\n",
    "- In 2013, PhD student Matt Zeiler and his supervisor, Rob Fergus, published “Visualizing and Understanding Convolutional Networks”, which showed how to visualize the neural network weights learned in each layer of a model. \n",
    "\n",
    "\n",
    "- They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/S1_Layer_1.png\" width=\"400\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/S1_Layer_2.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/S1_Layer_3.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/S1_Layer_4.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Image Recognizers Can Tackle Non-Image Tasks</h3>\n",
    "\n",
    "- An image recognizer can, as its name suggests, only recognize images. \n",
    "\n",
    "\n",
    "- But a lot of things can be represented as images, which means that an image recognizer can learn to complete many tasks.\n",
    "\n",
    "\n",
    "- Sspectrograms of sounds:\n",
    "\n",
    "<center><img src=\"images/S1_Sounds.png\" width=\"800\"  alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- Malware examples:\n",
    "\n",
    "<center><img src=\"images/S1_Malware.png\" width=\"800\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Part A</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
