{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Course: **Machine Learning**\n",
    "<br>\n",
    "\n",
    "Title: **Lecture 4**\n",
    "<br>\n",
    "\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "\n",
    "Bibliography:\n",
    "<br>\n",
    "[1] Bishop, Christopher M., *Pattern Recognition and Machine Learning*, Springer, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Linear Models for Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Problem Statement</h3>\n",
    "\n",
    "- We explored a class of regression models having particularly simple analytical and computational properties.\n",
    "\n",
    "\n",
    "- We now discuss an analogous class of models for solving classification problems.\n",
    "\n",
    "\n",
    "- The **goal** in **classification** is to take an **input vector** $\\mathbf{x}$ and to **assign** it to **one of** $K$ **discrete classes** $C_k$ where $k = 1, \\cdots , K$.\n",
    "\n",
    "\n",
    "- In general, the **classes** are taken to be **disjoint**, so that **each input** is assigned to **one and only one class**.\n",
    "\n",
    "\n",
    "- This divide the **input space** into **decision regions**, whose boundaries are called **decision boundaries**, or **decision surfaces**.\n",
    "\n",
    "\n",
    "- In this lecture we consider **linear models for classification**, by which we mean that the **decision surfaces** are **linear functions** of the **input vector** $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "- If the **input space** is $D$-**dimesional space**, hence the **decision surfaces** are defined by $(D − 1)$-**dimensional hyperplanes**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Target Variable</h3>\n",
    "\n",
    "- There are various ways of using target values to represent class labels.\n",
    "\n",
    "\n",
    "- In the case of **two-class problems**, the **binary representation** is used in which there is a **single target variable** $t\\in\\{0, 1\\}$ such that $t = 1$ represents **class** $C_1$ and $t = 0$ represents class $C_2$.\n",
    "\n",
    "  One can **interpret** the **value of** $t$ as the **probability** that the **class is** $C_1$, with the values of probability taking only the extreme values of $0$ and $1$.\n",
    "  \n",
    "  \n",
    "- For $K > 2$ classes, it is convenient to use a $1$-of-$K$ **coding scheme** in which $t$ is a **vector of length** $K$ such that **if the class is** $C_j$, then **all elements** $t_k$ of $t$ are **zero except element** $t_j$ , which takes the value $1$:\n",
    "\n",
    "  $$\\mathbf{t} = \\left ( 0, 1, 0, 0, 0\\right )^T.$$\n",
    "  \n",
    "  Again, one can **interpret** the **value of** $t_k$ as the **probability** that the **class is** $C_k$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Different Approaches to the Problem of Classification</h3>\n",
    "\n",
    "- There are **three distinct approaches** to the **classification problem**:\n",
    "\n",
    "  - A **discriminant function** that directly assigns each vector $x$ to a specific class.\n",
    "  \n",
    "  - Model **conditional probability distribution** $p(C_k | \\mathbf{x})$ using direct modeling approach.\n",
    "    \n",
    "  - Model **conditional probability distribution** $p(C_k | \\mathbf{x})$ using **Bayes’ theorem**:\n",
    "  \n",
    "    $$p(C_k | \\mathbf{x}) = \\frac{p(\\mathbf{x}| C_k) p(C_k)}{p(\\mathbf{x})}.$$\n",
    "    \n",
    "    \n",
    "- Today we will consider only the **first approach**!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Activation Function</h3>\n",
    "\n",
    "- In the **linear regression models** the model prediction $y(\\mathbf{x}, \\mathbf{w})$ was the real number.\n",
    "\n",
    "\n",
    "- For **classification** problems, however, we wish to predict discrete class labels so that $y$ lie in the range $(0,1)$.\n",
    "\n",
    "\n",
    "- To achieve this, we consider a **generalization of this model** in which we **transform the linear function** of $\\mathbf{w}$ using a **nonlinear function** $f( \\cdot )$ so that:\n",
    "  \n",
    "  $$y(\\mathbf{x}) = f\\left( \\mathbf{w}^T \\mathbf{x} + w_0\\right).$$\n",
    "  \n",
    "- In the **Machine Learning** literature $f( \\cdot)$ is known as an **activation function**.\n",
    "\n",
    "\n",
    "- The **decision surfaces** correspond to $y(x) =$ $constant$, so that $\\mathbf{w}^T \\mathbf{x} + w_0 =$ $constant$ and hence the **decision surfaces** are **linear functions** of $x$, **even** if the **function** $f( \\cdot)$ is **nonlinear**.\n",
    "\n",
    "\n",
    "- For this reason, this **class of models described** are called **generalized linear models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discriminant Functions (Two Classes Case)</h3>\n",
    "\n",
    "- The simplest representation of a **linear discriminant function** is obtained by taking a linear function of the input vector so that:\n",
    "\n",
    "  $$y(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}}\\mathbf{x} + w_0,$$\n",
    "  where $\\mathbf{w}$ is called a **weight vector**, and $w_0$ is a **bias**.\n",
    "\n",
    "\n",
    "- In case of **two classes** classification problem, an **input vector** $x$ is **assigned** to **class** $\\mathcal{C}_1$ if $y(\\mathbf{x})\\geq 0$ and to **class** $\\mathcal{C}_2$ otherwise.\n",
    "\n",
    "- The corresponding **decision boundary** is therefore defined by the relation $y(\\mathbf{x}) = 0$, which corresponds to a **$(D − 1)$-dimensional hyperplane** within the **$D$-dimensional input space**.\n",
    "\n",
    "\n",
    "- If $\\mathbf{x}$ and $\\mathbf{x}'$ are two points which **lie on the decision surface**, then $y(\\mathbf{x}) = y(\\mathbf{x'}) = 0 \\rightarrow \\mathbf{w}^{\\mathbf{T}}(\\mathbf{x} - \\mathbf{x}') = 0$ and hence the **vector** $\\mathbf{w}$ is **orthogonal** to every vector lying within the decision surface, and so **determines** the **orientation of the decision surface**.\n",
    "\n",
    "\n",
    "- If $\\mathbf{x}$ is a **point on the decision surface**, then the **normal distance** from the **origin** to the **decision surface** is given by:\n",
    "\n",
    "  $$\\frac{\\mathbf{w}^{\\mathbf{T}}\\mathbf{x}}{\\| \\mathbf{w}\\|} = -\\frac{w_0}{\\| \\mathbf{w}\\|},$$\n",
    "\n",
    "  i.e the **bias parameter** $w_0$ determines the location of the decision surface.\n",
    "  \n",
    "  \n",
    "- The **perpendicular distance** $r$ of the point $\\mathbf{x}$ from the **decision surface** can be calculates as:\n",
    "\n",
    "  $$r = \\frac{y(\\mathbf{x})}{\\| \\mathbf{w}\\|}.$$\n",
    "\n",
    "  Indeed, let's consider an arbitrary point $\\mathbf{x}$ and let $\\mathbf{x}_{\\perp}$ be its orthogonal projection onto the decision surface, so that:\n",
    "  \n",
    "  $$r = \\mathbf{x} - \\mathbf{x}_{\\perp}.$$\n",
    "  \n",
    "  Multiplying both sides by $\\mathbf{w}^{\\mathbf{T}}$ and using the expressions $y(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}}\\mathbf{x} + w_0$ and $y(\\mathbf{x}_{\\perp}) = \\mathbf{w}^{\\mathbf{T}}\\mathbf{x}_{\\perp} + w_0 = 0$, we get:\n",
    "  \n",
    "  $$\\mathbf{w}^{\\mathbf{T}} r = \\mathbf{w}^{\\mathbf{T}} \\mathbf{x} - \\mathbf{w}^{\\mathbf{T}} \\mathbf{x}_{\\perp} = \\mathbf{w}^{\\mathbf{T}} \\mathbf{x} + w_0 - \\mathbf{w}^{\\mathbf{T}} \\mathbf{x}_{\\perp} - w_0 = y(\\mathbf{x}) - y(\\mathbf{x}_\\perp) = y(\\mathbf{x}),$$\n",
    "  \n",
    "  Thus, we get our expression:\n",
    "  \n",
    "  $$\\frac{\\mathbf{w}^{\\mathbf{T}}\\mathbf{x}}{\\| \\mathbf{w}\\|} = -\\frac{w_0}{\\| \\mathbf{w}\\|},$$\n",
    "\n",
    "\n",
    "<img src=\"images/L4_Perpendicular_Distance.png\" width=\"600\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discriminant Functions (Multiple Classes Case)</h3>\n",
    "\n",
    "- We might be tempted be to **build** a $K$-class discriminant by **combining a number of two-class discriminant functions**. \n",
    "\n",
    "  However, this leads to some serious difficulties:\n",
    "  \n",
    "  - **one-versus-the-rest** classifier: use $K−1$ classifiers each of which solves a two-class problem of separating points in a particular class $\\mathcal{C}_k$ from points not in that class.\n",
    "\n",
    "  - **one-versus-one** classifier: introduce $K(K − 1)/2$ binary discriminant functions, one for every possible pair of classes.\n",
    "\n",
    "    $$y(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}}\\mathbf{x} + w_0,$$\n",
    "    where $\\mathbf{w}$ is a **weight vector**, and $w_0$ is a **bias**.\n",
    "\n",
    "  As we see, this approach runs into the **problem of ambiguous regions**, as illustrated in the Figure.\n",
    "  \n",
    "<img src=\"images/L4_Multiple_Classes.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- We can avoid these difficulties by considering a **single $K$-class discriminant** comprising $K$ linear functions of the form:\n",
    "\n",
    "  $$y_k(\\mathbf{x}) = \\mathbf{w}_k^{\\mathbf{T}}\\mathbf{x} + w_{k0},$$\n",
    "  \n",
    "  and then assigning a point $\\mathbf{x}$ to class $\\mathcal{C}_k$ if $y_k(\\mathbf{x}) > y_j(\\mathbf{x})$ for all $j \\neq k$.\n",
    "  \n",
    "  The decision boundary between class $\\mathcal{C}_k$ and class $\\mathcal{C}_j$ is therefore given by $y_k(\\mathbf{x}) = y_j(\\mathbf{x})$ and hence corresponds to a $(D − 1)$-dimensional hyperplane defined by:\n",
    "  \n",
    "  $$(\\mathbf{w}_k - \\mathbf{w}_j)^{\\mathbf{T}} \\mathbf{x} + (w_{k0} - w_{j0}) = 0.$$\n",
    "  \n",
    "  This has the same form as the decision boundary for the two-class.\n",
    "  \n",
    "- Note, that he **decision regions** of such a discriminant are always **singly connected** and **convex**.\n",
    "\n",
    "  Indeed, if $\\mathbf{x}$ and $\\mathbf{x}'$ are two points both of which lie inside decision region $\\mathcal{R}_k$, then any point $\\widehat{\\mathbf{x}}$ that lies on the line connecting $\\mathbf{x}$ and $\\mathbf{x}'$ can be expressed in the form:\n",
    "  \n",
    "  $$\\widehat{\\mathbf{x}} = \\lambda \\mathbf{x}  + (1- \\lambda) \\mathbf{x}',$$\n",
    "  \n",
    "  where $0 \\leq \\lambda \\leq 1$. \n",
    "  \n",
    "  From the linearity of the discriminant functions, it follows that:\n",
    "  \n",
    "  $$y_k(\\widehat{\\mathbf{x}}) = \\lambda y_k(\\mathbf{x})  + (1- \\lambda) y_k(\\mathbf{x}').$$\n",
    "  \n",
    "  Because both $\\mathbf{x}$ and $\\mathbf{x}'$ lie inside $\\mathcal{R}_k$, it follows that $y_k({\\mathbf{x}}) > y_j({\\mathbf{x}})$ and $y_k({\\mathbf{x}}') > y_j({\\mathbf{x}}')$ for all $j\\neq k$, and hence $y_k(\\widehat{\\mathbf{x}}) > y_j(\\widehat{\\mathbf{x}})$, and so $\\widehat{\\mathbf{x}}$ also lies inside $\\mathcal{R}_k$.\n",
    "  \n",
    "  \n",
    "<img src=\"images/L4_Convex_decision_boundary.png\" width=\"600\" alt=\"Example\" />\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Least Squares for Classification</h3>\n",
    "\n",
    "- The **least-squares** approach gives an exact **closed-form solution** for the discriminant function **parameters**. \n",
    "\n",
    "\n",
    "- Each class $\\mathcal{C}_k$ is described by its own linear model so that:\n",
    "\n",
    "  $$y_k(\\mathbf{x}) = \\mathbf{w}_k^{\\mathbf{T}}\\mathbf{x} + w_{k0},$$\n",
    "\n",
    "  where $k = 1, ..., K$. \n",
    "  \n",
    "  We can conveniently group these together using vector notation so that:\n",
    "  \n",
    "  $$\\mathbf{y}(\\mathbf{x}) = \\tilde{\\mathbf{W}}^{\\mathbf{T}}\\tilde{\\mathbf{x}}.$$\n",
    "\n",
    "  where $\\tilde{\\mathbf{W}}$ is a matrix whose $k^{th}$ column comprises the $(D + 1)$-dimensional vector $\\tilde{\\mathbf{w}}_k = (w_{k0}, \\mathbf{w}_k^{\\mathbf{T}})^{\\mathbf{T}}$ and $\\tilde{\\mathbf{x}}$ is the corresponding augmented input vector $(1, \\mathbf{x}^{\\mathbf{T}})^{\\mathbf{T}}$ with a dummy input $x_0 = 1$.\n",
    "\n",
    "\n",
    "- As we do for regression, we can **determine** the **parameter matrix** $\\tilde{\\mathbf{W}}$ by **minimizing** a **sum-of-squares error** function.\n",
    "\n",
    "\n",
    "- Lets consider a training data set $\\mathbf{x}_n, \\mathbf{t}_n$ where $n = 1, ..., N$, and define matrix $\\mathbf{T}$ whose $n^{th}$ row is the vector $\\mathbf{t}_n^{\\mathbf{T}}$ and matrix $\\tilde{\\mathbf{X}}$ whose $n^{th}$ row is the vector $\\mathbf{\\tilde{x}}_n^{\\mathbf{T}}$.\n",
    "  \n",
    "  The **sum-of-squares error** function can then be written as:\n",
    "  \n",
    "  $$E_D(\\tilde{\\mathbf{W}}) = \\frac{1}{2} \\mathbf{Tr} \\left \\{ (\\tilde{\\mathbf{X}}\\tilde{\\mathbf{W}} - \\mathbf{T})^{\\mathbf{T}} (\\tilde{\\mathbf{X}}\\tilde{\\mathbf{W}} - \\mathbf{T})\\right \\}.$$\n",
    "  \n",
    "  **Setting the derivative** with respec to $\\tilde{\\mathbf{W}}$ to **zero**, and rearranging, we then obtain the solution for $\\tilde{\\mathbf{W}}$ in the form:\n",
    "  \n",
    "  $$\\tilde{\\mathbf{W}} = \\left ( \\tilde{\\mathbf{X}}^{\\mathbf{T}}\\tilde{\\mathbf{X}} \\right )^{-1} \\tilde{\\mathbf{X}}^{\\mathbf{T}}\\mathbf{T} = \\tilde{\\mathbf{X}}^{\\dagger} \\mathbf{T},$$\n",
    "  \n",
    "  where $\\tilde{\\mathbf{X}}^{\\dagger} = \\left ( \\tilde{\\mathbf{X}}^{\\mathbf{T}}\\tilde{\\mathbf{X}} \\right )^{-1} \\tilde{\\mathbf{X}}^{\\mathbf{T}}$ is the pseudo-inverse of the matrix $\\tilde{\\mathbf{X}}$.\n",
    "  \n",
    "  The discriminant function will have the form:\n",
    "  \n",
    "  $$y(\\mathbf{x}) = \\tilde{\\mathbf{W}}^{\\mathbf{T}}\\tilde{\\mathbf{x}}  = \\mathbf{T}^{\\mathbf{T}} \\left ( \\tilde{\\mathbf{X}}^{\\dagger} \\right )^{\\mathbf{T}} \\tilde{\\mathbf{x}}.$$\n",
    "\n",
    "  However, even as a **discriminant function**, this **solution suffers** from some **severe problems**:\n",
    "\n",
    "\n",
    "- **Lack robustness**: the **additional data points** produce a **significant change** in the location of the **decision boundary**.\n",
    "  \n",
    "  <img src=\"images/L4_LS_suffering.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "  The left plot shows data from **two classes**, denoted by **red crosses** and **blue circles**, together with the decision boundary found by **least squares (magenta curve)** and also by the **logistic regression model (green curve)**.\n",
    "\n",
    "\n",
    "- The problems with least squares can be more severe than simply lack of robustness:\n",
    "\n",
    "  <img src=\"images/L4_LS_suffering2.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "  On the left is the result of using a least-squares discriminant. We see that the region of input space assigned to the green class is too small and so most of the points from this class are misclassified. On the right is the result of using logistic regressions.\n",
    "  \n",
    "  \n",
    "  \n",
    "- The **failure of least squares** should not surprise us when we recall that it **corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution**, whereas **binary target vectors** clearly have a **distribution** that is **far from Gaussian**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Fisher’s Linear Discriminant</h3>\n",
    "\n",
    "- One way to view a **linear classification model** is in terms of **dimensionality reduction**.\n",
    "\n",
    "- Suppose we take the $D$-**dimensional input vector** $\\mathbf{x}$ and **project** it down to **one dimension** using:\n",
    "\n",
    "  $$y = \\mathbf{w}^{\\mathbf{T}} \\mathbf{x}.$$ \n",
    "  \n",
    "  If we place a **threshold** on $y$ and **classify** $y \\geq -w_0$ as **class** $\\mathcal{C}_1$ and otherwise **class** $\\mathcal{C}_2$, then we obtain our standard linear **classifier**.\n",
    "  \n",
    "  <img src=\"images/L4_Fisher_discriminant.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- Lets consider a **two-class problem** in which there are $N_1$ **points** of **class** $\\mathcal{C}_1$ and $N_2$ **points** of **class** $\\mathcal{C}_2$, so the **mean vectors** of the two classes are given by:\n",
    "\n",
    "  $$\\mathbf{m}_1 = \\frac{1}{N_1} \\sum_{n \\in \\mathcal{C}_1} \\mathbf{x_n}, $$\n",
    "  $$\\mathbf{m}_2 = \\frac{1}{N_2} \\sum_{n \\in \\mathcal{C}_2} \\mathbf{x_n}.$$\n",
    "  \n",
    "  The simplest **measure of the separation** of the classes, when projected onto $\\mathbf{w}$, is the  **separation of the projected class means**. \n",
    "  \n",
    "  This suggests that we might choose $\\mathbf{w}$ so as to maximize:\n",
    "   \n",
    "  $$m_2 - m_1 = \\mathbf{w}^{\\mathbf{T}}(\\mathbf{m_2} - \\mathbf{m_1}),$$\n",
    "  \n",
    "  where $m_k = \\mathbf{w}^{\\mathbf{T}}\\mathbf{m}_k.$\n",
    "  \n",
    "  \n",
    "- However, this expression can be made **arbitrarily large** simply by increasing the magnitude of $\\mathbf{w}$. \n",
    "\n",
    "  To solve this problem, we could **constrain** $\\mathbf{w}$ to have **unit length**:\n",
    "\n",
    "  $$\\sum_{i} w_i^{2} = 1.$$\n",
    "  \n",
    "- Therefore, we **transformed** the **set of labelled data points** in $\\mathbf{x}$ into a **labelled set in the one-dimensional space** $y$.\n",
    "  \n",
    "  The within-class **variance** of the transformed data from **class** $\\mathcal{C}_k$ is therefore given by:\n",
    "  \n",
    "  $$s_k^{2} = \\sum_{n \\in \\mathcal{C}_k} (y_n - m_k)^2,$$\n",
    "  \n",
    "  where $y_n = \\mathbf{w}^{\\mathbf{T}} \\mathbf{x}_n.$\n",
    "  \n",
    "  \n",
    "- We can define the **total within-class variance** for the whole data set to be simply $s_1^2 + s_2^2.$\n",
    "  \n",
    "\n",
    "- The **Fisher criterion** is defined to be the **ratio** of the **between-class variance** to the **within-class variance** and is given by:\n",
    "  \n",
    "  $$J(\\mathbf{w}) = \\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}.$$\n",
    " \n",
    " \n",
    "-  We can introduce the **between-class** covariance matrix $\\mathbf{S_B}$ and the **total within-class** covariance matrix $\\mathbf{S_W}$ as follows:\n",
    "\n",
    "  $$\\mathbf{S_B} = (\\mathbf{m_2} - \\mathbf{m_1})(\\mathbf{m_2} - \\mathbf{m_1})^{\\mathbf{T}},$$\n",
    "  $$\\mathbf{S_W} = \\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x_n} - \\mathbf{m_1})(\\mathbf{x_n} - \\mathbf{m_1})^{\\mathbf{T}} + \\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x_n} - \\mathbf{m_2})(\\mathbf{x_n} - \\mathbf{m_2})^{\\mathbf{T}}.$$\n",
    "  \n",
    "  Thus, we can rewrite the **Fisher criterion** in the form:\n",
    "  \n",
    "  $$J(\\mathbf{w}) = \\mathbf{\\frac{w^T S_B w}{w^T S_W w}}.$$\n",
    "  \n",
    "  \n",
    "- **Differentiating** with respect to $\\mathbf{w}$ we find that $J(\\mathbf{w})$ is **maximized** when:\n",
    "  \n",
    "  $$\\mathbf{ (w^T S_B w) S_w w = (w^T S_W w) S_B w }.$$\n",
    "  \n",
    "  \n",
    "- But $\\mathbf{S_B w}$ is always in the direction of $\\mathbf{m_2 - m_1}$, i.e. furthermore, we do not care about the magnitude of $\\mathbf{w}$ only its direction, and so we can drop the scalar factors $\\mathbf{ (w^T S_B w)}$ and $\\mathbf{(w^T S_W w)}.$\n",
    "\n",
    "  Finally, we get:\n",
    "  \n",
    "  $$\\mathbf{ w \\propto S_W^{-1} (m_2 - m_1)}.$$\n",
    "  \n",
    "  This result is is known as **Fisher’s linear discriminant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Relation of Fisher’s Criterion to Least Squares</h3>\n",
    "\n",
    "- Let's show that, for the **two-class** problem, the **Fisher criterion** can be obtained as a **special case** of **least squares**.\n",
    "\n",
    "\n",
    "- For this we need to adopt a slightly **different target coding scheme**.\n",
    "\n",
    "\n",
    "- Let's take **targets** for **class** $C_1$ to be $N/N_1$, where $N_1$ is the **number** of patterns in **class** $C_1$, and $N$ is the **total number** of patterns.\n",
    "\n",
    "\n",
    "- Similarly, for **class** $C_2$, we take the **targets** to be $−N/N_2$, where $N_2$ is the **number** of patterns in **class** $C_2$.\n",
    "\n",
    "\n",
    "- Thus, the **sum-of-squares error** function can be written:\n",
    "\n",
    "  $$E = \\frac{1}{2} \\sum_{n=1}^{N} \\left ( \\mathbf{w}^T \\mathbf{x}_n + w_0 -t_n \\right )^2.$$\n",
    "  \n",
    "\n",
    "- Setting the **derivatives** of $E$ with respect to $w_0$ and $w$ to **zero**, we obtain respectively:\n",
    "\n",
    "  $$\\sum_{n=1}^{N} \\left ( \\mathbf{w}^T \\mathbf{x}_n + w_0 -t_n \\right ) = 0,$$\n",
    "  $$\\sum_{n=1}^{N} \\left ( \\mathbf{w}^T \\mathbf{x}_n + w_0 -t_n \\right ) \\mathbf{x}_n = 0.$$\n",
    "  \n",
    "\n",
    "- Making use of our **choice** of **target coding scheme** for the $t_n$, we obtain an expression for the **bias** in the form:\n",
    "  \n",
    "  $$w_0 = - \\mathbf{w}^T \\mathbf{m},$$\n",
    "  \n",
    "  where we have used:\n",
    "  \n",
    "  $$\\sum_{n=1}^{N} t_n = N_1 \\frac{N}{N_1} - N_2 \\frac{N}{N_2} = 0$$\n",
    "  \n",
    "  and where $\\mathbf{m}$ is the **mean** of the **total data set** and is given by:\n",
    "  \n",
    "  $$m = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n = \\frac{1}{N} (N_1 \\mathbf{m}_1 + N_2 \\mathbf{m}_2).$$\n",
    "  \n",
    "  \n",
    "- The **second equation** can be written as:\n",
    "\n",
    "  $$\\left ( \\mathbf{S}_W + \\frac{N_1 N_2}{N} \\mathbf{S}_B\\right ) \\mathbf{w} = N (\\mathbf{m}_1 - \\mathbf{m}_2).$$\n",
    "  \n",
    "  Thus we can write:\n",
    "  \n",
    "  $$\\mathbf{ w \\propto S_W^{-1} (m_2 - m_1)},$$\n",
    "  \n",
    "  where we have ignored irrelevant scale factors.\n",
    "  \n",
    "- Thus the **weight vector coincides** with that found **from** the **Fisher criterion**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">The Perceptron Algorithm</h3>\n",
    "\n",
    "- Another example of a **linear discriminant model** is the **perceptron of Rosenblatt**.\n",
    "\n",
    "- Lets take a **two-class** model in which the **input vector** $\\mathbf{x}$ is **first transformed** using a **fixed nonlinear transformation** to give a **feature vector** $\\phi(\\mathbf{x})$ and this is then used to construct a **generalized linear model** of the form:\n",
    "\n",
    "  $$y(\\mathbf{x}) = f(\\mathbf{w}^\\mathbf{T} \\phi (\\mathbf{x}) ),$$\n",
    "\n",
    "  where the **nonlinear activation function** $f(\\cdot)$ is given by a **step function** of the form:\n",
    "  \n",
    "  $$f(a) = \n",
    "\\left\\{\\begin{matrix}\n",
    "+1, a \\geq 0\n",
    "\\\\ \n",
    "-1, a < 0\n",
    "\\end{matrix}\\right.$$\n",
    " \n",
    "  For the **perceptron**, it is more convenient to use **target values** $t = +1$ for **class** $\\mathcal{C}_1$ and $t = -1$ for **class** $\\mathcal{C}_2$.\n",
    "\n",
    "\n",
    "\n",
    "- We are **seeking** a **weight vector** $\\mathbf{w}$ such that **patterns**  $\\mathbf{x}_n$ in **class** $\\mathcal{C}_1$ will have $\\mathbf{w^T} \\phi(\\mathbf{x}_n)>0$, whereas **patterns** $\\mathbf{x}_n$ in **class** $\\mathcal{C}_2$ have $\\mathbf{w^T} \\phi(\\mathbf{x}_n)<0$.\n",
    "\n",
    "  Using the $t \\in \\{-1, +1\\}$ target coding scheme it follows that we would like all patterns to satisfy $\\mathbf{w^T} \\phi(\\mathbf{x}_n) t_n>0$.\n",
    "  \n",
    "  \n",
    "- The **perceptron criterion** is given by:\n",
    "\n",
    "  $$ E_P(\\mathbf{w}) = - \\sum_{n \\in \\mathcal{M}} \\mathbf{w^T} \\phi(\\mathbf{x}_n) t_n,$$\n",
    "  \n",
    "  where $\\mathcal{M}$ denotes the set of all **misclassified patterns**, i.e. it associates **zero error** with any **pattern** that is **correctly classified**, whereas for a **misclassified pattern** $\\mathbf{x}_n$ it tries to **minimize** the quantity  $-\\mathbf{w^T} \\phi(\\mathbf{x}_n) t_n$.\n",
    "  \n",
    "  \n",
    "- We can apply the **stochastic gradient descent** algorithm to this **error function** and get:\n",
    "\n",
    "  $$\\mathbf{w}^{(\\tau+1)} = \\mathbf{w}^{(\\tau)}  - \\eta \\nabla E_P(\\mathbf{w})  = \\mathbf{w}^{(\\tau)} + \\eta \\phi_n t_n,$$\n",
    "  \n",
    "  where $\\eta$ is the **learning rate** parameter and $\\tau$ is an integer that indexes the **steps of the algorithm**.\n",
    "  \n",
    "  We can set the **learning** rate parameter $\\eta = 1$, becasue the **perceptron function** $y (\\mathbf{x, w})$ is **unchanged** if we **multiply** $\\mathbf{w}$ by a **constant**.\n",
    "  \n",
    "  \n",
    "- **Interpretation**: We **cycle through** the **training patterns** in turn, and for **each pattern** $\\mathbf{x}_n$ we **evaluate the perceptron function**. \n",
    "  - If the pattern is **correctly classified**, then the **weight vector** remains **unchanged**;\n",
    "  - If the pattern is **incorrectly classified**, then for **class** $\\mathcal{C}_1$ we add the **vector** $\\phi(\\mathbf{x}_n)$ onto the **current estimate** of weight vector $\\mathbf{w}$ while for **class** $\\mathcal{C}_2$ we subtract the **vector** $\\phi(\\mathbf{x}_n)$ from $\\mathbf{w}$. \n",
    "\n",
    "\n",
    "- The **perceptron learning algorithm** is illustrated in next Figure:\n",
    "\n",
    "  <img src=\"images/L4_Perceptron_discriminant.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- The **perceptron convergence theorem** states that if there **exists an exact solution** (in other words, if the training data set is linearly separable), then the **perceptron learning algorithm** is **guaranteed to find** an exact solution in a **finite number of steps**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
