{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Linear Algebra (Part II)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Linear transformations</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. A $linear$ $transformation$ (or $operator$) $A$ on a vector space $\\mathcal{V}$ over a field $\\mathcal{F}$, is a correspondence that assigns to every vector $x \\in \\mathcal{V}$ a vector $Ax \\in \\mathcal{V}$ in such a way that:\n",
    "\n",
    "$$A(\\alpha x  + \\beta y) = \\alpha Ax + \\beta A y$$\n",
    "\n",
    "for any $x, y \\in \\mathcal{V}$ and $\\alpha, \\beta \\in \\mathcal{F}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples</h3>\n",
    "\n",
    "$\\bullet$ Two special types of transformations:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **Null** transformation: $\\Theta x = 0$ for any $x \\in \\mathcal{V}$;\n",
    "<br>\n",
    "&emsp; $\\bullet$ **Identity** transformation: $Ix = x$ for any $x \\in \\mathcal{V}$.\n",
    "\n",
    "$\\bullet$ Let $\\mathcal{X} = \\{ x_1, ..., x_n \\}$ be any basis in $n$-dimensional vectors space $\\mathcal{V}$ and let $\\mathcal{X}' = \\{ y_1, ..., y_n\\}$ be the dual basis of linear functionals in $\\mathcal{V}'$. Write:\n",
    "$$A(x) = y_1(x)x_1 + ... y_n(x)x_n.$$\n",
    "&ensp; It is easy to prove that every linear transformation $A$ has the form just described.\n",
    "\n",
    "$\\bullet$ For any $x \\in \\mathcal{P}$, such that $x(t) = \\sum_{i = 0}^{n-1} \\xi_i t^i$, write:\n",
    "$$(Dx)(t) = \\sum_{i = 0}^{n-1} i \\xi_i t^{i-1}; \\text{     }\n",
    "(Sx)(t) = \\sum_{i = 0}^{n-1} \\frac{\\xi_i}{i+1}  i^{i+1}.$$\n",
    "&ensp; Observe that for polynomials the definition of differentiation and integration can be given purely algebraically, and does not need the usual theory of limiting processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Transformations as vectors</h3>\n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{10}$. The set of all linear transformations $A$ on a vector space $\\mathcal{V}$ is itself a vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$.\n",
    "Let $A$ and $B$ be linear transformations on a vector space $\\mathcal{V}$, we define their $sum$, by the equation:\n",
    "$$S(x) = Ax + Bx \\text{ (for every } x \\in \\mathcal{V} \\text{)}.$$\n",
    "We observe that the commutativity and associativity of addition imply immediately that the addition of linear transformations is commutative and associative.\n",
    "<br>\n",
    "If we consider the sum of any linear transformations $A$ and the linear transformation $\\Theta$, we see that:\n",
    "$$A + \\Theta = A.$$\n",
    "If, for each $A$, we denote by $-A$ the transformation defined by $(-A)x = -(Ax)$, we see that:\n",
    "$$A + (-A) = \\Theta.$$\n",
    "For any $A$, and any $\\alpha \\in \\mathcal{F}$ we define the product $\\alpha A$ by the equation: \n",
    "$$(\\alpha A)x = \\alpha(Ax).$$\n",
    "To sum up: the properties of a vector space, described in the axioms **(A)**, **(B)** and **(C)**, appear again in the set of all linear transformations on the space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 6.1</h3>\n",
    "\n",
    "Prove that if $\\mathcal{V}$ is a finite-dimensional vector space, then the space of all linear transformations $A$ on $\\mathcal{V}$ is finite-dimensional, and find its dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 6.2</h3>\n",
    "\n",
    "&emsp; (a) Suppose that $\\mathcal{U}$ and $\\mathcal{V}$ are vector spaces over the same field $\\mathcal{F}$.\n",
    "<br>\n",
    "&emsp; &emsp; If $A$ and $B$ are linear transformations from $\\mathcal{U}$ to $\\mathcal{V}$, if $\\alpha, \\beta \\in \\mathcal{F}$, and if:\n",
    "\n",
    "$$Cx = \\alpha Ax + \\beta Bx,$$\n",
    "\n",
    "&emsp; &emsp; for each $x \\in \\mathcal{U}$, then $C$ is a linear transformation from $\\mathcal{U}$ to $\\mathcal{V}$.\n",
    "\n",
    "&emsp; (b) If we write, by definition, $C = \\alpha A + \\beta B$, then the set of all linear transformations from $\\mathcal{U}$ to $\\mathcal{V}$\n",
    "<br>\n",
    "&emsp; &emsp; becomes a vector space with respect to this definition of the linear operations.\n",
    "\n",
    "&emsp; (c) Prove that if $\\mathcal{U}$ and $\\mathcal{V}$ are finite-dimensional, \n",
    "<br>\n",
    "&emsp; &emsp; then so is the space of all linear transformations from $\\mathcal{U}$ to $\\mathcal{V}$, and find its dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Products</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. The $product$ $P$ of two linear transformations $A$ and $B$, $P = AB$, is defined by the equation:\n",
    "\n",
    "$$Px = A(Bx).$$\n",
    "\n",
    "Transformation multiplication is, in general, **not commutative**, and the order in which we transform makes a lot of difference.\n",
    "The order to transform by $AB$ means to transform first by $B$ and then by $A$. \n",
    "\n",
    "Most of the formal algebraic properties of numerical multiplication are still valid for products:\n",
    "$$A\\Theta = \\Theta A =\\Theta$$\n",
    "$$AI = IA =I$$\n",
    "$$A(B+C) = AB + AC$$\n",
    "$$(A+B)C =AC + BC$$\n",
    "$$A(BC)= (AB)C$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples</h3>\n",
    "\n",
    "Let's consider the differentiation and multiplication transformations $D$ and $T$, defined by:\n",
    "$$(Dx)(t) = \\frac{dx}{dt} \\text{ and } (Tx)(t) = tx(t).$$\n",
    "We have that:\n",
    "$$(DTx)(t)= \\frac{d}{dt}(tx(t))= x(t) + t \\frac{dx}{dt},$$\n",
    "and \n",
    "$$(TDx)(t) = t \\frac{dx}{dt}.$$\n",
    "\n",
    "In other words, not only is it false that $DT \\neq TD$, but, in fact, $(DT - TD)x = x$ for every $x \\in \\mathcal{V}$, \n",
    "<br> i.e. we have that $DT - TD = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Polinomials</h3>\n",
    "\n",
    "\n",
    "$\\bullet$ Although in general transformation multiplication is not commutative, for the powers of one transformation $A$ we do have the usual laws of exponents:\n",
    "\n",
    "$$A^nA^m = A^{n+m} \\text{ and } (A^n)^m = A^{nm}.$$\n",
    "\n",
    "$\\bullet$ We observe that $A^1 = A$. It is customary also to write, by definition, $A^0 = I$.\n",
    "\n",
    "$\\bullet$ The calculus of powers of a single transformation is almost exactly the same as in ordinary arithmetic.\n",
    "<br> \n",
    "&ensp; We may, in particular, define polynomials in a linear transformation. Thus, if:\n",
    "\n",
    "$$p(t) = \\alpha_0 + \\alpha_1 t + ... + \\alpha_n t^n,$$\n",
    "\n",
    "we may form the linear transformation:\n",
    "\n",
    "$$p(A) = \\alpha_0 I + \\alpha_1 A + ... + \\alpha_n A^n,$$\n",
    "\n",
    "$\\bullet$ The rules for the algebraic manipulation of such polynomials are easy:\n",
    "$$p(t)q(t) = r(t) \\text{ implies } p(A)q(A) = r(A).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 7.1</h3>\n",
    "\n",
    "Suppose that $x$ is any polynomial in $\\mathcal{P}$ and $D$ is a differentiation transformation on $\\mathcal{P}$.\n",
    "<br>\n",
    "It is easy to see that for every positive integer k:\n",
    "$$(D^kx)(t) = \\frac{d^kx}{dt^k}.$$\n",
    "If $x$ is a polynomial of degree $n - 1$, what is $D^nx$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\textbf{Definition}$.  A non-zero transformation whose product with some non-zero transformation is zero is called a $divisor$ $of$ $zero$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 7.2</h3>\n",
    "\n",
    "Calculate the linear transformations $D^n S^n$ and $S^n D^n$, $n = 1, 2, 3, ...$; \n",
    "<br>In other words, compute the effect of each such transformation on an arbitrary element of $\\mathcal{P}$. \n",
    "\n",
    "(Here $D$ and $S$ denote the differentiation and integration transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 7.3</h3>\n",
    "\n",
    "Suppose that $Ax(t) = x(t + 1)$ for every $x \\in \\mathcal{P_n}$. Prove that if $D$ is the differentiation operator, then:\n",
    "$$1 + \\frac{D}{1!} + \\frac{D^2}{2!} + ... + \\frac{D^{n-1}}{(n-1)!} = A.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Inverses</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. If linear transformations $A$ has properties:\n",
    "<br>\n",
    "&emsp; (1) If $x_1 \\neq x_2$, then $Ax_1 \\neq Ax_2$;\n",
    "<br>\n",
    "&emsp; (2) To every vector $y$ there corresponds (at least) one vector $x$ such that $Ax = y$;\n",
    "<br>\n",
    "&emsp; then we say that $A$ is $invertable$.\n",
    "\n",
    "For any invertibale linear transformation $A$ we can define a linear transformation, called the $inverse$ of\n",
    "$A$ and denoted by $A^{-1}$, as follows:\n",
    "<br>\n",
    "&emsp; $\\bullet$ If $y$ is any vector, we may (by (2)) find $x$ for with $Ax = y$;\n",
    "<br>\n",
    "&emsp; $\\bullet$ This $x$ is uniquely determined, since $x_1 \\neq x_2$ implies (by (1)) that $y = Ax_0 \\neq Ax_1 = y$.\n",
    "<br>\n",
    "&emsp; $\\bullet$ We define $A^{-1}y$ to be $x$.\n",
    "\n",
    "It is immediate from the definition that for any invertible $A$ we have:\n",
    "$$A A^{-1} = A^{-1} A = I.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{11}$. \n",
    "If $A$, $B$, and $C$ are linear transformations such that $AB = CA = I$, then $A$ is invertible and $A^{-1}=B=C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$.\n",
    "<br>\n",
    "If $Ax_1 = Ax_2$, then $CAx_1 = CAx_2$, so that $x_1 = x_2$, i.e. the first condition of invertibility is satisfied.\n",
    "<br>\n",
    "If $y$ is any vector and $x = By$, then $y = ABy = Ax$, i.e. the second condition is also satisfied.\n",
    "<br>\n",
    "Multiplying $AB = I$ on the left, and $CA = I$ on the right, by $A^{-1}$, we see that $A^{-1} = B = C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{11}$. \n",
    "A linear transformation $A$ on a finite-dimensional vector space $\\mathcal{V}$ is invertible if and only if $Ax = 0$ implies that $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. \n",
    "Let $A$ be the invertible transformation, i.e. both conditions are satisfied.\n",
    "<br> Suppose now that $Ax = 0$ implies that $x = 0$. \n",
    "If $u \\neq v$, that is, $u-v \\neq 0$, implies that $A(u - v) \\neq 0 $, that is, that $Au \\neq Av$. This proves (1).\n",
    "<br> To prove (2), let $\\{ x_1, ... , x_n\\}$ be a basis in $\\mathcal{V}$. We assert that $\\{Ax_1, ..., Ax_n\\}$ is also a basis. \n",
    "<br>\n",
    "According to $\\textbf{Theorem} \\space \\textbf{4}$, we need only prove linear independence. But:\n",
    "$$\\sum_{i=1}^{n} \\alpha_i A x_i = A \\left ( \\sum_{i=1}^{n} \\alpha_i x_i \\right ) = 0.$$\n",
    "By hypothesis, this implies that $\\sum_{i} \\alpha_i x_i = 0$. \n",
    "The linear independence of the $x_i$ implies that $\\alpha_i = 0$ for all $i$.\n",
    "\n",
    "Now let's assume that every $y$ is an $Ax$, and let $\\{y_1, ..., y_n\\}$ be any basis in $\\mathcal{V}$.\n",
    "<br> Corresponding to each $y_i$ we may find a $x_i$ for which $y_i = Ax_i$. We assert that $\\{x_1, ..., x_n\\}$ is also a basis. \n",
    "For $\\sum_{i} \\alpha_i x_i = 0 $ implies $\\sum_{i} \\alpha_i A x_i = \\sum_{i} \\alpha_i y_i = 0$, so that $\\alpha_i = 0$ for all $i$.\n",
    "Consequently every $x$ may be written in the form $\\sum_{i} \\alpha_i x_i$, and $Ax = 0$ implies, as in the argument just given, that $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{12}$.\n",
    "\n",
    "1. If $A$ and $B$ are invertible, then $AB$ is invertible and $(AB)^{-1} = B^{-1}A^{-1}$.\n",
    "2. If $A$ is invertible and $\\alpha \\neq 0$, then $\\alpha A$ is invertible and $(\\alpha A)^{-1}$= $\\alpha^{-1}A^{-1}$.\n",
    "3. If $A$ is invertible, then $A^{-1}$ is invertible and $(A^{-1})^{-1} = A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. \n",
    "The proofs of these statements we leave to the students!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 8.1</h3>\n",
    "\n",
    "A linear transformation $A$ is defined on $\\mathbb{C}^2$ by:\n",
    "\n",
    "$$A(\\xi_1, \\xi_2) = (\\alpha \\xi_1 + \\beta \\xi_2, \\gamma \\xi_1 + \\delta \\xi_2),$$\n",
    "\n",
    "where $\\alpha, \\beta, \\gamma$ and $\\delta$ are fixed scalars. \n",
    "\n",
    "Prove that $A$ is invertible if and only if $\\alpha \\delta - \\beta \\gamma \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 8.2</h3>\n",
    "\n",
    "Show that if $A$ is a linear transformation such that $A^2 - A + I = \\Theta$ then $A$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 8.3</h3>\n",
    "\n",
    "If $A$ and $B$ are linear transformations (on the same vector space) and if $AB = I$, \n",
    "<br> then $A$ is called a $left$ $inverse$ of $B$ and $B$ is called a $right$ $inverse$ of $A$. \n",
    "\n",
    "Prove that if $A$ has exactly one right inverse, say $B$, then $A$ is invertible. \n",
    "\n",
    "($\\textbf{Hint:}$ consider $BA + B - I$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 8.4</h3>\n",
    "\n",
    "If $A$ is an invertible linear transformation on a finite-dimensional vector space $\\mathcal{V}$,\n",
    "then there exists a polynomial $p$ such that $A^{-1} = p(A)$. \n",
    "\n",
    "($\\textbf{Hint:}$ find a non-zero polynomial $q$ of least degree such that $q(A) = 0$ and prove that its constant term cannot be $0$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Matrices</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. Let $\\mathcal{V}$ be an $n$-dimensional vector space over a field $\\mathcal{F}$, \n",
    "let $\\mathcal{X} = \\{x_1, ..., x_n \\}$ be any basis of $\\mathcal{V}$, and let $A$ be a linear transformation on $\\mathcal{V}$. \n",
    "Since every vector is a linear combination of the $x_i$, we have:\n",
    "$$A x_j = \\sum_{i=1}^{n} \\alpha_{ij} x_i$$\n",
    "for $j = \\overline{1,n}$. The set $\\alpha_{ij}$ of $n^2$ scalars, indexed with the double subscript $i$, $j$, \n",
    "is the $matrix$ of $A$ in the coordinate system $\\mathcal{X}$. \n",
    "We denote it by $[A]$.\n",
    "<br>\n",
    "A matrix $[A]$ is usually written in the form of a square array:\n",
    "\n",
    "$$[A] = \\begin{bmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\cdots  & \\alpha_{1n} \\\\ \n",
    "\\alpha_{21} & \\alpha_{11} & \\cdots  & \\alpha_{2n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots  & \\vdots \\\\ \n",
    "\\alpha_{n1} & \\alpha_{11} & \\cdots  & \\alpha_{nn} \n",
    "\\end{bmatrix};$$\n",
    "\n",
    "the scalars $(\\alpha_{i1}, ... , \\alpha_{in})$ form a $row$, and scalars $(\\alpha_{1j}, ... , \\alpha_{nj})$ a $column$, of $[A]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\textbf{Note:}$ We comment on notation. It is customary to use the same symbol, say, $A$, for the matrix as for the transformation. The justification for this is to be found in the discussion below (of properties of matrices).\n",
    "We do not follow this custom here, because one of our principal aims, in connection with matrices, is to emphasize that they depend on a coordinate system (whereas the notion of linear transformation does not), and to study how the relation between matrices and linear transformations changes as we pass from one coordinate system to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples</h3>\n",
    "\n",
    "Let's consider the differentiation transformations $D$ on the space $\\mathcal{P}_n$, and the basis $\\mathcal{X} =\\{x_1, ..., x_n\\}$ defined by $x_i(t) = t^{i-1}$ for $i = \\overline{1, n}$. What is the matrix of $D$ in the basis $\\mathcal{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have:\n",
    "<font size=\"5\">\n",
    "$$\\begin{matrix}\n",
    "Dx_1 = 0x_1 + 0x_2 + & \\cdots  & + 0x_{n-1} + 0x_n \\\\\n",
    "Dx_2 = 1x_1 + 0x_2 + & \\cdots  & + 0x_{n-1} + 0x_n \\\\ \n",
    "Dx_3 = 0x_1 + 2x_2 + & \\cdots  & + 0x_{n-1} + 0x_n \\\\ \n",
    "\\vdots & \\cdots  & \\vdots\\\\ \n",
    "Dx_n = 0x_1 + 0x_2 + & \\cdots  & + (n-1)x_{n-1} + 0 x_n\n",
    "\\end{matrix};$$\n",
    "</font>\n",
    "so that\n",
    "<font size=\"5\">\n",
    "$$[D] = \\begin{bmatrix}\n",
    "0 & 1 & 0 & \\cdots  & 0 & 0 \\\\ \n",
    "0 & 0 & 2 & \\cdots  & 0 & 0 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots  & \\vdots & \\vdots \\\\ \n",
    "0 & 0 & 0 & \\cdots  & 0 & n-1 \\\\ \n",
    "0 & 0 & 0 & \\cdots  & 0 & 0\n",
    "\\end{bmatrix}.$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 9.1</h3>\n",
    "\n",
    "Let's consider the multiplication transformations $T$ on the space $\\mathcal{P}_n$, defined by:\n",
    "$(Tx)(t) = tx(t)$.\n",
    "<br>\n",
    "Let $\\mathcal{X} =\\{x_1, ..., x_n\\}$ be a basis defined by $x_i(t) = t^{i-1}$ for $i = \\overline{1, n}$. \n",
    "\n",
    "What is the matrix of $T$ in the basis $\\mathcal{X}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Matrices of transformations</h3>\n",
    "\n",
    "\n",
    "In a fixed coordinate $\\mathcal{X} = \\{x_1, ..., x_n \\}$, the matrices of $A$ and $B$, how can we find the matrices of $\\Theta$ and $I$, of $C = \\alpha A + \\beta B$, of $P = AB$, etc.?\n",
    "\n",
    "Write $[A] = (\\alpha_{ij})$, $[B] = (\\beta_{ij})$, $[C] = (\\gamma_{ij})$, $[P] = (\\rho_{ij})$, and $[\\Theta] = (o_{ij})$, $[I] = (e_{ij})$ then:\n",
    "\n",
    "\n",
    "$$\\begin{matrix}\n",
    "\\gamma_{ij}  & = & \\alpha \\alpha_{ij} + \\beta \\beta_{ij}; \\\\\n",
    "\\rho_{ij}  & = &\\sum_{k=1}^{n} \\alpha_{ik} \\beta_{kj}; \\\\\n",
    "o_{ij}  & = & 0; \\\\\n",
    "e_{ij}  & = & \\delta_{ij}.\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "To prove the second rule we use the definition of the matrix associated with a transformation,\n",
    "and juggle, thus:\n",
    "$$Px_j = A(Bx_j) = A \\left ( \\sum_{k=1}^{n} \\beta_{kj} x_k \\right ) = \\sum_{k=1}^{n} \\beta_{kj} A x_k = \n",
    "\\sum_{k=1}^{n} \\beta_{kj} \\left ( \\sum_{i=1}^{n} \\alpha_{ik} x_i \\right ) = \\sum_{i=1}^{n} \\left ( \\sum_{k=1}^{n} \\alpha_{ik} \\beta_{kj} \\right ) x_i $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{13}$. Among the set of all matrices $\\alpha_{ij}$, $\\beta_{ij}$, etc., $i, j = \\overline{1,n}$ (not considered in relation to linear transormations), we define sum, scalar multiplication, product, $(o_{ij})$, and $e_{ij}$, by:\n",
    "$$(\\alpha_{ij}) + (\\beta_{ij}) = (\\alpha_{ij} + \\beta_{ij});$$\n",
    "$$\\alpha (\\alpha_{ij}) = (\\alpha  \\alpha_{ij});$$\n",
    "$$(\\alpha_{ij})(\\beta_{ij}) = (\\sum_{k=1}^{n} \\alpha_{ik}\\beta_{kj} );$$\n",
    "$$o_{ij} = 0;$$\n",
    "$$e_ij = \\delta_{ij}.$$\n",
    "\n",
    "Then the correspondence (established by means of an arbitrary coordinate system $\\mathcal{X} =\\{x_1, ..., x_n\\}$ of the $n$-dimensional vector $\\mathcal{V}$), between all linear transformations $A$ on $\\mathcal{V}$ and all matrices $(\\alpha_{ij})$, described by $Ax_i = \\sum_{i} \\alpha_{ij} x_i$, is an isomorphism. In other words, it is a one-to-one correspondence\n",
    "that preserves sum, scalar multiplication, product, 0, and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Matrix multiplication</h3>\n",
    "\n",
    "<img src=\"images/RM_Matrix_Multiplication.png\" width=\"1500\" height=\"1000\" alt=\"Example\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 10.1</h3>\n",
    "\n",
    "Let $A$ be the linear transformation on $\\mathcal{P}_n$ defined by $(Ax)(t) = x(t+1)$,\n",
    "<br> and let $\\mathcal{X} = \\{x_0, ...., x_{n-1}\\}$ be the basis of $\\mathcal{P}_n$ defined by $x_i(t) = t^i$ for $i = \\overline{0, n-1}$. \n",
    "\n",
    "What is the matrix of $A$ in the basis $\\mathcal{X}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Change of basis</h3>\n",
    "\n",
    "\n",
    "Let $\\mathcal{V}$ be an $n$-dimensional vector space and $\\mathcal{X} =\\{x_1, ..., x_n\\}$ and $\\mathcal{Y} =\\{y_1, ..., y_n\\}$ be two bases in $\\mathcal{V}$. \n",
    "<br>\n",
    "We may ask the following two questions.\n",
    "\n",
    "$\\textbf{Question I}$. If $x \\in \\mathcal{V}$ and $x = \\sum_{i} \\xi x_i = \\sum_{i} \\eta_i y_i$, what is the relation between its coordinates $(\\xi_1, ..., \\xi_n)$ with respect to $\\mathcal{X}$ and its coordinates $(\\eta_1, ..., \\eta_n)$ with respect to $\\mathcal{Y}$?\n",
    "\n",
    "$\\textbf{Question II}$. If $(\\xi_1, ..., \\xi_n)$ is an ordered set of $n$ scalars, what is the relation between the vectors $x = \\sum_{i} \\xi_i x_i$ and $y = \\sum_{i} \\xi_i y_i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both these questions are easily answered in the language of linear transformations.\n",
    "<br>\n",
    "We consider, namely, the linear transformation $A$ defined by $Ax_i = y_i$ for $i = \\overline{1,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Answer to Question I}$. Since:\n",
    "$$\\sum_{j=1}^{n} \\eta_j y_j = \\sum_{j=1}^{n} \\eta_j A x_j = \\sum_{j=1}^{n} \\eta_j \\sum_{i=1}^{n} \\alpha_{ij} x_i = \\sum_{i=1}^{n} \\left ( \\sum_{j=1}^{n} \\alpha_{ij} \\eta_j \\right ) x_i,$$\n",
    "i.e. we have:\n",
    "$$\\xi_i = \\sum_{j=1}^{n} \\alpha_{ij} \\eta_j.$$\n",
    "\n",
    "$\\textbf{Answer to Question II}$. $$y = Ax$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Roughly speaking, the invertible linear transformation $A$ (or, more properly, the matrix $[A]$) may be considered as a **transformation of coordinates**, or it may be considered as a **transformation of vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Similarity</h3>\n",
    "\n",
    "The following two questions are closely related to those of the preceding\n",
    "section:\n",
    "\n",
    "$\\textbf{Question III}$. If $B$ is a linear transformation on $\\mathcal{V}$, what is the relation between its matrix $(\\beta_{ij})$ with respect to $\\mathcal{X}$ and its matrix $(\\gamma_{ij})$ with respect $\\mathcal{Y}$?\n",
    "\n",
    "$\\textbf{Question IV}$. If $(\\beta_{ij})$ is a matrix, what is the relation between the linear transformations $B$ and $C$ defined, respectively, by $B x_i = \\sum_{i=1}^{n} \\beta_{ij} x_i$ and $C y_i = \\sum_{i=1}^{n} \\beta_{ij} y_i$?\n",
    "\n",
    "Questions III and IV are explicit formulations of a problem we raised before: \n",
    "<br>\n",
    "$\\bullet$ to **one transformation** there correspond **many matrices** (in different coordinate systems);\n",
    "<br>\n",
    "$\\bullet$ to **one matrix** there correspond **many transformations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Answer to Question III}$. We have:\n",
    "$$B x_i = \\sum_{i=1}^{n} \\beta_{ij} x_i$$\n",
    "$$B y_i = \\sum_{i=1}^{n} \\gamma_{ij} y_i$$\n",
    "Using the linear transformation $A$ defined in the preceding section, we may write:\n",
    "$$B y_j = B A x_j = B \\left ( \\sum_{k=1}^{n} \\alpha_{kj} x_k\\right ) = \\sum_{k=1}^{n} \\alpha_{kj} B x_k = \n",
    "\\sum_{k=1}^{n} \\alpha_{kj} \\sum_{i=1}^{n} \\beta_{ik} x_i = \\sum_{i=1}^{n} \\left ( \\sum_{k=1}^{n} \\beta_{ik} \\alpha_{kj} x_i\\right ),$$\n",
    "$$\\sum_{k=1}^{n} \\gamma_{kj} y_k = \\sum_{k=1}^{n} \\gamma_{kj} A x_k = \n",
    "\\sum_{k=1}^{n} \\gamma_{kj} \\sum_{i=1}^{n} \\alpha_{ik} x_i = \\sum_{i=1}^{n} \\left ( \\sum_{k=1}^{n} \\alpha_{ik} \\gamma_{kj} x_i\\right ).$$\n",
    "comparing with each other, we see that:\n",
    "$$\\sum_{k=1}^{n} \\alpha_{ik} \\gamma_{kj} = \\sum_{k=1}^{n} \\beta_{ik} \\alpha_{kj}$$\n",
    "\n",
    "Using matrix multiplication, we can write this in the \"dangerously\" simple form:\n",
    "$[A][C] = [B][A]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Answer to Question IV}$. We observe that:\n",
    "$$C y_j = C A x_j$$,\n",
    "$$\\sum_{i=1}^{n} \\beta_{ij} y_i = \\sum_{i=1}^{n} \\beta_{ij} A x_i = \n",
    "A \\left ( \\sum_{i=1}^{n} \\beta_{ij} x_i \\right ) = AB x_j.$$\n",
    "Hence $C$ is such that $CA x_j = AB x_j$, or, finally:\n",
    "$$C = A B A^{-1}.$$\n",
    "\n",
    "The situation is conveniently summed up in the following mnemonic diagram:\n",
    "<center><img src=\"images/RM_Diagram.svg\" width=\"300\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Range and null-space</h3>\n",
    "\n",
    "$\\textbf{Definition}$. If $A$ is a linear transformation on a vector space $\\mathcal{V}$ and if $\\mathfrak{M}$ is a subspace of $\\mathcal{V}$, the $image$ of $\\mathfrak{M}$ under $A$, in symbols $A\\mathfrak{M}$, is the set of all vectors of the form $Ax$ with $x \\in \\mathfrak{M}$. \n",
    "\n",
    "$\\textbf{Definition}$. The $range$ of $A$ is the set $\\mathfrak{R}(A)= A\\mathcal{V}$. \n",
    "\n",
    "$\\textbf{Definition}$. The $null-space$ of $A$ is the set $\\mathfrak{N}$ of all vectors $x\\in \\mathcal{V}$ for which $Ax = 0$.\n",
    "\n",
    "$\\textbf{Statements:}$\n",
    "<br>\n",
    "$\\bullet$ The transformation $A$ is invertible if and only if $\\mathfrak{R}(A) = \\mathcal{V}$ and $\\mathfrak{N} = 0$.\n",
    "<br>\n",
    "$\\bullet$ In case $\\mathcal{V}$ is finite-dimensional, $A$ is invertible if and only if $\\mathfrak{R}(A) = \\mathcal{V}$ or $\\mathfrak{N} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Rank and nullity</h3>\n",
    "\n",
    "$\\textbf{Definition}$. The $rank$, $\\rho(A)$, of a linear transformation $A$ on a finite-dimensional vector space is the dimension of $\\mathfrak{R}(A)$.\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. The $nullity$, $\\nu(A)$, of a linear transformation $A$ on a finite-dimensional vector space is the dimension of $\\mathfrak{N}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{14}$. If $A$ is a linear transformation on an $n$-dimensional vector space $\\mathcal{V}$, then $\\nu(A) = n - \\rho(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. \n",
    "The proofs of this theorem we leave to the students!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{15}$. If $A$ and $B$ are linear transformations on a finite-dimensional vector space $\\mathcal{V}$, then:\n",
    "$$\\rho(A + B) \\leq \\rho(A) + \\rho(B);$$\n",
    "\n",
    "$$\\left.\\begin{matrix}\n",
    "\\rho(AB) \\leq min \\{ \\rho(A), \\rho(B)\\}\\\\ \n",
    "\\nu(AB) \\leq \\nu(A) + \\nu(B)\n",
    "\\end{matrix}\\right\\}\\text{(known as Sylvester's law of nullity).}\n",
    "$$\n",
    "\n",
    "If $B$ is invertible, then:\n",
    "$$\\rho(AB)  = \\rho(BA)=\\rho(A).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. Since $(AB)x = A(Bx)$, it follows that  $\\mathfrak{R}(AB)$ is contained in  $\\mathfrak{R}(A)$, so that $\\rho(AB) \\leq \\rho(A)$. \n",
    "<br> In other words, the rank of a product is not greater than the rank of the first factor. \n",
    "<br> If $B$ is invertible, then:\n",
    "$$\\rho(A) = \\rho(ABB^{-1}) \\leq \\rho (AB) \\text{ and } \\rho(A) = \\rho(B^{-1}BA) \\leq \\rho (BA).$$\n",
    "The proof of other statements we leave as an exercises for the students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Determinant</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. The $determinant$ of the linear transformation $A$ over the vector space $\\mathcal{V}$, denoted as $det(A)$, or $|A|$, is a scalar value that describes the $n$-dimensional volume **scaling factor** of the $A$.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/RM_Determinant.gif\" width=\"1000\" height=\"300\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">How to define the determinant</h3>\n",
    "\n",
    "$\\bullet$ The $Leibniz$ formula for the determinant of a $2 \\times 2$ matrix is:\n",
    "$$|A| = \\begin{vmatrix}\n",
    "a & b\\\\ \n",
    "c & d\n",
    "\\end{vmatrix}\n",
    "= ad - bc.\n",
    "$$\n",
    "\n",
    "$\\bullet$ The $Laplace$ formula for the determinant of a $3 \\times 3$ matrix is:\n",
    "\n",
    "$$|A| =\n",
    "\\begin{vmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{vmatrix}\n",
    "=\n",
    "a \\cdot\n",
    "\\begin{vmatrix}\n",
    "e & f \\\\\n",
    "h & i\n",
    "\\end{vmatrix}\n",
    "- b \\cdot\n",
    "\\begin{vmatrix}\n",
    "d & f \\\\\n",
    "g & i\n",
    "\\end{vmatrix}\n",
    "+ c \\cdot\n",
    "\\begin{vmatrix}\n",
    "d & e \\\\\n",
    "g & h\n",
    "\\end{vmatrix}\n",
    "= aei + bfg + cdh - ceg - bdi - afh.\n",
    "$$\n",
    "\n",
    "$\\bullet$ The $Leibniz$ formula for the determinant of an $n \\times n$ matrix is:\n",
    "$$|A| = \\sum_{\\sigma \\in S_n} \\left ( sign(\\sigma)\\prod_{i=1}^{n} \\alpha_{i\\sigma_i} \\right ),$$\n",
    "where the sum is computed over all permutations $\\sigma$ of the set $\\{1, 2, ..., n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Properties of the determinant</h3>\n",
    "\n",
    "The determinant has many properties. Some basic properties of determinants are:\n",
    "1. $|I_n| = 1$, where $I_n$ is the $n \\times n$ identity matrix;\n",
    "\n",
    "2. $|A^T| = |A|$, where $A^T$ denotes the transpose of $A$;\n",
    "\n",
    "3. $|A^{-1}| = |A|^{-1}$;\n",
    "\n",
    "4. $|AB| = |A|\\cdot |B|$ for square matrices $A$ and $B$ of equal size;\n",
    "\n",
    "5. $|\\alpha A| = \\alpha^n |A|$, for an $n \\times n$ matrix $A$;\n",
    "\n",
    "6. $|A + B| \\geq |A| + |B|$ positive semidefinite matrices $A$ and $B$;\n",
    "\n",
    "7. $|A| = \\prod_{i=1}^{n} \\alpha_{ii} $ for triangular matrix $A$, i.e. $\\alpha_{ij} =0$ whenever $i >j$;\n",
    "\n",
    "8. $|A| = \\prod_{i=1}^{n} \\lambda_{i} $ for an $n \\times n$ matrix $A$ with eigenvalues $\\lambda_1, ..., \\lambda_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Eigenvectors and eigenvalues</h3>\n",
    "\n",
    "$\\textbf{Definition}$. If $A$ is a linear transformation on $n$-dimensional vector space $\\mathcal{V}$ over a field $\\mathcal{F}$, and there exists **non-zero** vector $x\\in \\mathcal{V}$ and scalar $\\lambda \\in \\mathcal{F}$, such that $Ax = \\lambda x$, then $x$ is called $eigenvector$ and $\\lambda$ is called $eigenvalue$ of the linear transformation $A$.\n",
    "\n",
    "A more usual form is:\n",
    "$$Ax = \\lambda x \\rightarrow Ax - \\lambda x = 0 \\rightarrow (A - \\lambda I) x = 0$$\n",
    "\n",
    "If this equation has a solution, we can calculate eigenvector and eigenvalue for the linear transformation $A$.\n",
    "\n",
    "The eigenvalues can be obtained from the equation $det(A - \\lambda I ) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Eigendecomposition of a matrix</h3>\n",
    "\n",
    "\n",
    "Let $A$ be a linear transformation on $n$-dimensional vector space $\\mathcal{V}$.\n",
    "Suppose that $\\mathcal{Y} = \\{q_1, ..., q_n\\}$ is the set (basis) of linear independent eigenvectors of $A$ with corresponding eigenvalues $\\lambda_1 \\geq ...\\geq \\lambda_n$ (sorted in descending order).\n",
    "Easy to see that matrix of the linear transformation $A$ in basis $\\mathcal{Y}$ has the form:\n",
    "\n",
    "$$\n",
    "\\Lambda = \n",
    "\\begin{bmatrix}\n",
    " \\lambda_1 & 0 & \\cdots & 0 \\\\ \n",
    " 0 & \\lambda_2 & \\cdots & 0 \\\\ \n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    " 0 & 0 & \\cdots & \\lambda_n \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Lets suppose now that $\\mathcal{X} = \\{ x_1, ..., x_n\\}$ is any basis of vector space $\\mathcal{V}$, \n",
    "and $[A]$ is the matrix of $A$ in the coordinate system $\\mathcal{X}$.\n",
    "If $[Q]$ is the matrix of the transformation of coordinates: $Q: \\mathcal{X} \\to \\mathcal{Y}$, i.e. $Q x_i = y_i$, \n",
    "then the matrix $[A]$ of the linear transformation $A$ can be fictorized to the form:\n",
    "\n",
    "$$[A] = [Q][\\Lambda][Q^{-1}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Visualisation of eigenvalues and eigenvectors</h3>\n",
    "\n",
    "<br>\n",
    "<img src=\"images/RM_Eigenvalues_and_Eigenvectors.gif\" width=\"1000\" height=\"300\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Application of matrices for solving a system of linear equations</h3>\n",
    "\n",
    "$\\bullet$ A general system of $m$ linear equations with $n$ unknowns can be written as:\n",
    "$$\\begin{matrix}\n",
    "\\alpha_{11} x_1 + \\alpha_{12} x_2 + \\cdots + \\alpha_{1n} x_n = b_1\\\\ \n",
    "\\alpha_{21} x_1 + \\alpha_{22} x_2 + \\cdots + \\alpha_{2n} x_n = b_2\\\\  \n",
    "\\vdots \\\\\n",
    "\\alpha_{m1} x_1 + \\alpha_{12} x_2 + \\cdots + \\alpha_{mn} x_n = b_m\n",
    "\\end{matrix}$$\n",
    "\n",
    "where $x_1, ..., x_n$ are the unknowns, $\\alpha_{11}, ..., \\alpha_{mn}$ are the coefficients of the system, and $b_1, ..., b_m$ are the constant terms.\n",
    "<br>\n",
    "$\\bullet$ We can write this system of linear equations in the equivalent matrix form:\n",
    "$$Ax = b,$$\n",
    "$$\\text{where } A = \n",
    "\\begin{bmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\cdots  & \\alpha_{1n} \\\\ \n",
    "\\alpha_{21} & \\alpha_{11} & \\cdots  & \\alpha_{2n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots  & \\vdots \\\\ \n",
    "\\alpha_{n1} & \\alpha_{11} & \\cdots  & \\alpha_{nn} \n",
    "\\end{bmatrix}\n",
    "\\text{, }\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\text{ and }\n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Solving a linear system</h3>\n",
    "\n",
    "There are **several algorithms** for solving a system of linear equations:\n",
    "- **Elimination of variables**. The simplest method for solving a system of linear equations by repeatedly eliminating the variables.\n",
    "- **Gaussian elimination**. The matrix is modified using elementary row operations until it reaches reduced **Row Echelon Form**:\n",
    " - **Type 1:** Swap the positions of two rows;\n",
    " - **Type 2:** Multiply a row by a nonzero scalar;\n",
    " - **Type 3:** Add to one row a scalar multiple of another.\n",
    "- **Cramer's rule**. An explicit formula for the solution of a system of linear equations, with each variable given by a fraction of two determinants;\n",
    "- **Matrix solution**: If the matrix $A$ is square and has full rank then the system has a unique solution given by $x = A^{-1}b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Singular Value Decomposition (SVD)</h3>\n",
    "\n",
    "The matrix $AA^T$ and $A^TA$ are very special in linear algebra, since they has next useful properties:\n",
    "<br> $\\bullet$ they are symmetrical;\n",
    "<br> $\\bullet$ they are square;\n",
    "<br> $\\bullet$ they are positive semidefinite, i.e. eigenvalues are zero or positive: $\\sigma_i \\geq 0$;\n",
    "<br> $\\bullet$ both matrices have the same positive eigenvalues;\n",
    "<br> $\\bullet$ both have the same rank: $\\rho(AA^T)=\\rho(A^TA)=\\rho(A)$;\n",
    "\n",
    "Let $u_i$ and $v_i$ be the eigenvectors of $AA^T$ and $A^TA$ respectively: $(AA^T)u_i = \\sigma_i u_i$ and $(A^TA)v_i = \\sigma_i v_i$;\n",
    "\n",
    "$\\textbf{Definition}$. The eigenvectors $u_i$ and $v_i$ are called the $singular$ $vectors$ and the square roots of $\\sigma_i$ eigenvalues are called $singular$ $values$ of the matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{16}$. Let $A$ be the matrix of the linear transformation $A$ over the $n$-dimensional vector space $\\mathcal{V}$. Then $A$ can be factorized as:\n",
    "$$A = USV^T,$$\n",
    "where $U$ and $V$ are $m \\times r$ and $r \\times n$ orthogonal matrices, i.e. $UU^T = U^TU = I$ and $VV^T = V^TV = I$, \n",
    "<br>with eigenvectors chosen from $AA^T$ and $A^TA$ respectively.\n",
    "<br>$S$ is an $r \\times r$ diagonal matrix with elements equal to the **root of the positive eigenvalues** of $AA^T$ or $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. Lets consider the eigenvectors and eigenvalues of the matrix $AA^T$ and $A^TA$ :\n",
    "$$AA^T u_i = \\sigma_i u_i \\text{, for } i =\\overline{1,m};$$\n",
    "$$A^TA v_i = \\sigma_i v_i \\text{, for } i =\\overline{1,n}.$$\n",
    "We can write these equations in matrix form:\n",
    "$$AA^T U = U S^2 \\text{ and } A^TA V = V S^2,$$\n",
    "where $U = \\{u_1, ..., u_m\\}$, $V = \\{v_1, ..., v_n\\}$ and $S = diag(\\sigma_1, ... , \\sigma_r, 0, ..., 0).$<br>\n",
    "Remembering, that $UU^T = I$ and $V^TV = I$, we have:\n",
    "$$AA^T = U S^2 U^T = U S V^T V S U^T = (U S V^T)(U S V^T)^T.$$\n",
    "Therefore, $A$ can be expressed in the form:\n",
    "$$A = U S V^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Visualisation of SVD</h3>\n",
    "\n",
    "<br>\n",
    "<img src=\"images/RM_SVD.jpg\" width=\"1500\" height=\"300\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Principal Component Analysis (PCA)</h3>\n",
    "\n",
    "$\\textbf{Definition}$. $Principal \\text{ } Component \\text{ } Analysis$ ($PCA$) is a dimensionality reduction technique that enables you to identify correlations and patterns in a data set so that it can be transformed into a data set of significantly lower dimension without loss of any important information.\n",
    "\n",
    "\n",
    "The standard context for PCA as an exploratory data analysis tool involves a dataset with observations on $m$ numerical $vectors$, for each of $n$ $features$. \n",
    "These $m$ numerical values define $n$-dimensional vectors $\\{x_1, ..., x_m\\}$, where $x_i = \\{x_{1i}, ..., x_{ni}\\}$ for $i = \\overline{1, m}$. ($x_{ij} \\in \\mathbb{R}$ for $i=\\overline{1,m}$ and $j=\\overline{1,n}$).\n",
    "<br> Or, equivalently, data is defined as $m \\times n$ data matrix $X =(x_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step by step computation of PCA</h3>\n",
    "\n",
    "The below steps need to be followed to perform dimensionality reduction using PCA:\n",
    "\n",
    "1. Standardization of the data set;\n",
    "\n",
    "2. Calculation of the covariance matrix;\n",
    "\n",
    "3. Calculation of the singular values and singular vectors and factorization of the covariance matrix;\n",
    "\n",
    "4. Calculation of the Prinipal omponents and reduction of the data set size;\n",
    "\n",
    "5. Data reconstruction from a reduced data set;\n",
    "\n",
    "6. Validation of the reconstricted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 1: Standardization of the data</h3>\n",
    "\n",
    "$\\bullet$ The properties of PCA have some **undesirable features** when variables are measures in **different units**;\n",
    "\n",
    "$\\bullet$ To **overcome** this undesirable feature, it is common practice to begin by **standardizing** the variables;\n",
    "\n",
    "$\\bullet$ Standardization is carried out by replacing initial data matrix $X$ with the standardized data matrix $Y$;\n",
    "\n",
    "$\\bullet$ Each data value $x_{ij}$ is both centered and divided by the standard deviation $s_j$ of the $n$ observations of the variable $j$:\n",
    "$$y_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j},$$\n",
    "$$\\text{where } \\bar{x}_j = \\frac{1}{m}\\sum_{i=1}^{m} x_{ij} \\text{ and } s_j = \\sqrt{\\frac{1}{m-1} \\sum_{i=1}^{m}(x_{ij} - \\bar{x}_j)^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 2: Computing the covariance matrix $\\Sigma$</h3>\n",
    "\n",
    "$\\bullet$ A covariance matrix $\\Sigma$ expresses the **correlation between each two different features** in the data set:\n",
    "\n",
    "$$\\Sigma =  \n",
    "\\begin{bmatrix}\n",
    "cov[y_1, y_1] & cov[y_1, y_2] & \\cdots & cov[y_1, y_n]\\\\\n",
    "cov[y_2, y_1] & cov[y_2, y_2] & \\cdots & cov[y_2, y_n]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "cov[y_n, y_1] & cov[y_m, y_2] & \\cdots & cov[y_n, y_n]\\\\ \n",
    "\\end{bmatrix}\n",
    ",$$\n",
    "where where each element represents the **covariance between two features** (remember that $y_i$ are centered variables):\n",
    "$$cov[y_i, y_j] = \\frac{1}{m-1} \\sum_{k=1}^{m} (y_i)(y_j)^T.$$\n",
    "\n",
    "\n",
    "$\\bullet$ If the covariance value is **negative**, then the respective features are **indirectly proportional** to each other;\n",
    "\n",
    "$\\bullet$ A **positive** covariance denotes that the respective features are **directly proportional** to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 3: Calculating the singular values and singular vectors</h3>\n",
    "\n",
    "$\\bullet$ The next step is to factorize the matrix $\\Sigma$ using the SVD:\n",
    "\n",
    "$$\\Sigma = U S V^T,$$\n",
    "\n",
    "$\\bullet$ $U$ and $V$ are $n \\times n$ orthogonal matrices, i.e. $UU^T = U^TU = I$ and $VV^T = V^TV = I$,\n",
    "with singular vectors chosen from $\\Sigma\\Sigma^T$ and $\\Sigma^T\\Sigma$ respectively. \n",
    "\n",
    "$\\bullet$  The $S$ is an $n \\times n$ diagonal matrix with elements equal to the $\\sqrt{\\sigma_i}$ of the singular values of $\\Sigma\\Sigma^T$ or $\\Sigma^T\\Sigma$.\n",
    "\n",
    "$\\bullet$  Singular values are sorted in descending order: $\\sigma_1 \\geq \\sigma_2, .... \\geq \\sigma_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 4: Computing the Principal Components (PC)</h3>\n",
    "\n",
    "$\\bullet$ $Principal$ $Components$ (PC) are the new set of variables that are obtained from the initial set of variables;\n",
    "\n",
    "$\\bullet$ Once we have computed the **singular values** and **singular vectors**, we order them in the **descending order**, <br>where the singular vector with the **highest singular value** is the most significant and forms the first principal components.\n",
    "\n",
    "$\\bullet$ The principal components of **lesser significances** can thus be **removed** in order to **reduce the dimensions** of the data.\n",
    "\n",
    "$\\bullet$ Thus we take first $k \\leq n$ columns of $U$ and consider new matrix $U_k$:\n",
    "\n",
    "$$U_k = \\{u_1, ..., u_k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 5: Reducing the dimensions of the data set</h3>\n",
    "\n",
    "$\\bullet$ The last step in performing PCA is to re-arrange the original data with the final principal components which represent the maximum and the most significant information of the data set. \n",
    "\n",
    "$\\bullet$ In order to replace the original data axis with the newly formed Principal Components, you simply multiply the transpose of the original data set by the transpose of the obtained feature vector.\n",
    "\n",
    "$\\bullet$ Thus newly formed Principal Components are:\n",
    "\n",
    "$$z_i = U_k^T y_i.$$\n",
    "\n",
    "Since $U_k^T \\in \\mathbb{R}^{k\\times n}$ and $y_i \\in \\mathbb{R}^{n}$, thus $z_i \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 6: Validation of the reconstricted data</h3>\n",
    "\n",
    "$\\bullet$ We can approximate the reconstruction of the original value by ${y_{i}}' = U_{k}z_{i}$ and compare it with the original value $y_i$:\n",
    "$$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left \\| y_{i} - {y_{i}}' \\right \\|^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left \\| y_{i} \\right \\|^{2}\n",
    "} \\leq \\epsilon$$\n",
    "where $\\epsilon$ is the $proportion \\text{ } of \\text{ } total \\text{ } variance$.\n",
    "<br>Using the next inequality:\n",
    "$$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\\left \\| y_{i} - {y_{i}}' \\right \\|^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\\left \\| y_{i} \\right \\|^{2}\n",
    "} \\leq 1 -\n",
    "\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "},$$\n",
    "we can write:\n",
    "$$\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "} \\geq \\epsilon$$\n",
    "$\\bullet$ It is common practice $\\epsilon = 70\\%$ is used to decide how many PCs should be retained."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
