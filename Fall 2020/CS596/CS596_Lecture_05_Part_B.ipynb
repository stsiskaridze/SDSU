{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **23 September 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 5 - Part B**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Bibliography:\n",
    "<br>\n",
    "[1] Bishop, Christopher M., *Pattern Recognition and Machine Learning*, Springer, 2006\n",
    "<br>\n",
    "[2] <a href=\"https://www.youtube.com/watch?v=slBI5YuVUTM\"> Logistic Regression - VISUALIZED!</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Logistic Regression Reminder</h3>\n",
    "\n",
    "- Logistic Regression is tipicly used for classification in Machine Leanring.\n",
    "\n",
    "\n",
    "- The **main difference** is that instead of assigning an input vector $\\mathbf{x}$ to one of the $K$ discrete classes $C_k$, for each class it gives the **probability** with which $\\mathbf{x}$ belongs to that class$C_k$:\n",
    "\n",
    "  $$P(C_k | \\mathbf{x}),$$\n",
    "  \n",
    "  where $k = 1, \\cdots, K$.\n",
    "\n",
    "\n",
    "  <center><img src=\"images/L5_Logistic_Regression.png\" width=\"700\" alt=\"Example\" /></center>\n",
    "\n",
    "- To do this, we need to **convert** a **real number** $\\mathbf{x}$ to the **value** in the **interval** $[0, 1]$ and this is where the **Sigmoid Function** comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Sigmoid Function</h3>\n",
    "\n",
    "- **Historical reference**. \n",
    "\n",
    "  - A **Sigmoid function** is a mathematical function whose name comes from the Greek letter **Sigma**.\n",
    "  \n",
    "  - A **Sigmoid function** has a characteristic **$S$-shaped curve** or **Sigmoid curve**.\n",
    "  \n",
    "  - A **Sigmoid function** is a type of **logistic function** and purely refers to any function that retains the $S$-shape. \n",
    "  \n",
    "  - Traditionaly sigmoidal function exists between $0$ and $1$.\n",
    "\n",
    "  \n",
    "  \n",
    "  <center><img src=\"images/L5_Sigmoid_Function.png\" width=\"700\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- **Sigmoid functions** have several useful properties:\n",
    "\n",
    "  1. $\\sigma(x)$ is **between 0 and 1**, i.e. $0 \\leq \\sigma(x) \\leq 1$ for any $x \\in (-\\infty , +\\infty )$;\n",
    "\n",
    "  2. $\\sigma(x)$ is **not linear**, i.e. $\\sigma \\left ( \\alpha x + \\beta y \\right ) \\neq \\alpha \\sigma \\left ( x \\right ) + \\beta \\sigma \\left ( y \\right )$;\n",
    "\n",
    "  3. $\\sigma(x)$ is **monotonic**, i.e. if $x < y$ then $\\sigma(x) < \\sigma(y)$;\n",
    "\n",
    "  4. **Derivative** of ${\\sigma}'(x)= \\frac{d}{dx}\\sigma(x)$ is **continuous** and it **can be expressed through** $\\sigma(x)$:\n",
    "  \n",
    "  $$\\sigma(x)' = \\sigma(x)(1-\\sigma(x)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">From Input Space to Probability Space</h3>\n",
    "\n",
    "- Input takes **only one value** $x \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "- We substitue this $x$ into the **Sigmoid Function**:\n",
    "\n",
    "  $$\\sigma(x) = \\frac{1}{1 + e^{-x}},$$\n",
    "  \n",
    "  and get the **probability** $\\sigma(x)\\in [0,1]$.\n",
    "\n",
    "  $$\n",
    "y = \n",
    "\\left\\{\\begin{matrix}\n",
    "1, & \\text{ } \\sigma(x) \\geq 0.5 \\\\\\ \n",
    "0, & \\text{ } \\sigma(x)  < 0.5\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "  where $y\\in\\{0,1\\}$ is **binary** response **variable**,\n",
    "  \n",
    "  i.e. if **probability** of datapoint being **positive class** $\\geq 50\\%$, then assign it as a **positive class**, **otherwise** it is a **negative class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">1-Dimensional Decision Boundary</h3>\n",
    "\n",
    "- In the context of logistic regression training, we could also write an equation:\n",
    "\n",
    "  $$\\sigma(b + wx) = \\frac{1}{1 + e^{-(b + wx)}},$$\n",
    "\n",
    "  where $b$ is a intercept **bias** term and $w$ **weight** of the independent variable $x$.\n",
    "  \n",
    "  \n",
    "- If we give a threshold of $50%$, then:\n",
    "\n",
    "  $$\\sigma(b + wx) = 0.5.$$\n",
    "  \n",
    "- Let's simplify this equation:\n",
    "\n",
    "  $$\\sigma(b + wx) = 0.5\\\\ \\Rightarrow 0.5 = \\frac{1}{1 + e^{-(b + wx)}} \\\\ \\Rightarrow\n",
    "  0.5 + 0.5\\cdot e^{-(b+wx)} = 1 \\\\ \\Rightarrow 1 + e^{-(b+wx)} = 2 \\\\ \\Rightarrow e^{-(b+wx)}=1 \\\\ \\Rightarrow b+wx = 0 \\\\ \\Rightarrow x = -\\frac{b}{w}.$$\n",
    "\n",
    "- I.e. $x = -\\frac{b}{w}$ is a **point decision boundary**.\n",
    "\n",
    "\n",
    "<center><img src=\"images/L5_Logistic_Regression_1D.png\" width=\"900\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- Implications: \n",
    "\n",
    "  $$P \\left ( y = 1 | x > -\\frac{b}{w} \\right ) > 0.5. $$\n",
    "  $$P \\left ( y = 1 | x = -\\frac{b}{w} \\right ) = 0.5. $$\n",
    "  $$P \\left ( y = 1 | x < -\\frac{b}{w} \\right ) < 0.5. $$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Training Phase for the 1-Dimensional Case</h3>\n",
    "\n",
    "- During the training we are going to find $w$ and $b$ that maximize probability of seeing traning data.\n",
    "\n",
    "\n",
    "- To do this, the **Gradient Descent** Algorithm is used:\n",
    "  - Initial points: \n",
    "    \n",
    "    $$w_0 =1, b_0 = 1$$\n",
    "    \n",
    "  - For iteration $m$ to $M$:\n",
    "  \n",
    "    $$w_m = w_{m-1} + \\alpha \\sum_{i=1}^{N} (y_i - \\sigma)x_i,$$\n",
    "    $$b_m = b_{m-1} + \\alpha \\sum_{i=1}^{N} (y_i - \\sigma),$$\n",
    "    \n",
    "    where $\\alpha$ is a **learning rat**e, $n$ is a **number of traning samples**,  $(x_i, y_i)$ is a **training sample**, and $\\sigma$ is **Sigmoid Function**.\n",
    "    \n",
    "    \n",
    "- As a result we get the **optimal values** $\\mathbf{w}_M$ and $b_M$ and **decision boundary** takes form: \n",
    "\n",
    "  $$b_M + \\mathbf{w}_M \\mathbf{x} = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Training Phase for the D-Dimensional Case</h3>\n",
    "\n",
    "- In case of $D$-dimensional case we have:\n",
    "  \n",
    "  $$y = \\sigma(b + \\mathbf{w}^T \\mathbf{x}).$$\n",
    "\n",
    "  - Initial points: \n",
    "    \n",
    "    $$w_0 =[1, ..., 1]^T, b_0 = 1.$$\n",
    "    \n",
    "  - For iteration $m$ to $M$:\n",
    "  \n",
    "    $$\\mathbf{w}_m = \\mathbf{w}_{m-1} + \\alpha \\sum_{i=1}^{N} (y_i - \\sigma)\\mathbf{x}_i,$$\n",
    "    $$\\mathbf{b}_m = \\mathbf{b}_{m-1} + \\alpha \\sum_{i=1}^{N} (y_i - \\sigma),$$\n",
    "    \n",
    "    where $\\alpha$ is a **learning rat**e, $n$ is a **number of traning samples**,  $(\\mathbf{x}_i, \\mathbf{y}_i)$ is a **training sample**, and $\\sigma$ is **Sigmoid Function**.\n",
    "    \n",
    "    \n",
    "- As a result we get the **optimal values** $w_M$ and $b_M$ and **decision boundary** takes form: \n",
    "\n",
    "  $$b_M + w_M x = 0.$$\n",
    "  \n",
    "  \n",
    "<center><img src=\"images/L5_Logistic_Regression_2D.png\" width=\"900\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Prior and Posterior Probabilities</h3>\n",
    "\n",
    "- Lets assume that we want to make inferences about quantities of the feature parameters $\\mathbf{w}$.\n",
    "\n",
    "\n",
    "- We capture our assumptions about $\\mathbf{w}$, **before observing the data**, in the form of a **prior probability** distribution $\\mathcal{p}(\\mathbf{w})$.\n",
    "\n",
    "\n",
    "- The effect of the observed data $\\mathcal{D} = \\{t_1, ..., t_n\\}$ is expressed through the conditional probability   $\\mathcal{p}(\\mathcal{D | \\mathbf{w}})$\n",
    "\n",
    "\n",
    "- Bayes’ theorem, which takes the form:\n",
    "  $$\\mathcal{p}(\\mathbf{w} | \\mathcal{D}) = \\frac{\\mathcal{p}(\\mathcal{D} | \\mathbf{w}) \\mathcal{p}(\\mathbf{w})}{\\mathcal{p}(\\mathcal{D})},$$\n",
    "\n",
    "  then allows us to evaluate the uncertainty in $\\mathbf{w}$ *after* we observe $\\mathcal{D}$ in the form of the posterior probability $\\mathcal{p}(\\mathbf{w} | \\mathcal{D})$.\n",
    "\n",
    "\n",
    "- The quantity $\\mathcal{p}(\\mathcal{D} | \\mathbf{w})$ is evaluated for the observed data set $\\mathcal{D}$ and can be viewed as a function of the parameter vector $\\mathbf{w}$, in which case it is called the **likelihood function**.\n",
    "\n",
    "\n",
    "- Given this definition of **likelihood**, we can state **Bayes’ theorem** in next words:\n",
    "\n",
    "  $$\\texttt{posterior} \\propto \\texttt{likelihood} \\times \\texttt{prior}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Linear Basis Function Models</h3>\n",
    "\n",
    "- The simplest linear model for regression is one that involves a **linear combination** of the input variables:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^{D} w_i x_i = w_0 + w_1 x_1 + ... + w_D x_D,$$\n",
    "\n",
    "  where $\\mathbf{x} = [x_1, ..., x_D]^{\\mathbf{T}}$. \n",
    "\n",
    "\n",
    "- The **key property** of this model is that it is a **linear function** of the parameters $w_0, ..., w_D$.\n",
    "\n",
    "\n",
    "- We can **extend** the class of models by considering **linear combinations of fixed nonlinear functions** of the input variables, of the form:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^{D} w_i \\phi_i(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}} \\mathbf{\\phi}(\\mathbf{x}),$$\n",
    "\n",
    "  where $\\phi_i(\\mathbf{x})$ are known as **basis functions**:\n",
    "  \n",
    "  $$\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ \\vdots \\\\w_{D}\n",
    "\\end{bmatrix}\n",
    " \\text{ and }\n",
    "\\mathbf{\\phi} = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0 \\\\ \\vdots \\\\ \\phi_{D}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probabilistic Generative Models</h3>\n",
    "\n",
    "-  Lets consider the case of two classes $\\mathcal{C}_1$ and $\\mathcal{C}_2$.\n",
    "\n",
    "  The **posterior probability** for class $\\mathcal{C}_1$ can be written as:\n",
    "  $$p(\\mathcal{C}_1 | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1) + p(\\mathbf{x} | \\mathcal{C}_2)p(\\mathcal{C}_2)}\n",
    "= \\frac{1}{1 + \\textrm{exp}(-a)} = \\sigma(a),$$\n",
    "  where we have defined:\n",
    "  $$a = \\ln \\frac{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\mathbf{x} | \\mathcal{C}_2)p(\\mathcal{C}_2)},$$\n",
    "  and $\\sigma(a)$ is the **logistic sigmoid function** defined by:\n",
    "  $$\\sigma(a) = \\frac{1}{1 + \\textrm{exp}(-a)}.$$\n",
    "  The **inverse** of the **logistic sigmoid** is given by:\n",
    "  $$a = \\ln \\left ( \\frac{\\sigma}{ 1 - \\sigma}\\right ),$$\n",
    "  and is known as the **logit function**.\n",
    "\n",
    "\n",
    "- For the case of $K > 2$ classes, we have:\n",
    "  $$p(\\mathcal{C}_k | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{j} p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}\n",
    "= \\frac{\\mathrm{exp}(a_k)}{\\sum_{j} \\mathrm{exp}(a_k)},$$\n",
    "  where quantities $a_k$ are defined by:\n",
    "  $$a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k),$$\n",
    "  and is known as the **softmax function**.\n",
    "  \n",
    "  If $a_k \\gg a_j$ for all $j \\neq k$, then $p(\\mathbf{x} | \\mathcal{C}_k) \\simeq 1$ and $p(\\mathbf{x} | \\mathcal{C}_j) \\simeq 0.$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Continuous Inputs</h3>\n",
    "   \n",
    "- Let us assume that the class-conditional densities are **Gaussian**:\n",
    "\n",
    "  $$p(\\mathbf{x} | \\mathcal{C}_k) = \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\mathrm{exp} \\left \\{ -\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\right \\}.$$\n",
    "  \n",
    "- For the case of two classes we have:\n",
    "\n",
    "  $$p(\\mathcal{C}_1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^{\\mathbf{T}}\\mathbf{x} + w_0).$$\n",
    "  where we have defined:\n",
    "  \n",
    "  $$\\mathbf{w}  =  \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu_1} - \\mathbf{\\mu_2}).$$\n",
    "  \n",
    "  $$w_0 = -\\frac{1}{2} \\mathbf{\\mu_1}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_1}  + \\frac{1}{2}\\mathbf{\\mu_2}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_2} + \\ln \\frac{p(\\mathcal{C}_1)}{p(\\mathcal{C}_1)}$$\n",
    "\n",
    "  We see that the **quadratic terms** in $\\mathbf{x}$ from the exponents of the Gaussian densities **have cancelled** leading to a **linear function** of $\\mathbf{x}$ in the argument of the **logistic sigmoid**.\n",
    "  \n",
    "\n",
    "- For the general case of $K$ classes we have:\n",
    "\n",
    "  $$a_k(\\mathbf{x}) = \\mathbf{w}_k^{\\mathbf{T}} \\mathbf{x} + w_{k_0}.$$\n",
    "  where we have defined:\n",
    "  \n",
    "   $$ \\mathbf{w}_k  =  \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_k$$\n",
    "   $$ w_{k_0} = -\\frac{1}{2} \\mathbf{\\mu_1}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_1} + \\ln p(\\mathcal{C}_k)$$ \n",
    "\n",
    "  We see that the $a_k(\\mathbf{x})$ are again linear functions of $\\mathbf{x}$ as a consequence of the cancellation of the quadratic terms due to the shared covariances.\n",
    "  \n",
    "  \n",
    "- The **resulting decision boundaries**, corresponding to the **minimum misclassification rate**, will occur when **two of the posterior probabilities are equal**, and so will be defined by **linear functions** of $\\mathbf{x}$, and so again we **have** a **generalized linear model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discrete Features</h3>\n",
    "   \n",
    "- Let us now consider the case of discrete feature values $x_i$.\n",
    "\n",
    "- For simplicity, we begin by looking at binary feature values $x_i \\in \\{0,1\\}$.\n",
    "\n",
    "- If there are $D$ inputs, then a general distribution would correspond to a table of $2D$ numbers for each class, containing $2D − 1$ independent variables.\n",
    "\n",
    "- Because this grows exponentially with the number of features, we might seek a more restricted representaSection:\n",
    "\n",
    "  $$p(\\mathbf{x} | \\mathcal{C}_k) = \\prod_{i=1}^{D} \\mu_{k_i}^{x_i} (1 - \\mu_{k_i})^{1 - x_i},$$\n",
    "  \n",
    "  which contains $D$ independent parameters for each class.\n",
    "  \n",
    "  Substituting into $a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)$ gives:\n",
    "  \n",
    "  $$a_k(\\mathbf{x}) = \\sum_{i=1}^{D}\\{ x_i \\ln \\mu_{k_i} + (1 - x_i) \\ln (1- \\mu_{k_i}) \\} + \\ln p(\\mathcal{C}_k),$$\n",
    "  which again are linear functions of the input values $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probabilistic Generative Models (Multiple Clases)</h3>\n",
    "   \n",
    "- For the case of $K > 2$ classes, we have:\n",
    "  $$p(\\mathcal{C}_k | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{j} p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}\n",
    "= \\frac{\\mathrm{exp}(a_k)}{\\sum_{j} \\mathrm{exp}(a_k)},$$\n",
    "  \n",
    "  where quantities $a_k$ are defined by:\n",
    "  \n",
    "  $$a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k),$$\n",
    "  \n",
    "  and is known as the **softmax function**.\n",
    "  \n",
    "  \n",
    "- If $a_k \\gg a_j$ for all $j \\neq k$, then $p(\\mathbf{x} | \\mathcal{C}_k) \\simeq 1$ and $p(\\mathbf{x} | \\mathcal{C}_j) \\simeq 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Logistic Regresion</h3>\n",
    "\n",
    "- Lets consider generalized linear models of **two-class classification**.\n",
    "\n",
    "  - The **posterior probability** of class $\\mathcal{C}_1$ can be written as a **logistic sigmoid** acting on a linear function of the feature vector $\\phi$ so that:\n",
    "\n",
    "    $$p(\\mathcal{C_1 | \\phi}) = y(\\phi) = \\sigma \\left ( \\mathbf{w}^{\\mathbf{T}}\\phi \\right ),$$\n",
    "    \n",
    "    and\n",
    "    \n",
    "    $$p(\\mathcal{C_2 | \\phi}) = 1 - p(\\mathcal{C_1 | \\phi}).$$\n",
    "\n",
    "\n",
    "- In the terminology of statistics, this model is known as **logistic regression**, although it should be emphasized that this is a **model for classification** rather than regression.\n",
    "\n",
    "\n",
    "- If feature space $\\phi$ is $M$-dimensional, then this model has $M$ adjustable parameters,\n",
    "<br>i.e. we have the **linear dependence** on $M$ of the number of parameters in **logistic regression**.\n",
    "\n",
    "\n",
    "- By contrast, if we had fitted Gaussian **class conditional densities** using **maximum likelihood**, we would have used $2M$ parameters for the **means** and $M(M+1)/2$ for **covariance matrix** (**symmetric!**), i.e overall $1 + M(M+5)/2$ parameters, which grows quadratically with $M$.\n",
    "\n",
    "\n",
    "- Now lets use **maximum likelihood** to determine the **parameters** of the **logistic regression** model.\n",
    "\n",
    "  For a data set $\\{ \\phi_n, t_n\\}$, where $t_n \\in {0, 1}$ and $\\phi_n = \\phi(\\mathbf{x}_n)$, with $n = 1, ..., N$, the likelihood function can be written:\n",
    "  \n",
    "  $$p(\\mathbf{t} | \\mathbf{w}) = \\prod_{n=1}^{N} y_n^{t_n}(1-y_n)^{1-t_n},$$\n",
    "  \n",
    "  where $\\mathbf{t} = (t_1, .., t_n)^{\\mathbf{T}}$ and $y_n = p(\\mathcal{C}_1 | \\phi_n).$\n",
    "\n",
    "  As usual, we can define an **error function** by taking the **negative logarithm of the likelihood**:\n",
    "  \n",
    "  $$E(\\mathbf{w}) = - \\ln p(\\mathbf{t} | \\mathbf{w}) = - \\sum_{n=1}^{N} (t_n \\ln y_n - (1 - t_n) \\ln(1 - y_n)),$$\n",
    "  \n",
    "  where $y_n = \\sigma(a_n)$ and $a_n = \\mathbf{w}^{\\mathbf{T}} \\phi_n.$\n",
    "  \n",
    "  Taking the gradient of the error function with respect to $\\mathbf{w}$, we obtain:\n",
    "  \n",
    "  $$\\nabla E(\\mathbf{w}) = \\sum_{n=1}^{N} (y_n - t_n) \\phi_n.$$\n",
    "\n",
    "  where we used the well known property of the logistic function: $\\frac{d\\sigma}{da} = \\sigma(1 - \\sigma).$\n",
    "  \n",
    "  \n",
    "- We see that the **factor involving the derivative** of the logistic sigmoid **has cancelled**, leading to a simplified form for the gradient of the log likelihood.\n",
    "\n",
    "- If desired, we could make use this result to give a **sequential algorithm** in which patterns are presented one at a time.\n",
    "\n",
    "\n",
    "- It is worth noting that maximum likelihood **can exhibit severe over-fitting** for data sets that are **linearly separable**.\n",
    "\n",
    "  This arises because the **maximum likelihood solution** **occurs** when the **hyperplane corresponding** to $\\sigma = 0.5$, equivalent to $\\mathbf{w}^{\\mathbf{T}} \\phi = 0$, separates the two classes and the magnitude of $\\mathbf{w}$ goes to infinity. In this case, the **logistic sigmoid function** becomes **infinitely steep in feature space**, corresponding to a **Heaviside step function**, so that every training point from each class $k$ is assigned a posterior probability $p(\\mathcal{C}_k |x) = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
