{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **2 September 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 2: Appendix A**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Appendix B</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Derivatives</h3>\n",
    "\n",
    "- Let $U \\subset \\mathbb{R}$ be a open subset and $f:U \\subset \\mathbb{R} \\to \\mathbb{R} $:\n",
    "\n",
    "\n",
    "- Function has a **derivative** at point $x_0 \\in \\mathbb{R}$ if for every $h \\in \\mathbb{R}$ there exists the limit of:\n",
    "\n",
    "  $$\\lim_{h \\to 0}\\frac{f(x_0 + h) - f(x_0)}{h}.$$\n",
    "\n",
    "\n",
    "- This limit point is called **derivate at point** $x_0$ and denoted by $f'(x_0)$.\n",
    "\n",
    "\n",
    "- A function is **differentiable** if it has derivative at any point $x\\in U$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/L2_Derivative.gif\" width=\"550\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Partial derivatives</h3>\n",
    "\n",
    "- Let $\\overrightarrow{f}:\\mathbb{R}^n \\to \\mathbb{R}^m$ be a vector function between two normed vector spaces.\n",
    "\n",
    "- The **partial derivative** of an function $\\overrightarrow{f} = f(x_1, \\dots, x_m$) in the direction $x_i$ at the point ($a_1, \\dots, a_n$) is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i}(a_1, \\ldots, a_n) = \\lim_{h \\to 0}\\frac{f(a_1, \\ldots, a_i+h,\\ldots,a_n) - f(a_1,\\ldots, a_i, \\dots,a_n)}{h}\n",
    "$$\n",
    "\n",
    "\n",
    "- All the variables are fixed except $x_i$. That choice of fixed values determines a function of one variable\n",
    "\n",
    "  $$f_{a_1,\\ldots,a_{i-1},a_{i+1},\\ldots,a_n}(x_i) = f(a_1,\\ldots,a_{i-1},x_i,a_{i+1},\\ldots,a_n)$$\n",
    "\n",
    " and by definition,\n",
    "\n",
    " $$\\frac{df_{a_1,\\ldots,a_{i-1},a_{i+1},\\ldots,a_n}}{dx_i}(a_i) = \\frac{\\partial f}{\\partial x_i}(a_1,\\ldots,a_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Jacobian matrix</h3>\n",
    "\n",
    "- Let $\\overrightarrow{f} = \\{f_1,  ..., f_m\\}:\\mathbb{R}^n \\to \\mathbb{R}^m$ be a function such that each of its partial derivatives exist on $\\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "- This function takes a vector $\\overrightarrow{x}\\in \\mathbb{R}^n$ as input and produces the vector $\\overrightarrow{f}(\\overrightarrow{x})\\in \\mathbb{R}^m$ as an output.\n",
    "\n",
    "\n",
    "- The Jacobian matrix of $\\overrightarrow{f}$ is defined to be an $m \\times n$ matrix, denoted by $\\mathbf{J}$, whose $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{ \\partial x_j}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{J} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\overrightarrow{f}}{\\partial x_1} & \\cdots  &  \\frac{\\partial \\overrightarrow{f}}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \n",
    "\\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient</h3>\n",
    "\n",
    "- Let $f :\\mathbb{R}^n \\to \\mathbb{R}$ be a function such that each of its partial derivatives exist on $\\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "- The **gradient** of the function $f$, denoted by $\\nabla f$, is a **vector of it's Jacobian matrix**:\n",
    "\n",
    "$$\n",
    "\\nabla f = \\mathbf{J} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\cdots  &  \\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<center><img src=\"images/L2_Gradient.gif\" width=\"300\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Global Maximum and Minimum</h3>\n",
    "\n",
    "- A funciton $f$ on a metrix space $X$ has a **global**, or **absolute**, **maximum** at point $x\\in X$, if:\n",
    "\n",
    "$$f(x) \\geq f({x}') \\text{ for any } {x}' \\in X.$$\n",
    "\n",
    "\n",
    "- A funciton $f$ on a metrix space $X$ has a **global**, or **absolute**, **minimum** at point $x\\in X$, if:\n",
    "\n",
    "$$f(x) \\leq f({x}') \\text{ for any } {x}' \\in X.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h3 align=\"center\">Local Maximum and Minimum</h3>\n",
    "\n",
    "\n",
    "- A funciton $f$ on a metrix space $X$ has a **local**, or **relative**, **maximum** at point $x\\in X$, if there exists an open neighborhood of $x$, call it $U_x$, such that:\n",
    "\n",
    "  $$f(x) \\geq f({x}') \\text{ for any } {x}' \\in U.$$\n",
    "\n",
    "\n",
    "- A funciton $f$ on a metrix space $X$ has a **local**, or **relative**, **minimum** at point $x\\in X$, if there exists an open neighborhood of $x$, call it $U_x$, such that:\n",
    "\n",
    "  $$f(x) \\leq f({x}') \\text{ for any } {x}' \\in U.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Fermat's theorem</h3>\n",
    "\n",
    "- Let $U$ be an open subset of $\\mathbb{R}^n$ and suppose that function $f: U \\to \\mathbb{R}$ is **differentiable** at point $x \\in U$. \n",
    "\n",
    "- $\\textbf{Fermat's theorem}$ states that if $f$ has a **local extremum** (**maximum** or **minimum**) in $x \\in U$, then: \n",
    "\n",
    "$$\\nabla f(x) = 0.$$\n",
    "\n",
    "<center><img src=\"images/L2_Extrema.jpg\" width=\"1500\"  alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Hessian  matrix</h3>\n",
    "\n",
    "- Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be a function such that all **second partial derivatives** exist on $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "- This function takes a vector $\\overrightarrow{x}\\in \\mathbb{R}^n$ as input and produces a scalar $f(\\overrightarrow{x})\\in \\mathbb{R}$ as an output.\n",
    "\n",
    "\n",
    "- The **Hessian matrix** of $f$ is a square $n \\times n$ matrix, denoted by $\\mathbf{H}$, whose $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{ \\partial x_i \\partial x_j}$:\n",
    "\n",
    "  $$\\mathbf{H} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "- If the **Hessian matrix**:\n",
    "  - is **positive definite** at $x$, then $f$ attains an isolated **local minimum** at $x$\n",
    "  - is **negative definite** at $x$, then $f$ attains an isolated **local maximum** at $x$. \n",
    "  - has **both positive and negative eigenvalues** then $x$ is a **saddle point** for $f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient Descent Method</h3>\n",
    "\n",
    "- **Gradient descent** is:\n",
    "  - a first-order **iterative optimization algorithm** for finding the **local minimum** of a differentiable function.\n",
    "  - based on the observation that $f(x)$ decreases **fastest** if one goes in the direction of a **negative gradient.**\n",
    "\n",
    "\n",
    "- It **starts with a guess** $x_0$  for a local minumum of $f$, and consider the **sequence** $x_0, x_1, x_2 ...$  such that:\n",
    "\n",
    "  $$x_{n+1} = x_n  - \\gamma \\nabla f(x_n),$$\n",
    "  \n",
    "  where $\\gamma$ is a **step size** and is allowed to change at every iteration.  \n",
    "\n",
    "<center><img src=\"images/L2_Gradient_Descent_2.png\" width=\"650\" alt=\"Example\" /></center>\n",
    "\n",
    "<center><img src=\"images/L2_Gradient_Descent.gif\" width=\"550\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient Descent Learning Rate Problems</h3>\n",
    "\n",
    "- If we **stack in local extrema** or saddle point, we can **stay there for long**;\n",
    "\n",
    "- **Large learning rate** might cause so called **bouncing gradient**.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/L2_GD_rate.png\" width=\"1000\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Optimization Methods for Gradient Descent</h3>\n",
    "\n",
    "- The following two animations provide some intuitions towards the behaviour of the optimization methods.\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/L2_GD_Optimizers.gif\" width=\"1000\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Least Square Error</h3>\n",
    "\n",
    "\n",
    "- One way of measuring the **performance of the model** is to compute the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\| \\mathbf{e}\\|^2 = \\|\\mathbf{y} - \\hat{\\mathbf{y}} \\|^2.$$\n",
    "\n",
    "- So our task is to find $\\hat{\\boldsymbol{\\theta}}$:\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}).$$\n",
    "\n",
    "- Let's expand $J(\\boldsymbol{\\theta})$:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\| \\mathbf{y} - \\hat{\\mathbf{y}} \\|^2 = (\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\mathbf{y} - X\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\theta}^TX^T\\mathbf{y} + \\boldsymbol{\\theta}^TX^TX\\boldsymbol{\\theta}.$$\n",
    "\n",
    "- Use **Fermat's Theorem** to find the minimum:\n",
    "  \n",
    "  $$\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = -2X^T\\mathbf{y} + 2X^TX\\boldsymbol{\\theta} = 0 \\Rightarrow X^TX\\boldsymbol{\\theta} = X^T \\mathbf{y} \\Rightarrow \\hat{\\boldsymbol{\\theta}} = (X^TX)^{-1}X^T\\mathbf{y} = X^{+}\\mathbf{y},$$\n",
    "\n",
    "  where $X^{+} = (X^TX)^{-1}X^T$ is the **Pseudoinverse** of $X$.\n",
    "\n",
    "\n",
    "- It also happens **Pseudoinverse** is the best solution in terms of **Least Squares Error**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}