{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **2 November 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 9**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Sources:\n",
    "Bibliography: \n",
    "<br>[1] **Chapter 13**. Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006.\n",
    "<br>[2] https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe\n",
    "<br>[3] https://medium.com/@annikabrundyn1/the-beginners-guide-to-recurrent-neural-networks-and-text-generation-44a70c34067f\n",
    "<br>[4] https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "<br>[5] https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "<br>[6] https://www.youtube.com/watch?v=QciIcRxJvsM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Recurrent Neural Networks (RNN)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L9_Siri.jpg\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "- You asked Siri about the **weather today**, and it brilliantly resolved your queries.\n",
    "\n",
    "\n",
    "- But, **how did it happen**? How it **converted your speech to the text** and fed it to the search engine?\n",
    "\n",
    "\n",
    "- This is the magic of **Recurrent Neural Networks** (**RNN**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">What is RNN?</h3>\n",
    "\n",
    "- RNNs **generalise** feedforward networks (FFNs) to be able to model **sequential data**.\n",
    "\n",
    "\n",
    "- **FFNs** take an **input** (e.g. an image) and immediately **produce an output** (e.g. probabilities of different classes). \n",
    "\n",
    "\n",
    "- **RNNs**, on the other hand, **consider the data sequentially**, and **can remember** what they have seen earlier in the sequence to help interpret elements from later in the **sequence**.\n",
    "\n",
    "\n",
    "- For example, imagine we want to **label words** as the **part-of-speech categories** that they belong to.\n",
    "\n",
    "  I.e. for the **input sentence** `I would like the duck` and `He had to duck`, our model **should predict** that **duck** is a `Noun` in the **first sentence** and a `Verb` in the **second**. \n",
    "  \n",
    "  To do this successfully, the **model needs to be aware of the surrounding context**. \n",
    "  \n",
    "  However, if we **feed a FFN model** only **one word at a time**, how could it **know the difference**? \n",
    "  \n",
    "  If we want to feed it all the words at once, **how do we deal** with the fact that **sentences are of different lengths**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Where RNN is Used?</h3>\n",
    "\n",
    "- **Sequence data** comes in **many** other **forms**:\n",
    "\n",
    "  - **Audio** (a natural sequence of audiograms);\n",
    "  - **Stock market prices** (numerical time series);\n",
    "  - **Genomes**;\n",
    "  - **Videos** (sequence of images)\n",
    "  \n",
    "  \n",
    " - **RNNs** can operate over sequences of vectors in both the input and the output.\n",
    " \n",
    " \n",
    " - The **many forms** of **sequence prediction problems** are probably best **described by** the types of **inputs** and **outputs** supported:\n",
    " \n",
    "   1. **One-to-one**: Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. **image classification**).\n",
    "  \n",
    "   2. **One-to-many**: Sequence output (e.g. **image captioning** takes an image and outputs a sentence of words).\n",
    "\n",
    "   3. **Many-to-one**: Sequence input (e.g. **sentiment analysis** where a given sentence is classified as expressing positive or negative sentiment or given some text predict the next character)\n",
    "\n",
    "   4. **Many-to-many**: Sequence input and sequence output (e.g. **Machine Translation**: an RNN reads a sentence in English and then outputs a sentence in French).\n",
    "\n",
    "   5. **Many-to-many**: Synced sequence input and output (e.g. **video classification** where we wish to label each frame of the video).\n",
    "   \n",
    "   <img src=\"images/L9_RNN_IO.jpeg\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Sequential Data</h3>\n",
    "\n",
    "- **Sequential Data** is any kind of data where the **order matters**.\n",
    "\n",
    "\n",
    "- There are **two types** of **sequential distributions**:\n",
    "  - **stationary sequential distributions**, when the data evolves in time, but the distribution from which it is generated remains the same.\n",
    "  - **nonstationary sequential distributions**, when the generative distribution itself is evolving with time.\n",
    "\n",
    "\n",
    "- We shall focus on the stationary case only.\n",
    "  \n",
    "  <img src=\"images/L9_Sequential_Data.png\" width=\"400\" alt=\"Example\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Markov Models</h3>\n",
    "\n",
    "- The **easiest way** to treat **sequential data** would be simply to ignore the sequential aspects and **treat the observations** as **i.i.d.**.\n",
    "\n",
    "  However, would **fail to exploit the sequential patterns in the data**, such as correlations between observations that are close in the sequence.\n",
    "\n",
    "\n",
    "- For example, suppose we observe a **binary variable** denoting whether on a particular day it **rained or not**.\n",
    "\n",
    "  Given a time series of recent observations of this variable, we **wish to predict whether it will rain on the next day**.\n",
    "  \n",
    "  If we **treat the data as i.i.d.**, then the **only information** we can glean from the data is the **relative frequency of rainy days**.\n",
    "  \n",
    "  However, we know in practice that the weather often exhibits trends that may last for several days.\n",
    "  \n",
    "  Observing whether or not it rains today is therefore of significant help in predicting if it will rain tomorrow.\n",
    "  \n",
    "  \n",
    "- To **express such effects** in a probabilistic model is to consider a **Markov model**.\n",
    "\n",
    "\n",
    "- Lets use he product rule to express the joint distribution for a sequence of observations in the form:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = \\prod_{n=1}^{N} p(\\mathbf{x}_n | \\mathbf{x}_1, ..., \\mathbf{x}_{n-1}).$$\n",
    "  \n",
    "\n",
    "- If we now assume that **each of the conditional distributions** on the right-hand side\n",
    "is **independent of all previous observations except the most recent**, we obtain the **first-order Markov chain**:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = p(\\mathbf{x}_1) \\prod_{n=2}^{N} p(\\mathbf{x}_n | \\mathbf{x}_{n-1}).$$\n",
    "\n",
    "  <img src=\"images/L9_MC1.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "  If the observations are **discrete variables having $K$ states**, then the conditional distribution $p(\\mathbf{x}_n| \\mathbf{x}_{n-1})$ in a **first-order Markov chain** will be specified by a set of $K − 1$ parameter for each of the $K$ state of $\\mathbf{x}_{n-1}$ giving a **total of $K(K − 1)$ parameters**.\n",
    "  \n",
    "\n",
    "- If we allow the **predictions** to **depend also on the previous-but-one value**, we obtain a **second-order Markov chain**:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = p(\\mathbf{x}_1) p(\\mathbf{x}_2| \\mathbf{x}_1) \\prod_{n=3}^{N} p(\\mathbf{x}_n | \\mathbf{x}_{n-1}, \\mathbf{x}_{n-2}).$$\n",
    "\n",
    "  <img src=\"images/L9_MC2.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- We can similarly **consider** extensions to an **$M^{th}$ order Markov chain** in which the **conditional distribution** for a particular variable **depends on the previous $M$ variables**.\n",
    "\n",
    "  If the observations are **discrete variables having $K$ states**, then the conditional distribution $p(\\mathbf{x}_n| \\mathbf{x}_{n-M, ..., \\mathbf{x}_{n-1}})$ in am **$M^{th}$-order Markov chain** will be specified by a **total of** $K^{M-1}(K − 1)$ **parameter**.\n",
    " \n",
    "\n",
    "\n",
    "- **Note**, that we have **paid a price** for this increased flexibility because the **number of parameters** in the model **grows exponentially** with $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">State Space Model</h3>\n",
    "\n",
    "- Suppose we **wish to build a model** for sequences that is **not limited by the Markov assumption** to **any order** and yet that **can be specified** using a **limited number of free parameters**.\n",
    "\n",
    "\n",
    "- We can achieve this by **introducing additional latent variables** to permit a rich class of models to be constructed out of simple components.\n",
    "\n",
    "\n",
    "- For each **observation** $\\mathbf{x}_n$, we introduce a corresponding **latent variable** $\\mathbf{z}_n$.\n",
    "\n",
    "\n",
    "- Assume that it is the **latent variables*- $\\mathbf{z}_m$ that **form a Markov chain**, giving rise to the graphical structure known as a **State Space Model**.\n",
    "\n",
    "\n",
    "- The **joint distribution** for this **model** is given by:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N, \\mathbf{z}_1, ..., \\mathbf{z}_N ) = p(\\mathbf{z}_1) \\left [ \\prod_{n=2}^{N} p(\\mathbf{z}_n| \\mathbf{z}_{n-1}) \\right ]  \\prod_{n=1}^{N} p(\\mathbf{x}_n | \\mathbf{z}_n).$$\n",
    "  \n",
    "  <img src=\"images/L9_SSM.png\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- There are **two important models** for sequential data that are described by this graph:\n",
    "  - If the **latent variables are discrete**, then we obtain the **Hidden Markov Model (HMM)**;\n",
    "  - If **both the latent** and the **observed variables are Gaussian**, then we obtain the **Linear Dynamical System**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Sequential Memory</h3>\n",
    "\n",
    "- Ok so, **RNN’s** are **good at processing sequence data** for predictions. **But how**??\n",
    "\n",
    "\n",
    "- Well, they do that by having a concept commonly called **sequential memory**. \n",
    "\n",
    "\n",
    "- To get a good intuition behind what sequential memory means, let's try to **say the alphabet** in your head.\n",
    "\n",
    "  <img src=\"images/L9_ABC.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "  That was pretty easy right. If you were **taught this specific sequence**, it should come quickly to you.\n",
    "\n",
    "  Now try **saying the alphabet backward**:\n",
    "  \n",
    "  <img src=\"images/L9_CBA.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "  Much harder, isn't it?. Unless you’ve practiced this specific sequence before, you’ll likely have a hard time.\n",
    "\n",
    "  Here’s a fun one, **start at the letter F**.\n",
    "\n",
    "  <img src=\"images/L9_F.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "  At first, you’ll struggle with the first few letters, but then after your brain picks up the pattern, the rest will come naturally.\n",
    "  \n",
    "  \n",
    "- **Sequential memory** is a **mechanism** that makes it easier for your brain to **recognize sequence patterns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Recurrent Neural Networks</h3>\n",
    "\n",
    "- Alright, **RNN**’s have this abstract concept of sequential memory, but **how does** an **RNN replicate this concept**?\n",
    "\n",
    "\n",
    "- Well, let’s look at a traditional neural network also known as a **Feed-Forward Neural Network** (FFN).\n",
    "\n",
    "  <img src=\"images/L9_FFN.png\" width=\"150\" alt=\"Example\" />\n",
    "  \n",
    "- How do we get a **FFN** network to be able to **use previous information** to effect later ones? \n",
    "\n",
    "  What if we **add a loop** in the neural network that can **pass prior information forward**?\n",
    "\n",
    "  And that’s essentially what a **recurrent neural network does**. \n",
    "  \n",
    "  <img src=\"images/L9_RNN.png\" width=\"130\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- An **RNN** has a **looping mechanism** that acts as a **highway** to allow information to **flow from one step to the next**.\n",
    "\n",
    "  This information is the **hidden state**, which is a **representation of previous inputs**. \n",
    "  \n",
    "  <img src=\"images/L9_HRNN.gif\" width=\"300\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">What time is it?</h3>\n",
    "\n",
    "- Let’s say we want to **build a chatbot**.\n",
    "\n",
    "\n",
    "- The **chatbot** can **classify intentions** from the **users inputted text**.\n",
    "\n",
    "\n",
    "- First, we are going to **encode the sequence of text using an RNN**. \n",
    "\n",
    "\n",
    "- Then, we are going to **feed the RNN output** into a **feed-forward neural network** which will **classify the intents**.\n",
    "\n",
    "\n",
    "- Let's assuem, that **user types** in: **what time is it?**.\n",
    "\n",
    "\n",
    "- **To start**, we **break up the sentence into individual words**:\n",
    "\n",
    "  <img src=\"images/L9_What.gif\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "- The **first step** is to feed **What** into the **RNN**. \n",
    "\n",
    "\n",
    "- The RNN encodes **What** and produces an **output**.\n",
    "  \n",
    "  <img src=\"images/L9_What2.gif\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "- For the **next step**, we feed the word **time** and the **hidden state** from the **previous step**. \n",
    "\n",
    "\n",
    "- The **RNN* now **has information** on both the word **What** and **time**.\n",
    "    \n",
    "  <img src=\"images/L9_What3.gif\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "- We **repeat this process, until the final step**. \n",
    "\n",
    "\n",
    "- You can see **by the final step** the **RNN has encoded information** from **all the words** in **previous steps**.\n",
    "      \n",
    "  <img src=\"images/L9_What4.gif\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "- Since the **final output** was created from the rest of the sequence, we should be able to **take the final output** and **pass it** to the **feed-forward layer** to classify an intent.\n",
    "        \n",
    "  <img src=\"images/L9_What5.gif\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- For those **who like looking at code** here is some **Python pseodocoude** showcasing the control flow.\n",
    "\n",
    "  <img src=\"images/L9_What6.png\" width=\"600\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Vanishing Gradients Problem</h3>\n",
    "\n",
    "- **How long can the sequence be**?\n",
    "\n",
    "  Theoreticly, they **can be infinite**, but we **run into a problem**. \n",
    "\n",
    "\n",
    "- Let's consider a simple **example of RNN** with **no hidden units** but with a **recurrence on some scalar** $x^{(0)}$.\n",
    "\n",
    "\n",
    "  <img src=\"images/L9_Sequence.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- After $n$ times units its value would be $x^{(n)}$ so we can consider a discrate dynamical system:\n",
    "\n",
    " $$x(n) = W^n x^{(0)},$$\n",
    " \n",
    " where $W \\in \\mathbb{R}$ and $x^{(i)} \\in \\mathbb{R}$ for $i \\in [0,n]$.\n",
    " \n",
    " \n",
    "- Now, if $W$ is **slighter greater than one**, then $W^{n}x^{(0)}$ would **explode**, and if $W$ is **slighter less than one**, then $W^{n}x^{(0)}$ would **vanish**:\n",
    "\n",
    "  $$W^{n}x^{(0)} \\rightarrow\n",
    "  \\left\\{\\begin{matrix}\n",
    "  \\infty, & \\text { if } W > 1,\\\\\n",
    "  0, &\\text { if } W < 1.\\\\\n",
    "  \\end{matrix}\\right.$$\n",
    "  \n",
    "\n",
    "- Because the **forward propagated** values **explode** or vanish**, the **same will happen** to its **gradients** in **backpropagation**:\n",
    "\n",
    "  $$\\frac{\\delta W^{n}x^{(0)}}{\\delta W} \\rightarrow\n",
    "  \\left\\{\\begin{matrix}\n",
    "  \\infty, & \\text { if } W > 1,\\\\\n",
    "  0, &\\text { if } W < 1.\\\\\n",
    "  \\end{matrix}\\right.$$\n",
    "  \n",
    "- This can be **generalized to matrices** as well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Vanishing Gradients Problem for RNN</h3>\n",
    "\n",
    "- The **vanishing** and **exploiding gradients** is **much worse** in **RNN** that it is for traditional **DNN**.\n",
    "\n",
    "\n",
    "- This is because **DNN** have **different wighted matrices** between layers.\n",
    "\n",
    "\n",
    "- Thus, if the **weights** between the **first two layers** are **grater than one** ($>1$), then the **next layer** can have **matrix wights** which are **less than one** and so their effect would **cancel each other out**.\n",
    "\n",
    "  <img src=\"images/L9_Layers.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- But in the sace of **RNN**, the **same weight parameters** occurs between different recurrent units.\n",
    "\n",
    "- So its **more of a problem**  because we **cannot cancel out**.\n",
    "\n",
    "  <img src=\"images/L9_Layers2.png\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Ways of dealing with Vanishing Gradients Problem for RNN</h3>\n",
    "\n",
    "\n",
    "- There are some ways of dealing with this problem of **vanishing** and **exploiding gradients**:\n",
    "\n",
    "  1. **Introduce Skip Connections**.\n",
    "  \n",
    "     We can add additional adges called **skip connections** tp connect States come $d$ neuron in front of it.\n",
    "     \n",
    "     So the current state is influenced by the previouse state and a state that accured $d$ times step ago.\n",
    "     \n",
    "     Gradint will now explode or vanish as a function of $\\frac{\\tau}{d}$ instead of just a function of $\\tau$.  \n",
    "     \n",
    "     <img src=\"images/L9_Layers3.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "  2. **Replace Length 1 Connections**.\n",
    "  \n",
    "     We can actively remove connections of length one and replace them with longer connections.\n",
    "     \n",
    "     This force the network to learn alogn this modified path. \n",
    "\n",
    "     <img src=\"images/L9_Layers4.png\" width=\"900\" alt=\"Example\" />\n",
    "     \n",
    "  3. **Leaky Recurrent Units**.\n",
    "  \n",
    "     Let's consider the **vanilla RNN** but this time append a constant $\\alpha$ over every edge joining the adjacent hidden units.\n",
    "     \n",
    "     Thus $\\alpha$ can regulate the amount of information the network remembers over time.\n",
    "     \n",
    "     If $\\alpha \\approx 1$ more memory is retained. If $\\alpha \\approx 0$ the memory of previous states it forgets.\n",
    "    \n",
    "     <img src=\"images/L9_Layers5.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "  \n",
    "  4. **Gated Recurrent Networks**.\n",
    "  \n",
    "     A **modification** of the **leaky hidden units** is the **Gated Reccurent Networks**.\n",
    "     \n",
    "     Instead of **manually assigning** a constant **value** $\\alpha$ to determine what to retain, we **introduce a set of parameters** one for every time step.\n",
    "     \n",
    "     So we **leave it up to the network** to decide **what to remember** and **what to forget** by introducing new parameters that act as gates. \n",
    "     \n",
    "     <img src=\"images/L9_Layers6.png\" width=\"900\" alt=\"Example\" />\n",
    "\n",
    "  5. **LSTM**.\n",
    "  \n",
    "     One of the most commonly used gated RNN architectures is **Long Short Term Memory** (**LSTM**).\n",
    "     \n",
    "     Take **vinalla RNN** and **replace all hidden units** with something called an **LSTM Cell** and **add another connection** from every cell called the **cell states**.\n",
    "     \n",
    "     <img src=\"images/L9_Layers7.png\" width=\"900\" alt=\"Example\" />\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">LSTM</h3>\n",
    "\n",
    "- An **LSTM** has a **similar control flow** as a **RNN**. \n",
    "\n",
    "\n",
    "- It **processes data** passing on information as it **propagates forward**. \n",
    "\n",
    "\n",
    "- The **differences** are the operations within the **LSTM’s cells** and it’s **various gates**.\n",
    "\n",
    "\n",
    "- These **operations** are **used to allow** the **LSTM** to **keep** or **forget information**.\n",
    "\n",
    "\n",
    "- The **cell state** act as a **transport highway** that **transfers relative information** all the way down the sequence chain.\n",
    "    \n",
    "\n",
    "- The **cell state**, in theory, can **carry relevant information throughout the processing of the sequence**.\n",
    "  \n",
    "\n",
    "- So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. \n",
    "  \n",
    "\n",
    "- As the cell state goes on its journey, **information** get’s **added** or **removed** to the cell state **via gates**. \n",
    "\n",
    "\n",
    "- The **gates** are different neural networks that **decide which information is allowed on the cell state**. \n",
    "\n",
    "\n",
    "- The **gates** can learn what **information** is **relevant to keep** or **forget** during training.\n",
    "\n",
    "  <img src=\"images/L9_LSTM.png\" width=\"800\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">LSTM</h3>\n",
    "\n",
    "- An **LSTM** has a **similar control flow** as a **RNN**. \n",
    "\n",
    "\n",
    "- It **processes data** passing on information as it **propagates forward**. \n",
    "\n",
    "\n",
    "- The **differences** are the operations within the **LSTM’s cells** and it’s **various gates**.\n",
    "\n",
    "\n",
    "- These **operations** are **used to allow** the **LSTM** to **keep** or **forget information**.\n",
    "\n",
    "\n",
    "- The **cell state** act as a **transport highway** that **transfers relative information** all the way down the sequence chain.\n",
    "    \n",
    "\n",
    "- The **cell state**, in theory, can **carry relevant information throughout the processing of the sequence**.\n",
    "  \n",
    "\n",
    "- So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. \n",
    "  \n",
    "\n",
    "- As the cell state goes on its journey, **information** get’s **added** or **removed** to the cell state **via gates**. \n",
    "\n",
    "\n",
    "- The **gates** are different neural networks that **decide which information is allowed on the cell state**. \n",
    "\n",
    "\n",
    "- The **gates** can learn what **information** is **relevant to keep** or **forget** during training.\n",
    "\n",
    "  <img src=\"images/L9_LSTM.png\" width=\"800\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Gate Activation Function</h3>\n",
    "\n",
    "- **Gates** may contain **Sigmoid activations function** that squishes values between $0$ and $1$.\n",
    "\n",
    "\n",
    "- That is **helpful** to **update** or **forget data** because any number getting multiplied by $0$ is $0$, causing values to disappears or be **forgotten**.\n",
    "\n",
    "\n",
    "- Any number multiplied by $1$ is the same value therefore that value stay’s the same or is **kept**. \n",
    "\n",
    "\n",
    "  <img src=\"images/L9_Sigmoid.gif\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- **Gates** may also contain **Tanh activation function** that squishes values to always be between $-1$ and $1$.\n",
    "\n",
    "\n",
    "- The **tanh activation function** is **used** to help **regulate** the **values** flowing through the network.\n",
    "\n",
    "  <img src=\"images/L9_Tanh.gif\" width=\"800\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    " \n",
    "- Suppose the **value** is **multiplied by 3** every time it **passes** through the **Hidden layer**. \n",
    "\n",
    "  As we can see below, the **values can explode** and become astronomical, **causing other values to seem insignificant**:\n",
    "\n",
    "  <img src=\"images/L9_Tanh2.gif\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- A **tanh function ensures** that the **values stay between** $-1$ and $1$, thus **regulating** the output of the **neural network**. \n",
    "\n",
    "  <img src=\"images/L9_Tanh3.gif\" width=\"800\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Forget Gate Layer</h3>\n",
    "\n",
    "- First, we have the **Forget Gate Layer**. \n",
    "\n",
    "\n",
    "- This gate **decides what information** should be **thrown** away or **kept**. \n",
    "\n",
    "\n",
    "- **Information** from the **previous hidden state** ($h_{t-1}$) and **information** from the **current input** ($x_t$) is **passed** through the **sigmoid function**:\n",
    "\n",
    "  $$f_t = \\sigma(W_f \\cdot [h_{t-1},  x_t] + b_f).$$\n",
    "\n",
    "\n",
    "- **Values** come out **between** $0$ and $1$. \n",
    "\n",
    "\n",
    "- The **closer to** $0$ means to **forget**, and the **closer to** $1$ means to **keep**.\n",
    "\n",
    "\n",
    "  <img src=\"images/L9_Forget_Gate.gif\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Input Gate Layer</h3>\n",
    "\n",
    "- To update the cell state, we have the **Input Gate Layer**. \n",
    "\n",
    "\n",
    "- First, we **pass** the **previous hidden state** ($h_{t-1}$) and **current input** ($x_t$) into a **sigmoid function**. \n",
    "\n",
    "\n",
    "- That **decides** which **values** will be **updated** by transforming the values to be between $0$ and $1$.\n",
    "\n",
    "\n",
    "- $0$ means **not important**, and $1$ means **important**. \n",
    "\n",
    "\n",
    "- We also **pass** the **previous hidden state**  ($h_{t-1}$) and **current input** ($x_t$) into the **tanh activation function** to squish values between $-1$ and $1$.\n",
    "\n",
    "\n",
    "- Then we **multiply** the **tanh output** with the **sigmoid output**, which **decide which information** is **important to keep from** the **tanh output**:\n",
    "\n",
    "\n",
    "  $$\\begin{matrix}\n",
    "  i_t = & \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i),\\\\ \n",
    "  \\tilde{C_t} = & \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "  \\end{matrix}\n",
    "   $$\n",
    "\n",
    "\n",
    "  <img src=\"images/L9_Input_Gate.gif\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Cell State</h3>\n",
    "\n",
    "- Now we should have **enough information** to **calculate** the **Cell State**. \n",
    "\n",
    "\n",
    "- First, the **cell state** gets **pointwise multiplied** by the **forget vector**. \n",
    "\n",
    "\n",
    "- Then we **take the output** from the **input gate** and do a **pointwise addition** which **updates** the **cell state** to **new values**:\n",
    "\n",
    "  $$C_t =  f_t \\times C_{t-1} + i_t \\times \\tilde{C_t}.$$\n",
    "\n",
    "\n",
    "  <img src=\"images/L9_Cell_Gate.gif\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Output Gate Layer</h3>\n",
    "\n",
    "- Last we have the **Output Gate Layer**.\n",
    "\n",
    "\n",
    "- The **output gate decides** what the **next hidden state should be**. Remember that the **hidden state contains information** on **previous inputs**. \n",
    "\n",
    "\n",
    "- The **hidden state** is also **used for predictions**. \n",
    "\n",
    "\n",
    "- First, we **pass** the **previous hidden state** ($h_{t-1}$) and the **current input** ($x_t$) into a **sigmoid function**. \n",
    "\n",
    "\n",
    "- Then we **pass** the **newly modified cell state** ($C_t$) to the **tanh function**. \n",
    "\n",
    "\n",
    "- Finally, we **multiply** the **tanh output** with the **sigmoid output** to **decide** what information the **hidden state** should carry. \n",
    "\n",
    "  $$\\begin{matrix}\n",
    "  o_t = & \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o),\\\\ \n",
    "  h_t = & o_t \\times \\tanh(C_t).\n",
    "  \\end{matrix}\n",
    "   $$\n",
    "\n",
    "- The **output** is the **new hidden state** ($h_t$). \n",
    "\n",
    "\n",
    "- The **new cell state** ($C_t$) and the **new hidden state** ($h_t$) is then **carried over to the next time step**.\n",
    "\n",
    "  <img src=\"images/L9_Output_Gate.gif\" width=\"800\" alt=\"Example\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">GRU</h3>\n",
    "\n",
    "- The **GRU** is the **newer generation** of **RNN** and is **pretty similar** to an **LSTM**. \n",
    "\n",
    "- **GRU**’s got rid of the **cell state** and used the **hidden state** to **transfer information**. \n",
    "\n",
    "\n",
    "- It also only **has two gates**, a **Reset Gate** and **Update Gate**.\n",
    "\n",
    "\n",
    "- **Reset Gate**:\n",
    "  \n",
    "  The **reset gate** is another gate is used to **decide how much past information to forget**.\n",
    "  \n",
    "  $$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
    "  \n",
    "- **Update Gate**:\n",
    "\n",
    "  The **update gate** acts **similar** to the **forget** and **input gate** of an **LSTM**. \n",
    "  \n",
    "  It **decides what information** to **throw away** and **what new information** to **add**.\n",
    "  \n",
    "  $$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
    "\n",
    "\n",
    "- So the **output** is the **new hidden state** ($h_t$):\n",
    "\n",
    "  $$h_t = (1-z_t) \\times h_{t-1} + z_t \\times \\tilde{h_t},$$\n",
    "  \n",
    "  where\n",
    "\n",
    "  $$\\tilde{h_t} = \\tanh{W \\cdot [r_t \\times h_{t-1}, x_t]}.$$\n",
    "\n",
    "\n",
    "\n",
    "- **GRU**’s **has fewer tensor operations**, therefore, they are a **little speedier** to train then **LSTM**’s.\n",
    "\n",
    "\n",
    "- **There isn’t a clear winner which one is better!** Researchers and engineers usually try both to determine which one works better for their use case.\n",
    "\n",
    "  <img src=\"images/L9_GRU.png\" width=\"600\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">What Next?</h3>\n",
    "\n",
    "- **LSTM**s were a **big step** in what we can accomplish with RNNs.\n",
    "\n",
    "\n",
    "- It’s natural to wonder: **is there another big step**? \n",
    "\n",
    "  A common opinion among researchers is: **Yes! There is a next step and it’s attention!** \n",
    " \n",
    " \n",
    "- The **idea** is to **let every step** of an RNN **pick information** to look at from some **larger collection** of information. \n",
    "\n",
    "  For example, if you are using an RNN to **create a caption describing an image**, it might **pick a part of the image** to look at for every **word it outputs**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
