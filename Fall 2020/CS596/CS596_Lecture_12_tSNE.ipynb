{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **30 November 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 12**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Dimensionality Reduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>\n",
    "\n",
    "- **t-distributed stochastic neighbor embedding (t-SNE)** is a **nonlinear** dimensionality reduction technique well-suited for **embedding high-dimensional data** for **visualization** in a **low-dimensional space** of **two** or **three dimensions**.\n",
    "\n",
    "\n",
    "- The **t-SNE algorithm** comprises **two main stages**:\n",
    "\n",
    "\n",
    "- **First**, t-SNE c**onstructs** a **probability distribution** over **pairs of high-dimensional objects**.\n",
    "\n",
    "  **Similar objects** are assigned a **higher probability** while **dissimilar points** are assigned a **lower probability**.\n",
    "  \n",
    "\n",
    "- **Second**, t-SNE **defines** a **similar probability distribution** over the points **in the low-dimensional map**.\n",
    "\n",
    "  It **minimizes the Kullback–Leibler divergence** (KL divergence) between the two distributions with respect to the locations of the points in the map. \n",
    " \n",
    " \n",
    "- While the **original algorithm** uses the **Euclidean distance** between objects as the base of its similarity metric, this can be changed as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Stochastic Neighbor Embedding</h3>\n",
    "\n",
    "- - Let's assume that we are given a **collection** of $N$ **high-dimensional objects** $x_1, .., x_N$.\n",
    "\n",
    "\n",
    "- **Stochastic Neighbor Embedding (SNE)** starts by **converting** the **high-dimensional Euclidean distances** between datapoints **into conditional probabilities** that represent **similarities**.\n",
    "\n",
    "\n",
    "- The **similarity** of datapoint $x_j$ to datapoint $x_i$ is the **conditional probability**, $p_{j|i}$, that $x_i$ **would pick** $x_j$ as its **neighbor** if neighbors were picked in proportion to their **probability density** under a **Gaussian** centered at $x_i$:\n",
    "\n",
    "  $$p_{j|i} = \\frac{e^{-\\frac{|| x_i  - x_j||^2}{2 \\sigma_i^2}}}{\\sum_{k \\neq i}e^{-\\frac{|| x_i  - x_k||^2}{2 \\sigma_i^2}}},$$\n",
    "  \n",
    "  where $σ_i$ is the **variance** of the **Gaussian** that is centered on datapoint $x_i$.\n",
    "\n",
    "  Because we are only interested in modeling pairwise similarities, we set the value of $p_{i|i}$ to zero.\n",
    "  \n",
    "  \n",
    "- For the **low-dimensional counterparts** $y_i$ and $y_j$ of the high-dimensional datapoints $x_i$ and $x_j$, it is possible to compute a similar conditional probability, which we denote by $q_{j|i}$:\n",
    "\n",
    "  $$q_{j|i} = \\frac{e^{-|| y_i  - y_j||^2}}{\\sum_{k \\neq i}e^{-|| y_i  - y_k||^2}},$$\n",
    "  \n",
    "  where we **set variance** of the Gaussian that is employed in the computation of the conditional probabilities $q_{j|i} = \\frac{1}{\\sqrt{2}}$.\n",
    "  \n",
    "  Again, since we are only interested in modeling pairwise similarities, we set $q_{i|i} = 0$.\n",
    "  \n",
    "  <img src=\"images/L12_SNE.png\" width=\"600\" alt=\"Example\" />\n",
    " \n",
    "- A natural **measure of the faithfulness** with which $q_{j|i}$ models $p_{j|i}$ is the **Kullback-Leibler divergence**     \n",
    "  SNE **minimizes** the **sum of Kullback-Leibler divergences** over all datapoints using a **gradient descent method**.\n",
    "  \n",
    "  The **cost function** $C$ is given by:\\\n",
    "  \n",
    "  $$C = \\sum_{i=1}^{N} KL(P_i || Q_i) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}.$$\n",
    "  \n",
    "  in which $P_i$ represents the **conditional probability distribution** over all other datapoints **given datapoint** $x_i$\n",
    "  \n",
    "  and \n",
    "  \n",
    "  $Q_i$ represents the **conditional probability distribution** over all other map points **given map point** $y_i$.\n",
    "\n",
    "\n",
    "- The **remaining parameter** to be **selected** is the **variance** $\\sigma_i$ of the Gaussian that is centered over each high-dimensional datapoint, $x_i$.\n",
    "\n",
    "  It is not likely that there is a single value of $\\sigma_i$ that is optimal for all datapoints in the dataset because the density of the data is likely to vary. \n",
    "  \n",
    "  In **dense regions**, a **smaller value** of $\\sigma_i$ is usually **more appropriate** than in sparser regions. \n",
    "  \n",
    "  Any particular value of $\\sigma_i$ induces a probability distribution, $P_i$, over all of the other datapoints. \n",
    "  \n",
    "  This distribution has an entropy which increases as $\\sigma_i$ increases. \n",
    "  \n",
    "  SNE performs a binary search for the value of $\\sigma_i$ that produces a $P_i$ with a fixed perplexity that is specified by the user.\n",
    "  \n",
    "  The **perplexity** is defined as:\n",
    "  \n",
    "  $$Perp(P_i) = 2^{H(P_i)},$$\n",
    "  \n",
    "  where $H(P_i)$ is the **Shannon entropy** of $P_i$ measured in bits:\n",
    "  \n",
    "  $$H(P_i) = - \\sum_{j=1}^{N} p_{j|i} \\log_2 p_{j|i}.$$\n",
    "  \n",
    "  The perplexity can be interpreted as a **smooth measure of the effective number of neighbors**. \n",
    "  \n",
    "  The **performance of SNE** is **fairly robust** to changes in the **perplexity**, and typical **values are between 5 and 50**.\n",
    "  \n",
    "- The **minimization** of the **cost function** $C$ is performed using a **gradient descent method:\n",
    "\n",
    "  $$\\frac{\\partial C}{\\partial y_i} = 2 \\sum_{j=1}^{N} (p_{j|i} - q_{j|i} + p_{j|j} - q{i|j})(y_i - y_j).$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Crowding Problem</h3>\n",
    "\n",
    "- As well as SNE preserves local relationships, it suffers from the **crowding problem**. \n",
    "\n",
    "\n",
    "- The **area of the 2D map** that is available to accommodate moderately distant data points **will not be large enough** compared with the area available to accommodate nearby data points.\n",
    "\n",
    "\n",
    "- Intuitively, there is **less space** in a **lower dimension** to **accommodate moderately distant data points** originally in higher dimension:\n",
    "\n",
    "<img src=\"images/L12_Crowding_Problem.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "- Although the **distances** between the closest points $AB$ and $BC$ **are preserved**, the **global distance** $AC$ has to **shrink**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>\n",
    "\n",
    "- **To address** the **crowding problem** and make SNE more robust to outliers, **t-SNE** was introduced.\n",
    "\n",
    "\n",
    "- Compared to SNE, **t-SNE** has **two main changes**:\n",
    "\n",
    "  1. A **symmetrized** version of the SNE **cost function** with simpler gradients.\n",
    "  \n",
    "  2. A **Student-t distribution** rather than a Gaussian to compute the similarity **in the low-dimensional space** to alleviate the crowding problem.\n",
    "  \n",
    "\n",
    "- In **t-SNE** $p_{ij}$ is defined instead as follows:\n",
    "\n",
    "  $$p_{ij} = \\frac{p_{i|j} + p_{j|i}}{2N}.$$\n",
    "  \n",
    "  In this way, $\\sum_{j = 1}^{N} p_{ij} > \\frac{1}{2N}$ for all data points $x_i$.\n",
    "  \n",
    "  As a result, each $x_i$ **makes a significant contribution** to the **cost function**.\n",
    "\n",
    "\n",
    "- **t-SNE** uses the **Student’s t-distribution** instead of the Gaussian to define $Q$:\n",
    "\n",
    "  $$q_{ij} = \\frac{\\left (1 + ||y_i - y_j||^2 \\right )^{-1}}{\\sum_{k \\neq l} \\left(1 + ||y_k - y_l||^2 \\right )^{-1}}$$\n",
    "\n",
    "  The **cost function** of **t-SNE** is now defined as:\n",
    "  \n",
    "  $$C = \\sum_{i=1}^{N} KL(P_i || Q_i) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.$$\n",
    "  \n",
    "  \n",
    "- The **heavy tails** of the normalized Student-t kernel **allow** dissimilar **input objects** $x_i$ and $x_j$ **to be modeled** by low-dimensional counterparts $y_i$ and $y_j$ that are **far apart** because $q_{ij}$ is would be large for two embedded points that are far apart. \n",
    "  \n",
    "  And since $q$ is what to be learned, the **outlier problem does not exist** for low-dimension.\n",
    "  \n",
    "  Below the **pairwise Euclidian distance** between **two points** in the **high-dim** and **low-dim** data are shown:\n",
    "\n",
    "  <img src=\"images/L12_Gradients.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- The **gradient** of the **cost function** is:\n",
    "\n",
    "  $$\\frac{\\partial C}{ \\partial y_i} = 4 \\sum_{j = 1, j \\neq i}^{N} (p_{ij} - q _{ij})\\left ( 1 + ||y_i - y_j||^2\\right)^{-1} (y_i - y_j).$$\n",
    "  \n",
    "  We can interpret the t-SNE gradient as a simulation of an **N-body system**:\n",
    "  \n",
    "    <img src=\"images/L12_Nbody.png\" width=\"1000\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- Notice also, that there is an **exaggeration parameter** $\\alpha > 1$ in the tSNE algorithm, which is **used as a coefficient** for $p_{ij}$.\n",
    "\n",
    "<img src=\"images/L12_tSNE_Algorithm.png\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- Visualization of **6,000 digits** from the **MNIST dataset** produced by the **t-SNE** are shown below:\n",
    "\n",
    "<img src=\"images/L12_MNIST.png\" width=\"800\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Weaknesses</h3>\n",
    "\n",
    "- Although we have shown that t-SNE compares favorably to other techniques for data visualization, t-SNE has three potential weaknesses:\n",
    "\n",
    "\n",
    "1. It is **unclear how t-SNE performs** on general **dimensionality reduction** tasks.\n",
    "\n",
    "\n",
    "2. The relatively local nature of t-SNE makes it **sensitive to the curse of the intrinsic dimensionality** of the data.\n",
    "\n",
    "\n",
    "3. t-SNE is **not guaranteed to converge** to a global optimum of its cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
