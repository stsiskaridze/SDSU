{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **28 October 2020**\n",
    "\n",
    "\n",
    "Title: **Seminar 7**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Image Classification Problem using CIFAR-10</h2>\n",
    "\n",
    "- For demonstration purposes, we will use the **CIFAR-10 dataset**:\n",
    "\n",
    "\n",
    "- The <a href=\"https://www.kaggle.com/c/cifar-10\">CIFAR-10</a>  dataset is a standard dataset used in computer vision and deep learning community. \n",
    "\n",
    "\n",
    "- It consists of $60000$ $32 \\times 32$ **colour images** in $10$ **classes**, with $6000$ **images per class**. \n",
    " \n",
    " \n",
    "- There are $50000$ **training images** and $10000$ **test images**. \n",
    "\n",
    "\n",
    "\n",
    "- Classes in the dataset, as well as 10 random images from each, are presented below:\n",
    "\n",
    "  <img src=\"images/S7_Cifar10.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- It is a fairly **simple dataset**. Hence, it provides the flexibility to play with various techniques, such as hyperparameter tuning, regularization, training-test split, parameter search, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">Load CIFAR-10 dataset</h2>\n",
    "\n",
    "- Let's **load** the dataset.\n",
    "\n",
    "\n",
    "- For this we will need `tensorflow` library. \n",
    "\n",
    "\n",
    "- To use `tensorflow` in Google Colab directly, you need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we are ready to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CIFAR-10 dataset from keras' datasets\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Import this PyPlot to visualize images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of training and testing set\n",
    "print(\"X_train.shape =\", X_train.shape, \"Y_train.shape =\", Y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape, \"Y_test.shape =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can tell from the shapes that,\n",
    "\n",
    "  - `X_train` has $50000$ **training images**, each $32$ **pixel wide**, $32$ **pixel high**, and $3$ **color channels**;\n",
    "\n",
    "  - `X_test` has $10000$ **testing images**, each $32$ **pixel wide**, $32$ **pixel high**, and $3$ **color channels**;\n",
    "  \n",
    "  - `Y_train` has $50000$ **labels**;\n",
    "\n",
    "  - `Y_test` has $10000$ **labels**.\n",
    "  \n",
    "\n",
    "- Now, let's **define constants** for **number of classes** and its **labels**, to make the code more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's look at some random images from the training set. \n",
    "\n",
    "\n",
    "- You can change the number of columns and rows to get more/less images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show random images from training set\n",
    "cols = 8 # Number of columns\n",
    "rows = 4 # Number of rows\n",
    "\n",
    "fig = plt.figure(figsize=(2 * cols, 2 * rows))\n",
    "\n",
    "# Add subplot for each random image\n",
    "for col in range(cols):\n",
    "    for row in range(rows):\n",
    "        random_index = np.random.randint(0, len(Y_train)) # Pick a random index for sampling the image\n",
    "        ax = fig.add_subplot(rows, cols, col * rows + row + 1) # Add a sub-plot at (row, col)\n",
    "        ax.grid(b=False) # Get rid of the grids\n",
    "        ax.axis(\"off\") # Get rid of the axis\n",
    "        ax.imshow(X_train[random_index, :]) # Show random image\n",
    "        ax.set_title(CIFAR10_CLASSES[Y_train[random_index][0]]) # Set title of the sub-plot\n",
    "plt.show() # Show the imageb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">Data Preprocessing</h2>\n",
    "\n",
    "- Before defining the model and training the model, let us prepare the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(\"TensorFlow's version is\", tf.__version__)\n",
    "print(\"Keras' version is\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training and testing pixel values\n",
    "X_train_normalized = X_train / 255 - 0.5\n",
    "X_test_normalized = X_test / 255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "Y_train_coded = tf.keras.utils.to_categorical(Y_train, NUM_CLASSES)\n",
    "Y_test_coded = tf.keras.utils.to_categorical(Y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">Defining Convolutional Neural Network Model</h2>\n",
    "\n",
    "- Next, let us define a **model** that **takes images** as **input**, and **outputs class probabilities**.\n",
    "\n",
    "\n",
    "- We will define following layers in the model:\n",
    "\n",
    "  - **Convolutional layer** which takes ($32 \\times 32 \\times  3$) **shaped images as input**, outputs $16$ **filters**, and has a** kernel size** of ($3 \\times 3$), with the **same padding**, and uses **LeakyReLU** as activation function;\n",
    "\n",
    "  - **Convolutional layer** which takes ($32 \\times 32 \\times  16$) **shaped tensor** as input, outputs $32$ **filters**, and has a **kernel size** of ($3 \\times 3$), with the **same padding**, and uses **LeakyReLU** as activation function;\n",
    "\n",
    "  - **Max Pool layer** with **pool size** of ($2 \\times 2$), this outputs ($16 \\times 16 \\times  16$) **tensor**;\n",
    "\n",
    "  - **Dropout layer** with the **dropout rate** of $0.25$, to **prevent overfitting**;\n",
    "\n",
    "  - **Convolutional layer** which takes ($16 \\times 16 \\times  16$) **shaped tensor as input**, outputs $32$ **filters**, and has a **kernel size** of ($3 \\times 3$), with the **same padding**, and uses **LeakyReLU** as activation function;\n",
    "\n",
    "  - **Convolutional layer** which takes ($16 \\times 16 \\times  32$) **shaped tensor as input**, outputs $64$ **filters**, and has a **kernel size** of ($3 \\times 3$), with the **same padding**, and uses **LeakyReLU** as activation function;\n",
    "\n",
    "  - **Max Pool layer** with **pool size** of ($2 \\times 2$), this outputs ($8 \\times 8 \\times  64$) **tensor**;\n",
    "\n",
    "  - **Dropout layer** with the **dropout rate** of $0.25$, to **prevent overfitting**;\n",
    "\n",
    "  - **Dense layer** which takes **input from $8 \\times 8 \\times  64$ neurons**, and has $256$ **neurons**;\n",
    "\n",
    "  - **Dropout layer** with the **dropout rate** of $0.5$, to **prevent overfitting**;\n",
    "\n",
    "  - **Dense layer** with $10$ **neurons**, and **softmax activation**, is the **final layer**.\n",
    "\n",
    "\n",
    "- As you can see, **all the layers** use **LeakyReLU** activations, except the last layer. \n",
    "\n",
    "  This is a **pretty good choice** most of the time, but you change these as well to play with other activations such as **tanh**, **sigmoid**, **ReLU**, etc.\n",
    "  \n",
    "  \n",
    "  <img src=\"images/S7_CNN2.jpeg\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary building blocks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(Dense(units=10))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe model\n",
    "s = tf.keras.backend.clear_session()\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Model Training</h2>\n",
    "\n",
    "- Next, we train the model that we defined above. \n",
    "\n",
    "\n",
    "- We will use:\n",
    "\n",
    "  - **Initial learning rate**: $0.005$;\n",
    "  \n",
    "  - **Training batch size**: $64$;\n",
    "  \n",
    "  - **Number of epochs**: 10;\n",
    "\n",
    "\n",
    "- We define a **learning rate scheduler**, which **decays learning rate after each epoch**.\n",
    "\n",
    "\n",
    "- Also, We define a **class** that **handles callbacks from keras**. It **prints out** the **learning rate used in that epoch**\n",
    "\n",
    "\n",
    "\n",
    "- Feel free to change these hyperparameters, to dive deeper and know their effects. \n",
    "\n",
    "\n",
    "- We use categorical **cross entropy loss** as our **loss function** and **Adamax optimizer for convergence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "s = tf.keras.backend.clear_session()  # clear default graph\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 10-way classification\n",
    "    optimizer=tf.keras.optimizers.Adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(\n",
    "    X_train_normalized, Y_train_coded,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler), \n",
    "               LrHistory()],\n",
    "    validation_data=(X_test_normalized, Y_test_coded),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the model\n",
    "\n",
    "def save_model(model):# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Evaluate the Model Accuracy</h2>\n",
    "\n",
    "- Now that we have trained our model, let us see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the model\n",
    "def load_model():\n",
    "    from tensorflow.keras.models import model_from_json\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the learning curve during the training of our model\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the classes for each image in testing set\n",
    "\n",
    "Y_pred_test = model.predict_proba(X_test_normalized) # Predict probability of image belonging to a class, for each class\n",
    "Y_pred_test_classes = np.argmax(Y_pred_test, axis=1) # Class with highest probability from predicted probabilities\n",
    "Y_test_classes = np.argmax(Y_test_coded, axis=1) # Actual class\n",
    "Y_pred_test_max_probas = np.max(Y_pred_test, axis=1) # Highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the confusion matrix to understand the performance of our model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.imshow(confusion_matrix(Y_test_classes, Y_pred_test_classes))\n",
    "plt.xticks(np.arange(10), CIFAR10_CLASSES, rotation=45, fontsize=12)\n",
    "plt.yticks(np.arange(10), CIFAR10_CLASSES, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", accuracy_score(Y_test_classes, Y_pred_test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, **Test accuracy of** $\\approx 80\\%$isn’t bad for such a simple model. \n",
    "\n",
    "\n",
    "- Now, Let us look at **some random predictions** from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect preditions\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(Y_test))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(b=False)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(X_test[random_index, :])\n",
    "        pred_label = CIFAR10_CLASSES[Y_pred_test_classes[random_index]]\n",
    "        pred_proba = Y_pred_test_max_probas[random_index]\n",
    "        true_label = CIFAR10_CLASSES[Y_test[random_index][0]]\n",
    "        ax.set_title(\"pred: {}\\nscore: {:.3}\\ntrue: {}\".format(\n",
    "               pred_label, pred_proba, true_label\n",
    "        ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Summary</h2>\n",
    "\n",
    "- We show how to develop a **CNN for CIFAR-10** classification from scratch using **TensorFlow**\n",
    "\n",
    "\n",
    "- Specifically, we showed:\n",
    "\n",
    "  - How to **load** CIFAR-10 in your python program\n",
    "\n",
    "  - How to **look at random images** in the dataset\n",
    "\n",
    "  - How to **define** and **train a model**\n",
    "\n",
    "  - How to **save the learnt weights** of the model **to disk**\n",
    "\n",
    "  - How to **predict clsses** using the model\n",
    "\n",
    "\n",
    "\n",
    "- This is a pretty good model, however **one can achieved around $99\\%$ accuracy** for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Seminar</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
