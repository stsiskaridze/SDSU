{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **17 November 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 10**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Sources:\n",
    "Bibliography: \n",
    "<br>[1] **Chapter 6-7**. Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Kernel Methods</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Kernel Function</h3>\n",
    "\n",
    "- For models which are based on a fixed nonlinear **feature space** mapping $\\phi (\\mathbf{x})$ the kernel function is given\n",
    "by the relation:\n",
    "\n",
    "  $$k(\\mathbf{x}, \\mathbf{x}') = \\phi (\\mathbf{x})^T \\phi (\\mathbf{x}').$$\n",
    "\n",
    "  From this definition, we see that the **kernel** is a **symmetric function** of its arguments.\n",
    "\n",
    "- Examples of kernel functions:\n",
    "  - **stationary** kernel: $k(\\mathbf{x}, \\mathbf{x}') = k(\\mathbf{x} - \\mathbf{x}')$;\n",
    "  - **homogeneous** or **radial basis functions** kernel: $k(\\mathbf{x}, \\mathbf{x}') = k(\\|\\mathbf{x} - \\mathbf{x}'\\|)$;\n",
    "  \n",
    "- Many **linear parametric models** can be **re-cast** into an equivalent **dual representation** in which the predictions are based on linear combinations of a **kernel function** evaluated at the **training data points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Dual Representations</h3>\n",
    "\n",
    "- Lets consdier linear regression model whose parameters are determined by minimizing a regularized sum-of-squares error function given by:\n",
    "\n",
    "  $$J(\\mathbf{w}) = \\frac{1}{2} \\{ \\mathbf{w^T\\phi(x_n)}  - t_n \\}^2 + \\frac{\\lambda}{2} \\mathbf{w^Tw},$$\n",
    "  \n",
    "  where $\\lambda \\geq 0$.\n",
    "  \n",
    "  If we set the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ equal to zero, we see that the solution for  $\\mathbf{w}$ takes the form of a linear combination of the vectors $\\phi(\\mathbf{x_n})$, with coefficients that are functions of $\\mathbf{w}$, of the form:\n",
    "  \n",
    "  $$\\mathbf{w} = -\\frac{1}{\\lambda} \\sum_{n=1}^{N} \\{ \\mathbf{w^T\\phi(x_n)}  - t_n \\}^2 \\phi(\\mathbf{x_n})  = \n",
    "  \\sum_{n=1}^{N} a_n \\phi(\\mathbf{x_n}) = \\mathbf{\\Phi^T a},$$\n",
    "  \n",
    "  where $\\mathbf{\\Phi}$ is the design matrix, whose $n$-th row is given by $\\mathbf{\\phi(x_n)^T}$.\n",
    "  \n",
    "  Here the vector $\\mathbf{a} = (a_1, ..., a_n)^\\mathbf{T}$ is defnied as:\n",
    "  \n",
    "  $$a_n = -\\frac{1}{\\lambda}\\{ \\mathbf{w^T\\phi(x_n)}  - t_n \\}.$$\n",
    "  \n",
    "  Therefore, **instead of working** with the parameter vector $\\mathbf{w}$, we can now **reformulate the leastsquares algorithm** in **terms of** the parameter vector $\\mathbf{a}$, giving rise to a **dual representation**:\n",
    "  \n",
    "  $$J(\\mathbf{a}) = \\frac{1}{2} \\mathbf{ a^T \\Phi \\Phi^T \\Phi \\Phi^T a - a^T \\Phi \\Phi^T t +\\frac{1}{2} t^T t + \\frac{\\lambda}{2} a^T \\Phi \\Phi^T },$$\n",
    "  \n",
    "  where $\\mathbf{t} = (t_1, ..., t_N)^{\\mathbf{T}}.$\n",
    "  \n",
    "  We now define the **Gramm matrix** $K = \\Phi \\Phi^T$, which is an $N \\times N$ symmetric matrix with elements:\n",
    "  \n",
    "  $$K_{nm} = \\phi (\\mathbf{x}_n)^{\\mathbf{T}} \\phi (\\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m).$$\n",
    "  \n",
    "  where we have introduced the **kernel function** $k(\\mathbf{x}_n, \\mathbf{x}_m)$.\n",
    "  \n",
    "  In terms of the **Gram matrix**, the sum-of-squares error function can be written as:\n",
    "  \n",
    "  $$J(\\mathbf{a}) = \\frac{1}{2} \\mathbf{ a^T K K a  - a^T K t + \\frac{1}{2} t^T t + \\frac{\\lambda}{2} a^T K a}.$$\n",
    "  \n",
    "  Setting the gradient $J(\\mathbf{a})$ of with respect to a to zero, we obtain the following solution:\n",
    "  \n",
    "  $$\\mathbf{a = (K + \\lambda I_N)^{-1} t}.$$\n",
    "  \n",
    "  If we substitute this back into the linear regression model, we obtain the following prediction for a new input $\\mathbf{x}$:\n",
    "  \n",
    "  $$y(\\mathbf{x}) = \\mathbf{w^T \\phi(x_n) = a^T \\Phi \\phi(x_n) = k(x)^T (K + \\lambda I_N)^{-1} t},$$\n",
    "  \n",
    "  where we have defined the vector $\\mathbf{k(x)}$ with elements $k_n (\\mathbf{x}) = k(\\mathbf{x}_n, \\mathbf{x}).$\n",
    "  \n",
    "  This is known as a **dual formulation**.\n",
    "  \n",
    "  \n",
    "- In the **dual formulation**, we determine the parameter vector a by inverting an $N \\times N$ matrix, whereas in the **original parameter space formulation** we had to invert an $M \\times M$ matrix in order to determine $\\mathbf{w}$. \n",
    "\n",
    "- Because $N$ is typically **much larger** than $M$, the **dual formulation does not seem to be particularly useful**.\n",
    "  \n",
    "- However, the **advantage of the dual formulation** is that it is **expressed entirely in terms of the kernel function** $k(\\mathbf{x}, \\mathbf{x}')$. We can therefore **work directly in terms of kernels** and **avoid the explicit introduction of the feature vector** Ï†(x), which allows us implicitly to use **feature spaces of high dimensionality** (even infinite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Construction Kernels</h3>\n",
    "\n",
    "- One approach to construct valid kernel functions is to choose a feature space mapping $\\phi(x)$ and then use this to find the corresponding kernel.\n",
    "\n",
    "\n",
    "- The kernel function is defined for a one-dimensional input space by:\n",
    "\n",
    "  $$k(x, x') = \\phi(x)^T \\phi(x') = \\sum_{i=1}^{M}\\phi_i(x)\\phi_i(x'),$$\n",
    "  \n",
    "  where $\\phi_i$ are the basis functions.\n",
    " \n",
    "- The construction of kernel functions starting from a corresponding set of basis functions is shown below.\n",
    "\n",
    "  In each column the lower plot shows the kernel function $k(x, x')$ defined as a function of $x$ and $x' = 0$.\n",
    " \n",
    "  <img src=\"images/L10_Kernel_Function.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- An alternative approach is to construct kernel functions directly.\n",
    "\n",
    "\n",
    "- In this case, we must ensure that the function we choose is a **valid kernel**, in other words that it corresponds to a scalar product in some (perhaps infinite dimensional) feature space.\n",
    "\n",
    "\n",
    "- Given **valid kernels** $k_1(\\mathbf{x},\\mathbf{x}')$ and $k_2(\\mathbf{x},\\mathbf{x}')$, the following new kernels **will also be valid**:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "k(\\mathbf{x},\\mathbf{x}') & = & c k_1(\\mathbf{x},\\mathbf{x}')& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & f(\\mathbf{x}) k(\\mathbf{x},\\mathbf{x}') f(\\mathbf{x}')& \\\\\n",
    "k(\\mathbf{x},\\mathbf{x}') & = & q(k_1(\\mathbf{x},\\mathbf{x}'))& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & \\mathrm{exp}(k_1(\\mathbf{x},\\mathbf{x}'))& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & k_1(\\mathbf{x},\\mathbf{x}') + k_2(\\mathbf{x},\\mathbf{x}')& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & k_1(\\mathbf{x},\\mathbf{x}') \\cdot k_2(\\mathbf{x},\\mathbf{x}')& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & k_3( \\phi(\\mathbf{x}), \\phi(\\mathbf{x}'))& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & \\mathbf{ x^T A x'}& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = & k_a(\\mathbf{x}_a,\\mathbf{x}_a') + k_b(\\mathbf{x}_b,\\mathbf{x}_b')& \\\\ \n",
    "k(\\mathbf{x},\\mathbf{x}') & = &  k_a(\\mathbf{x}_a,\\mathbf{x}_a') \\cdot k_b(\\mathbf{x}_b,\\mathbf{x}_b')&\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Support Vector Machines</h3>\n",
    "\n",
    "- Lets consider a two-class classification problem using linear models of the form:\n",
    "\n",
    "  $$ y (\\mathbf{x}) = \\mathbf{w^T \\phi (x)} + b.$$\n",
    "  \n",
    "  where $\\mathbf{\\phi(x)}$ denotes a fixed feature-space transformation, and we have made the bias parameter $b$ explicit.\n",
    "  \n",
    "  The training data set comprises $N$ input vectors $\\mathbf{x}_1, ..., \\mathbf{x}_N$ with corresponding target values $t_1, .., t_N$ where $t_n \\in \\{ -1, 1\\}$, and new new data points $\\mathbf{x}$ are classified according to the sign of $y(\\mathbf{x})$.\n",
    "  \n",
    "  \n",
    "- Lets assume for the moment that the training data set is **linearly separable** in **feature space**, so that **by definition** there **exists at least one choice of the parameters** such that a function satisfies $y(\\mathbf{x}_n)> 0 $ for points having $t_n = +1$ and $y(\\mathbf{x}_n)< 0 $ for points having $t_n = -1$, so that $t_n y(\\mathbf{x}_n)> 0 $ for all training data points.\n",
    "\n",
    "\n",
    "- The perceptron algorithm that is guaranteed to find a solution in a finite number of steps. Howere, the solution will be dependent on the (arbitrary) initial values chosen for $\\mathbf{w}$ and $b$ as well as on the **order in which the data points are presented**.\n",
    "\n",
    "\n",
    "- If there are **multiple solutions** all of which classify the training data set exactly, then we should **try to find the one** that will give the **smallest generalization error**.\n",
    "\n",
    "\n",
    "- The **Support Vector Machine (SVM)** approaches this problem through the concept of the **margin**, which is defined to be the **smallest distance between the decision boundary and any of the samples**.\n",
    "\n",
    "  <img src=\"images/L10_SVM.png\" width=\"1800\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- Recall that the perpendicular distance of a point $\\mathbf{x}$ from a hyperplane defined by $y(\\mathbf{x}) = 0$ is given by:\n",
    "\n",
    "  $$\\frac{|y(\\mathbf{x})|}{ \\|\\mathbf{w}\\|}.$$\n",
    "  \n",
    "  Thus the distance of a point $\\mathbf{x}_n$ to the decision surface is given by:\n",
    "  \n",
    "  $$\\frac{t_n y(\\mathbf{x}_n)}{ \\| \\mathbf{w}\\|} = \\frac{t_n (\\mathbf{w^T} \\phi(\\mathbf{x}_n) +b)}{ \\| \\mathbf{w}\\|}.$$\n",
    "  \n",
    "  The **margin** is given by the **perpendicular distance** to the closest point $\\mathbf{x}_n$ from the data set, and we wish to optimize the parameters $\\mathbf{w}$ and $b$ in order to maximize this distance.\n",
    "  \n",
    "  Thus the **maximum margin solution** is found by solving:\n",
    "  \n",
    "  $$ \\mathbf{\\arg\\max_{w,b}} \\left \\{  \\frac{1}{\\| \\mathbf{w}\\|} \\min_{n} \\left [t_n (\\mathbf{w^T} \\phi(\\mathbf{x}_n) + b) \\right ] \\right\\}.$$\n",
    "  \n",
    "  where we have taken the factor $\\frac{1}{\\| \\mathbf{w}\\|}$ outside because $\\mathbf{w}$ does not depend on $n$.\n",
    "  \n",
    "  \n",
    "- Lets make the rescaling  $\\mathbf{w} \\rightarrow \\kappa \\mathbf{w}$ and $b \\rightarrow \\kappa b$, then the distance from any point $\\mathbf{x}_n$ to the **decision surface** is unchanged. We can use this freedom to set:\n",
    "\n",
    "  $$t_n (\\mathbf{w^T} \\phi(\\mathbf{x}_n) + b) = 1,$$\n",
    "  \n",
    "  for the point that is closest to the surface. In this case, all data points will satisfy the constraints:\n",
    "  \n",
    "  $$t_n (\\mathbf{w^T} \\phi(\\mathbf{x}_n) + b) \\geq 1,$$\n",
    "  \n",
    "  for $n = 1, ..., N$. \n",
    "  \n",
    "  This is known as the **canonical representation** of the **decision hyperplane**.\n",
    "  \n",
    "- In the case of data points for which the equality holds, the constraints are said to be **active**, whereas for the remainder they are said to be **inactive**.\n",
    "\n",
    "\n",
    "- By definition, there will always be at **least one active** constraint, because **there will always be a closest point**.\n",
    "\n",
    "\n",
    "- The optimization problem then simply requires that we maximize $\\| \\mathbf{w}\\|^{-1}$, which is equivalent to minimizing $\\| \\mathbf{w}\\|^2$. Therefore, we need to solve the optimization problem:\n",
    "\n",
    "  $$\\mathbf{\\arg\\max_{w,b}} \\frac{1}{2} \\| \\mathbf{w}\\| ^ 2.$$\n",
    "  \n",
    "  This is an example of a quadratic programming problem in which we are trying to minimize a quadratic function subject to a set of linear inequality constraints.\n",
    "  \n",
    "  \n",
    "- In order to solve this constrained optimization problem, we introduce Lagrange multipliers $a_n \\geq 0$, with one multiplier $a_n$ for each of the constraints, giving the Lagrangian function:\n",
    "\n",
    "  $$L(\\mathbf{w}, b, \\mathbf{a})  = \\frac{1}{2} \\| \\mathbf{w}\\|^2  - \\sum_{i=1}^{N} a_n \\{ t_n (\\mathbf{w^T} \\phi (\\mathbf{x}_n) + b) - 1 \\},$$\n",
    "  \n",
    "  where $\\mathbf{a} = (a_1, ..., a_n)^{\\mathbf{T}}$.\n",
    "  \n",
    "\n",
    "- Setting the derivatives of $L(\\mathbf{w}, b, \\mathbf{a})$ with respect to $\\mathbf{w}$ and $b$ equal to zero, we obtain the following two conditions:\n",
    "\n",
    "  $$\\mathbf{w} = \\sum_{n=1}^{N} a_n t_n \\phi(\\mathbf{x}_n),$$\n",
    "  $$0 = \\sum_{n=1}^{N} a_n t_n.$$\n",
    "\n",
    "  Eliminating $\\mathbf{w}$ and $b$ from  $L(\\mathbf{w}, b, \\mathbf{a})$ using these conditions, gives the **dual representation** of the **maximum margin problem** in which we maximize:\n",
    "  \n",
    "  $$\\widetilde{L}(\\mathbf{a}) = \\sum_{n=1}^{N} a_n - \\frac{1}{2} \\sum_{n=1}^{N}\\sum_{m=1}^{N} a_n a_m t_n t_m k(\\mathbf{x}_n, \\mathbf{x}_m),$$\n",
    "\n",
    "  with respect to $\\mathbf{a}$ subject to the constraints:\n",
    "  \n",
    "  $$a_n \\geq 0, n=1, ..., N;$$\n",
    "  $$\\sum_{n=1}^{N} a_n t_n = 0.$$\n",
    "  \n",
    "  Here the kernel function is defined by $k(\\mathbf{x, x'}) = \\mathbf{\\phi(x)^T \\phi (x')}.$\n",
    "  \n",
    "  \n",
    "- Again, this takes the form of a **quadratic programming problem** in which we optimize a quadratic function of $\\mathbf{a}$ subject to a set of inequality constraints.\n",
    "\n",
    "\n",
    "- In general, the **solution** to a **quadratic programming problem** in $M$ variables in has computational complexity that is $O(M^3)$.\n",
    "  \n",
    "  For a **fixed set of basis functions** whose **number $M$** is **smaller** than the **number $N$** of data points, the **move to the dual problem** appears **disadvantageous**. However, it allows the model to be reformulated using kernels, and so the maximum margin classifier can be applied efficiently to feature spaces whose dimensionality exceeds the number of data points, including infinite feature spaces.\n",
    "\n",
    "\n",
    "- In order to classify new data points using the trained model we evaluate the sign of $y(\\mathbf{x})$.\n",
    "\n",
    "  This can be expressed in terms of the parameters $\\{a_n\\}$ and the kernel function:\n",
    "  \n",
    "  $$y(\\mathbf{x}) = \\sum_{n=1}^{N} a_n t_n k(\\mathbf{x, x}_n +b).$$\n",
    "  \n",
    "- The constrained optimization of this form satisfies the **Karush-Kuhn-Tucker (KKT) conditions** which in this case require that the following three properties hold:\n",
    "  \n",
    "  $$a_n \\geq 0,$$\n",
    "  \n",
    "  $$t_n y(\\mathbf{x}_n) - 1 \\geq 0,$$\n",
    "  \n",
    "  $$a_n \\{ t_n y(\\mathbf{x}_n) -1\\} = 0$$\n",
    "  \n",
    "  Thus for every data point, either $a_n$ = 0 or $t_n y(\\mathbf{x}_n) = 1$.\n",
    "  \n",
    "  \n",
    "- Therefore, any data point for which $a_n =0$ will not appear in the sum and hence **plays no role** in making predictions for new data points.\n",
    "  \n",
    "  \n",
    "- The remaining data points are called **support vectors**, and because they satisfy $t_n y(\\mathbf{x}_n) = 1$. they correspond to points that lie on the maximum margin hyperplanes in feature space.\n",
    "\n",
    "\n",
    "- **This property is central to the practical applicability of support vector machines**. \n",
    "\n",
    "  **Once the model is trained, a significant proportion of the data points can be discarded and only the support vectors retained.**\n",
    "\n",
    "\n",
    "- Having solved the quadratic programming problem and found a value for $\\mathbf{a}$, we can then determine the value of the threshold parameter $b$ by noting that any **support vector** $\\mathbf{x}_n$ satisfies $t_n y(\\mathbf{x}_n) = 1$>:\n",
    "\n",
    "  $$t_n \\left ( \\sum_{m \\in \\mathcal{S}} a_m t_n k(\\mathbf{x}_n, \\mathbf{x}_m) + b \\right ) = 1,$$\n",
    "  \n",
    "  where $\\mathcal{S}$ denotes the set of indices of the **support vectors**.\n",
    "  \n",
    "  Solving for b to give:\n",
    "  \n",
    "  $$b = \\frac{1}{N_{\\mathcal{S}}} \\sum_{n \\in \\mathcal{S}} \\left ( t_n - \\sum_{m \\in \\mathcal{S}} a_m t_m k(\\mathbf{x}_n, \\mathbf{x}_m)\\right ),$$\n",
    "  \n",
    "  where $N_{\\mathcal{S}}$ is the total number of support vectors.\n",
    "\n",
    "\n",
    "- Figure below shows an example of the classification resulting from training a support vector machine on a simple synthetic data set using a Gaussian kernel.\n",
    "\n",
    "<img src=\"images/L10_SVM2.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
