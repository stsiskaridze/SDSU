{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Probability and Statistics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probability versus Statistics</h3>\n",
    "\n",
    "- Probability and statistics are two closely related mathematical subjects.\n",
    "- The main difference between probability and statistics:\n",
    " - **Probability** is a numerical description of how likely an event is to occur;\n",
    " - **Statistics** is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data.\n",
    "\n",
    "<center><img src=\"images/RM_PvsS.jpg\" width=\"700\" height=\"300\" alt=\"Example\" /></center>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Sigma-algebra</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Let $X$ be some set, and let $\\mathcal{P}(X)$ represent its power set, i.e. $\\mathcal{P} \\equiv   2^X = \\{A | A \\subseteq X\\}$.\n",
    "<br>\n",
    "A set $\\Sigma \\subset \\mathcal{P}$ is called $\\sigma$-algebra on a set $X$, if it satisfies the following three properties:\n",
    "<br> &emsp; $\\bullet$ $X \\in \\Sigma$, i.e. $X$ is considered to be the **universal set**;\n",
    "<br> &emsp; $\\bullet$ $\\Sigma$ is **closed under complementation**, i.e. if $A \\in \\Sigma$ then $X \\setminus A \\in \\Sigma$;\n",
    "<br> &emsp; $\\bullet$ $\\Sigma$ is **closed under countable unions**, i.e if $A_i \\in \\Sigma$ for $i = \\overline{1, \\infty}$ then $A = \\left ( \\bigcup_{i=1}^{\\infty}A_i \\right ) \\in \\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From these properties we can easily conclude that:\n",
    "<br> &emsp; $\\bullet$ $\\emptyset \\in \\Sigma$;\n",
    "<br> &emsp; $\\bullet$ For every set $X$ the **smallest** $\\sigma$-algebra will be $\\{X, \\emptyset\\}$ and the **largest** will be $\\mathcal{P}(X)$;\n",
    "<br> &emsp; $\\bullet$ $\\Sigma$ is **closed under countable intersections**, i.e. if $A_i \\in \\Sigma$ for $i = \\overline{1, n}$ then $A = \\left ( \\bigcap_{i=1}^{n}A_i \\right ) \\in \\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Definition}$. The pair  $(X, \\Sigma)$ is called **mesurable space**, or **Borel space**, and the elements of the $\\Sigma$ are called **mesurable sets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "$\\textbf{Definition}$. The function $f:(X, \\Sigma_X) \\to (Y, \\Sigma_Y)$ between two mesurable spaces is called **mesurable function** if for all $A \\in \\Sigma_Y$ the set $f^{-1}(A) \\in \\Sigma_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Mesure Space</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Let $(X, \\Sigma)$ be a measurable space.\n",
    "<br> A function $\\mu: \\Sigma \\to [0, \\infty]$ is called **mesure** if it satisfies the following properties:\n",
    "<br> &emsp; $\\bullet$ **Non-negativity**, i.e. for all $A \\in \\Sigma$, we have $\\mu(A) \\geq 0$;\n",
    "<br> &emsp; $\\bullet$ **Null empty set**, i.e. $\\mu(\\emptyset) = 0$;\n",
    "<br> &emsp; $\\bullet$ **Countable additivity**, i.e for pairwise disjoint sets $A_i \\in \\Sigma$ for $i = \\overline{1, \\infty}$, the following holds:\n",
    "\n",
    "$$\\mu \\left( \\bigcup_{i=1}^\\infty A_i\\right)=\\sum_{i=1}^\\infty \\mu(A_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Definition}$. A tripple $(X, \\Sigma, \\mu)$ is called a **measure space**, where:\n",
    "<br> &emsp; $\\bullet$ $X$ is a set;\n",
    "<br> &emsp; $\\bullet$ $\\Sigma$ is a $\\sigma$-algebra on the set $X$;\n",
    "<br> &emsp; $\\bullet$ $\\mu$ is a measure on $(X, \\Sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probability</h3>\n",
    "\n",
    "$\\textbf{Definition}$. A measure space $(\\Omega , \\Sigma, P)$ is called **probability space** if $P(\\Omega ) = 1$.\n",
    "\n",
    "The element of $\\Sigma$ is called **events**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each pair of events $A \\in \\Sigma$ and $B \\in \\Sigma$, the following properties of the probability space are valid:\n",
    "<br> &emsp; $\\bullet$ $P(\\emptyset) = 0$;\n",
    "<br> &emsp; $\\bullet$ $0 \\le P(A) \\le 1$\n",
    "<br> &emsp; $\\bullet$ $P(\\Omega  \\setminus A) = 1 - P(A)$;\n",
    "<br> &emsp; $\\bullet$ if $A \\subset B$ then $P(A) \\le P(B)$;\n",
    "<br> &emsp; $\\bullet$ $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example 1</h3>\n",
    "\n",
    "The experiment consists of tossing a fair coin, the outcome is either **H**eads or **T**ails.\n",
    "\n",
    "Thus, our probability space $(\\Omega, \\Sigma, P)$ is:\n",
    "<br> &emsp; $\\bullet$ $\\Omega = \\{H, T\\}$;\n",
    "<br> &emsp; $\\bullet$ The $\\sigma$-algebra contains 4 events: $\\Sigma = 2^\\Omega = \\left \\{ \\{\\emptyset\\}, \\{H\\}, \\{T\\}, \\{H, T\\} \\right \\}$;\n",
    "<br> &emsp; $\\bullet$ The probability measure $P$ in this example is:\n",
    "$$P(\\{\\emptyset\\}) = 0, \\text{ } P(\\{H\\}) = 0.5, \\text{ } P(\\{T \\}) = 0.5, \\text{ } P(\\{ H, T \\}) = 1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example 2</h3>\n",
    "\n",
    "A number between $0$ and $1$ is chosen at random, uniformly.\n",
    "<br>\n",
    "The open intervals of the form $(a,b)$, where $0 < a < b < 1$, could be taken as the generator sets.\n",
    "<br>\n",
    "Each set can be ascribed the probability of $P = (b âˆ’ a)$, which generates the **Lebesgue measure** on $[0,1]$.\n",
    "\n",
    "Thus, our probability space $(\\Omega, \\Sigma, P)$ is:\n",
    "<br> &emsp; $\\bullet$ $\\Omega = [0, 1]$;\n",
    "<br> &emsp; $\\bullet$ $\\Sigma$ is a $\\sigma$-algebra of **Borel set** on $\\Omega$;\n",
    "<br> &emsp; $\\bullet$ The probability measure $P$ in this example is the Lebesgue measure on $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 11.1</h3>\n",
    "\n",
    "The experiment consists of tossing a fair coin three times, the outcome of each toss is either **H**eads or **T**ails.\n",
    "\n",
    "Define the probability space $(\\Omega, \\Sigma, P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Independent events</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space.\n",
    "\n",
    "$\\textbf{Definition}$. Two events $A\\in \\Sigma$ and $B\\in \\Sigma$ are **independent** if:\n",
    "\n",
    "$$P(A \\cap B) = P(A)P(B)$$\n",
    "\n",
    "$\\textbf{Definition}$. A finite set of events $\\{A_i\\}_{i=1}^{n}$ are **pairwise independent** if for any $i,j \\in \\overline{1, n}$:\n",
    "\n",
    "$$P(A_i \\cap A_j) = P(A_i)P(A_j)$$\n",
    "\n",
    "$\\textbf{Definition}$. A finite set of events $\\{A_i\\}_{i=1}^{n}$ are **mutually independent** if:\n",
    "\n",
    "$$P(\\bigcap_{i=1}^{n}A_i) = \\prod_{i=1}^{n}P(A_i)$$\n",
    "\n",
    "$\\textbf{Note}$. Any collection of **mutually independent random variables is pairwise independent**, but some **pairwise independent collections are not mutually independent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "Let's consider the example describe in **Exercise 11.1**.\n",
    "\n",
    "Let:\n",
    "- $A$ be the event **Toss 1** and **Toss 2** give the same result;\n",
    "- $B$ be the event **Toss 2** and **Toss 3** give the same result;\n",
    "- $C$ be the event **Toss 3** and **Toss 1** give the same result.\n",
    "\n",
    "We have:\n",
    "$$P(A) = P(B) = P(C) = \\frac{1}{2} \\text{ and } P(A \\cap B) =  P(B \\cap C) = P(C \\cap A)  = \\frac{1}{4}.$$\n",
    "\n",
    "However, it is clear that $A$, $B$, and $C$ are not mutually independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 12.1</h3>\n",
    "\n",
    "The experiment consists of tossing the fair dice and $A = \\{2, 4, 6\\}$ and $B = \\{1, 2, 3, 4\\}$ are two events.\n",
    "\n",
    "Define the probability space $(\\Omega, \\Sigma, P)$ and prove that $A$ and $B$ are independent events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Conditional probability</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space and $A\\in \\Sigma$ and $B\\in \\Sigma$ are two events.\n",
    "\n",
    "$\\textbf{Definition}$. The **conditional probability** of $A$ given $B$ is defined as the quotient of the probability of the joint of events $A$ and $B$, and the probability of $B$.\n",
    "\n",
    "Or in other words, if $P(B) \\gt 0$, then **conditional probability** of $A$ given $B$ is given as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Statement}$. $A$ and $B$ are independent events if and only if $P(A|B) = P(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. If $A$ and $B$ are independent events, then $P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A).$\n",
    "<br>\n",
    "Opposite, if $P(A|B) \\equiv \\frac{P(A \\cap B)}{P(B)} = P(A)$, then $P(A \\cap B) = P(A)P(B)$ and $A$ and $B$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Total Probability Theorem</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space.\n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{17}$. If $B_1, B_2, B_3, \\cdots$ is a countable (or finite) partition of $\\Omega$, then for any event $A \\subset \\Omega$:\n",
    "\n",
    "$$P(A) = \\sum_{i}P(A|B_i)P(B_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. Since $B_1, B_2, B_3, \\cdots $ is a partition of the $\\Omega$, we can write:\n",
    "$$\\Omega = \\bigcup_{i}B_i \\text{ and } A = A \\cap \\Omega = A \\cap \\left ({ \\bigcup_i B_i }\\right ) = \\bigcup_i \\left ( A \\cap B_i \\right ).$$\n",
    "\n",
    "Now note that the sets $A \\cap B_i$ are disjoint since the $B_i$'s are disjoint. Thus:\n",
    "\n",
    "$$P(A) = P\\left  (\\bigcup_i ( A \\cap B_i )\\right ) = \\sum_{i}P( A \\cap B_i ) = \\sum_{i} P(A | B_i) P(B_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bayes' Theorem</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space and $A\\in \\Sigma$ and $B\\in \\Sigma$ are two events.\n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{18}$. **Bayesâ€™ theorem** is stated mathematically as the following equation:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.$$\n",
    "where:\n",
    "<br> &emsp; $\\bullet$ $P(A|B)$ is a conditional probability of occurring $A$ given that $B$ is true;\n",
    "<br> &emsp; $\\bullet$ $P(B|A)$ is a conditional probability of occurring $B$ given that $A$ is true;\n",
    "<br> &emsp; $\\bullet$ $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. We can rewrite the definitions of $P(A|B)$ and $P(B|A)$ in the following forms:\n",
    "\n",
    "$$P(A|B)P(B) = P(A \\cap B) \\text{ and } P(B|A)P(A) = P(B \\cap A).$$\n",
    "\n",
    "Equating the two yields, we get $P(A|B)P(B) = P(A \\cap B) = P(B \\cap A) = P(B|A)P(A)$, and thus: \n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "**Chris Wiggins**, an associate professor of applied mathematics at **Columbia University**, posed the following problem in an article in **Scientific American** (<a href = 'https://www.scientificamerican.com/article/what-is-bayess-theorem-an/'>Link to the article in Scientific American</a>):\n",
    "\n",
    "$\\textbf{Problem}$. A patient goes to see a doctor. The doctor performs a test with **99%** reliability - that is, **99%** of people who are sick test positive and **99%** of the healthy people test negative. The doctor knows that only **1%** of the people in the country are sick. Now the question is: **if the patient tests positive, what are the chances the patient is sick?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The intuitive answer is **99 %**, but the correct answer is **50 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Solution}$. Wiggins's explanation can be summarized with the help of the following table which illustrates the scenario in a hypothetical population of $10,000$ people:\n",
    "\n",
    "|      | Diseased | Not Diseased |     |\n",
    "|:----:|:--------:|:------------:|:---:|\n",
    "|test +| 99       | 99           | 198 | \n",
    "|test -| 1        | 9801         | 9802| \n",
    "|      | 100      | 9900         |10000|\n",
    "\n",
    "We want to know the probability of disease $(A)$ given that the patient has a positive test $(B)$, i.e. $P(A|B).$\n",
    "\n",
    "$\\bullet$ We know that the unconditional probability of disease is $1\\%$, i.e. $P(A) = 0.01$;\n",
    "<br> $\\bullet$ The unconditional probability of a positive test is $P(B) = 198/10000 = 0.0198$;\n",
    "<br> $\\bullet$ We also know the sensitivity of the test is $99\\%$, i.e. $P(B | A) = 0.99$.\n",
    "\n",
    "Using the Bayes's Theorem we get:\n",
    "\n",
    "$$P(A|B)= \\frac{P(B|A)P(A)}{P(B)} = \\frac{0.99 \\cdot 0.01}{0.0198} = \\frac{1}{2} = 50\\%.$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Exercises 13.1</h3>\n",
    "\n",
    "Consider the same problem, but assuming that **50%** (instead of **1%**) of the people in the country are sick. \n",
    "<br>\n",
    "What will be the answer on the same question in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Extended Bayes' Theorem</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space.\n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{19}$. If $B_1, B_2, B_3, \\dots$ is countable (or finite) partition of $\\Omega$ such that \n",
    "$P(B_i) > 0$ for each $i \\in \\{1, 2, 3, \\dots\\}$, \n",
    "then for any event $A \\subset \\Omega$, such that $P(A) > 0$, and for each $k \\in \\{1, 2, 3, \\dots \\}$:\n",
    "\n",
    "$$P(B_k|A) = \\frac{P(A|B_k)P(B_k)}{\\sum_{i=1}P(A|B_i)P(B_i)}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Proof}$. Let's consider any event $B \\in \\Omega$, such that $P(B)>0$. Using the total probability theorem on the conditional probability statement, we get:\n",
    "\n",
    "$$P(B|A) = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(B \\cap A)}{\\sum_{i}P(A|B_i)P(B_i)}.$$\n",
    "\n",
    "Now we use the definition of conditional probability:\n",
    "\n",
    "$$P(B \\cap A) = P(A \\cap B) = P(A|B) \\cdot P(B).$$\n",
    "\n",
    "Substituting this in the expression for $P(B|A)$ we immediately obtain the result:\n",
    "\n",
    "$$P(B|A) = \\frac{P(A|B) \\cdot P(B)}{\\sum_{i}P(A|B_i)P(B_i)}.$$\n",
    "\n",
    "This is true for any event $B$ and so, replacing $B$ by $B_k$, we get:\n",
    "\n",
    "$$P(B_k|A) = \\frac{P(A|B_k)P(B_k)}{\\sum_{i=1}P(A|B_i)P(B_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Naive Bayes classifier</h3>\n",
    "\n",
    "- **What is a Classifier?**\n",
    "<br> A classifier is a machine learning model that is used to discriminate different objects based on certain features.\n",
    "\n",
    "- **Principle of Naive Bayes Classifier:**\n",
    "<br> A Naive Bayes classifier is a probabilistic machine learning model thatâ€™s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "\n",
    "- **What does Naive Bayes mean?**\n",
    "<br> Naive Bayes classifiers assume strong, or **naive**, independence between the events.\n",
    "\n",
    "- **Where is Naive Bayes Classifier used?**\n",
    "<br> Popular uses of Naive Bayes Classifiers include **spam filters**, **text analysis** and **medical diagnosis**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- On the **Workshop â„–3** we will solve the **problem of playing golf** using the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Random variable</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space.\n",
    "\n",
    "$\\textbf{Definition}$. A function $X: \\Omega \\to \\mathbb{R}$ is called a **random variable** if for any Borel set $\\mathcal{B} \\in \\mathbb{R}$, the set $X^{-1}(\\mathcal{B})$ is an event in $\\Sigma$. The probability that $X$ takes on a set $\\mathcal{B} \\in \\mathbb{R}$ is written as:\n",
    "\n",
    "$$P(X \\in \\mathcal{B}) = P(X^{-1}(\\mathcal{B}))= P(\\{\\omega \\in \\Omega | X(\\omega) \\in \\mathcal{B} \\}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Definition}$. Two random variables $X$ and $Y$ are independent if:\n",
    "\n",
    "$$P(X \\in \\mathcal{A}, Y \\in \\mathcal{B}) = P(X \\in \\mathcal{A})\\cdot P(Y \\in \\mathcal{B}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\\textbf{Definition}$. The **Cumulative Distribution Function** (**CDF**) $F_X:\\mathbb{R} \\to [0, 1]$ of a real-valued **random variable** $X$ is the function given by:\n",
    "\n",
    "$$ F_X(x) = P(X \\le x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Theorem} \\space \\textbf{20}$. A function $F_X:\\mathbb{R} \\to [0, 1]$ is cumulative distribution function if and only if:\n",
    "- $F$ is **non decreasing**, i.e. for each $x_1 \\lt x_2$ we have $F(x_1) \\le F(x_2)$;\n",
    "- $F$ is **normalized**, i.e. $$\\lim_{x \\to -\\infty}F(x) = 0 \\text{ and } \\lim_{x \\to +\\infty}F(x) = 1;$$\n",
    "- $F$ is **rights continuous**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{Proof}$. \n",
    "The proofs of this theorem we leave to the students!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discret random variable</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Random variable $X$ is discrete if it takes countably (or finite) many values $\\{x_1, x_2, \\cdots \\}$.\n",
    "\n",
    "We can define the probability mass function: $f_X(x)=P(X=x)$. This function will have next properties:\n",
    "\n",
    "&emsp; $\\bullet$ $f_X(x) \\geq 0$  for each $x \\in \\mathbb{R}$;\n",
    " \n",
    "&emsp; $\\bullet$ $\\sum_{x}f_X(x) = 1.$\n",
    "\n",
    "Thus, the cumulative distribution function can be defines as:\n",
    "\n",
    "$$F_X(x) = P(X \\le x) = \\sum_{x_i \\le x}f_X(x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples: Bernoulli distribution</h3>\n",
    "\n",
    "- $\\operatorname{Bernoulli}(p)$: we have an experiment with two outcomes with probability $p$ and $q=(1 - p)$:\n",
    "\n",
    "$$\\operatorname{P}(X = k) = p^kq^{1 - k} \\text{ for } k \\in \\{0, 1\\}.$$\n",
    "\n",
    "- **Example**: Any problem where events having exactly two outcomes.\n",
    "\n",
    "<center><img src=\"images/RM_Bernoulli.svg\" width=\"1500\" height=\"300\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples: Binomial distribution</h3>\n",
    "\n",
    "- $\\operatorname{Binomial}(n, p)$: we have $n$ experiments with two outcomes with probability $p$ and $q=(1-p)$ each:\n",
    "\n",
    "$$\n",
    "\\operatorname{P}(X = k) = \n",
    "\\left\\{\\begin{matrix}\n",
    "C_n^k p^kq^{n-k}, & \\text{ for } k = 0, 1, \\dots, n \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{matrix}\\right.\n",
    ";$$\n",
    "\n",
    "- **Example**: Any problem where you have $n$ events and each event has exactly two outcomes.\n",
    "\n",
    "<center><img src=\"images/RM_Binomial.svg\" width=\"800\" height=\"300\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples: Geometric distribution</h3>\n",
    "\n",
    "- $\\operatorname{Geometric}(k)$: we have $k$ experiments with two outcomes until the first success.\n",
    "$$\\operatorname{P}(X=k) = p(1-p)^{k-1}.$$\n",
    "- **Example**: What is probability  that there are $k$ failures to get the first success in $k$ Bernoulli trials?\n",
    "<center><img src=\"images/RM_Geometric.svg\" width=\"1000\" height=\"300\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example: Hypergeometric distribution</h3>\n",
    "\n",
    "- $\\operatorname{Hypergeometric}(N,K,n)$:\n",
    "<br>\n",
    "$$\\operatorname{P}(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}}.$$\n",
    "- **Example**: Suppose a deck of cards contains 20 cards: 6 red cards and 14 black cards. 5 cards are drawn randomly without replacement. What is the probability that exactly 4 red cards are drawn?\n",
    "<center><img src=\"images/RM_Geometric.svg\" width=\"800\" height=\"300\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example: Poisson distribution</h3>\n",
    "\n",
    "- $\\operatorname{Poisson}(k, \\lambda)$:\n",
    "<br>\n",
    "$$\\operatorname{P}(X = k) = \\frac{e^{-\\lambda} \\lambda^{k}}{k!}.$$\n",
    "- **Example**: Poisson distribution is applied in situations where there are a large number of independent Bernoulli trials with a very small probability of success.\n",
    "<center><img src=\"images/RM_Poisson.svg\" width=\"800\" height=\"300\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Continuous random variable</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Random variable $X$ is continouos if there exists a positivly defined function $f:\\mathbb{R} \\to \\mathbb{R}$, <br> i.e.  $f(x) \\geq 0$ for all $x \\in \\mathbb{R}$, such that:\n",
    "\n",
    "$$P(a \\lt x \\lt b) = \\int_{a}^{b}f(x)dx;$$\n",
    "\n",
    "The function $f(x)$ is called **Probability Desity Function** (**PDF**) and we have that: \n",
    "\n",
    "$$F_X(x)=\\int_{-\\infty}^{x}f(t)dt,$$\n",
    "\n",
    "and\n",
    "$$f(x) = F_X'(x) \\text{ for all points } x \\in \\mathbb{R} \\text{ where } F_X \\text{ is differentiable}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples: Uniform distributions</h3>\n",
    "\n",
    "&emsp;$\\bullet$ $\\operatorname{Uniform distribution}$: \n",
    "<br> &emsp; &emsp; $\\bullet$ Discrete: $f(x) = \n",
    "\\left\\{\\begin{matrix}\n",
    "\\frac{1}{n}, & \\text{ for } x = 1, 2, \\dots, n \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{matrix}\\right.\n",
    ";$\n",
    "<br> &emsp;&emsp; $\\bullet$ Continuous: \n",
    " $ f(x) = \n",
    "\\left\\{\\begin{matrix}\n",
    "\\frac{1}{b-a}, & \\text{ for } x \\in [a, b] \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{matrix}\\right.\n",
    ".$\n",
    "\n",
    "<center><img src=\"images/RM_Uniform.svg\" width=\"1200\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples: Normal distribution</h3>\n",
    "\n",
    "- $\\operatorname{Gaussian}(\\mu, \\sigma^2)$, where $\\mu \\in \\mathbb{R}$ and $\\sigma \\gt 0$, :\n",
    "$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp{(-\\frac{1}{2\\sigma^2}(x - \\mu)^2)}\n",
    ".$\n",
    "\n",
    "    We say that $X$ had standard normal distribution if $\\mu=0$ and $\\sigma=1$.\n",
    "\n",
    "<center><img src=\"images/RM_Normal.svg\" width=\"1500\" height=\"300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<h3 align=\"center\">Expected value</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Let $(\\Omega, \\Sigma, P)$ be a probability space. The **expectation** of a random variable $X$ is defined as:\n",
    "$$\\operatorname{E} [X]  = \\int_\\Omega X(\\omega)\\,d\\operatorname{P}(\\omega).$$\n",
    "\n",
    "For discret and continuous random variables it takes the following form:\n",
    "$$\n",
    "\\operatorname{E}[X] = \n",
    "\\left\\{\\begin{matrix}\n",
    "\\sum_x xf(x), & \\text{ if } X \\text { is discrete}; \\\\\n",
    "\\int_{\\mathbb{R}} x f(x)\\, dx, & \\text{ if } X \\text { is continuous};\n",
    "\\end{matrix}\\right.\n",
    ".$$\n",
    "\n",
    "In many cases **expectation** is denoted by $\\mu$.\n",
    "\n",
    "Expectation value has the following properties:\n",
    "<br> &emsp; $\\bullet$ $\\operatorname{E}[X + Y] = \\operatorname{E}[X] + \\operatorname{E}[Y];$\n",
    "<br> &emsp; $\\bullet$ $\\operatorname{E}[aX]    = a \\operatorname{E}[X];$\n",
    "<br> &emsp; $\\bullet$ If $X_1, X_2, \\dots, X_n$ are independent random variables, then:\n",
    "$$\\operatorname{E} \\left [\\prod_{i=1}^nX_i \\right ] = \\prod_{i=1}^n \\operatorname{E} \\left [X_i \\right ].$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Variance</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. The **variance** of a random variable $X$ is the expected value of the squared deviation from the mean of $X$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\operatorname{Var}(X) &= \\operatorname{E}\\left[(X - \\operatorname{E}[X])^2\\right] \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2\\right] \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[X^2\\right] - 2\\operatorname{E}[X]\\operatorname{E}[X] + \\operatorname{E}[X]^2 \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[X^2 \\right] - \\operatorname{E}[X]^2.\n",
    "\\end{align}$$\n",
    "\n",
    "In many cases expectation is denoted by $\\sigma^2$.\n",
    "\n",
    "Variance has the following properties:\n",
    "<br> &emsp; $\\bullet$ $\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y);$\n",
    "<br> &emsp; $\\bullet$ $\\operatorname{Var}(\\alpha \\cdot X) = \\alpha^2\\operatorname{Var}(X).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Covariance</h3>\n",
    "\n",
    "\n",
    "$\\textbf{Definition}$. Let $X$ and $Y$ be a random variables. The **covariance** is defined as the expected value of the product of their deviations from their individual expected values:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\operatorname{Cov}(X, Y) &= \\operatorname{E}\\left[(X - \\operatorname{E}[X]) (Y - \\operatorname{E}[Y])\\right] \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[XY - X\\operatorname{E}[Y] - Y\\operatorname{E}[X] + \\operatorname{E}[X]\\operatorname{E}[Y] \\right] \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[XY\\right] - \\operatorname{E}[X]\\operatorname{E}[Y] - \\operatorname{E}[X]\\operatorname{E}[Y] + \\operatorname{E}[X]\\operatorname{E}[Y] \\\\[4pt]\n",
    "&= \\operatorname{E}\\left[XY \\right] - \\operatorname{E}[X]\\operatorname{E}[Y].\n",
    "\\end{align}$$\n",
    "\n",
    "Covariance has the following property:\n",
    "$$-1 \\leq Cov(X, Y) \\leq 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Covariance Matrix</h3>\n",
    "\n",
    "$\\textbf{Definition}$. Let $X = (X_1, X_2, ... , X_n)$ and $Y = (Y_1, Y_2, ... , Y_n)$ be the random vector variables with finite variance and expected value. \n",
    "The covariance matrix $\\operatorname{K}_{XX}$ is the matrix whose $(i,j)$ entry is:\n",
    "\n",
    "$$\\operatorname{K}_{X_i X_j} = \\operatorname{cov}[X_i, X_j] = \\operatorname{E}[(X_i - \\operatorname{E}[X_i])(X_j - \\operatorname{E}[X_j])].$$\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$\n",
    "\\operatorname{K}_{\\mathbf{X}\\mathbf{X}}=\\begin{bmatrix}\n",
    " \\mathrm{E}[(X_1 - \\operatorname{E}[X_1])(X_1 - \\operatorname{E}[X_1])] & \\mathrm{E}[(X_1 - \\operatorname{E}[X_1])(X_2 - \\operatorname{E}[X_2])] & \\cdots & \\mathrm{E}[(X_1 - \\operatorname{E}[X_1])(X_n - \\operatorname{E}[X_n])] \\\\ \\\\\n",
    " \\mathrm{E}[(X_2 - \\operatorname{E}[X_2])(X_1 - \\operatorname{E}[X_1])] & \\mathrm{E}[(X_2 - \\operatorname{E}[X_2])(X_2 - \\operatorname{E}[X_2])] & \\cdots & \\mathrm{E}[(X_2 - \\operatorname{E}[X_2])(X_n - \\operatorname{E}[X_n])] \\\\ \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n",
    " \\mathrm{E}[(X_n - \\operatorname{E}[X_n])(X_1 - \\operatorname{E}[X_1])] & \\mathrm{E}[(X_n - \\operatorname{E}[X_n])(X_2 - \\operatorname{E}[X_2])] & \\cdots & \\mathrm{E}[(X_n - \\operatorname{E}[X_n])(X_n - \\operatorname{E}[X_n])]\n",
    "\\end{bmatrix}\n",
    ".$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Sample Covariance</h3>\n",
    "\n",
    "- **Sample covariance** measures how two random vector variables $X = (X_1, X_2, \\cdots , X_n)$ and $Y = (Y_1, Y_2, \\cdots , Y_n)$ differ from their mean:\n",
    "$$Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y}).$$\n",
    "- Covariance can be understood as the **variability due to codependence**, whereas the variance is the **independent variability**.\n",
    "\n",
    "- **Positive covariance**: two variables are both above or both below their respective means. Variables with a positive covariance are **positively correlated** - they go up or done together. \n",
    "\n",
    "- **Negative covariance**: one variable tends to be above the mean and the other below their mean in other words, **negative covariance** means that if one variable goes up, the other variable goes down.\n",
    "\n",
    "- Similar to variance, the dimension of the covariance is ${unit}^2$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Convergence of random variables</h3>\n",
    "\n",
    "$\\textbf{Definition}$. A sequence $X_1, X_2, X_3, \\dots$ of random variables is said to **converge weakly** or **converge in distribution** to a random variable $X$ if:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} F_n(x)=F(x),$$\n",
    "\n",
    "for every number $x \\in \\mathbb {R}$ at which $F$ is continuous. \n",
    "<br>Here $F_n$ and $F$ are the cumulative distribution functions of random variables $X_n$ and $X$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\\textbf{Definition}$. A sequence $X_1, X_2, X_3, \\dots$ of random variables is said to **converge strongly** or **converge almost surely** to a random variable $X$ if:\n",
    "\n",
    "$$P\\left( \\lim_{n \\to \\infty} X_n = X \\right)=1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Law of large numbers</h3>\n",
    "\n",
    "$\\textbf{Concept}$. The law of large numbers states that as a sample size grows, its mean gets closer to the average of the whole population. \n",
    "\n",
    "Let  $X_1, X_2, \\cdots$ be a **independent and identically distributed** random variables, then:\n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{20}$. The **weak law of large numbers**, or **Khinchin's law**, states that the sample average **converges in probability** towards the expected value:\n",
    "$$\\lim_{n \\to \\infty}\\overline{X}_n \\xrightarrow {P} \\mu,$$\n",
    "i.e for any positive number $\\epsilon$:\n",
    "$$\\lim_{n \\to \\infty} P(|\\overline{X}_n - \\mu| > \\epsilon) = 0.$$\n",
    "$\\textbf{Theorem} \\space \\textbf{21}$. The **strong law of large numbers** states that the sample average converges **almost surely** to the expected value:\n",
    "$$ P \\left( \\lim _{n\\to \\infty } \\overline{X}_n = \\mu \\right)=1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Central Limit Theorem</h3>\n",
    "\n",
    "$\\textbf{Concept}$. Given certain conditions, the arithmetic mean of a **sufficiently large number** of iterates of **independent random variables**, each with a well-defined expected value and well-defined variance, will be approximately **normally distributed**, regardless of the underlying distribution.\n",
    "\n",
    "Let, $X_{1}, X_{2}, ..., X_{N}$ be a sequence of independent and identically distributed random variables with the mean $\\operatorname{E}[X_{i}]=\\mu$ and finite variance $Var[X_{i}]=\\sigma^2<\\infty$. \n",
    "\n",
    "$\\textbf{Theorem} \\space \\textbf{21}$. As $n$ approaches infinity, the random variables $\\frac{\\sqrt{n}}{\\sigma} \\left( S_n - \\mu \\right)$ converge in distribution to a normal $N(0, 1)$:\n",
    "$$\\lim_{n \\to \\infty} \\frac{\\sqrt{n}}{\\sigma}\\left( S_n - \\mu\\right) \\xrightarrow{d} N(0, 1),$$\n",
    "\n",
    "where $S_n$ is a sample mean for a choosen $n$:\n",
    " $$S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i.$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
