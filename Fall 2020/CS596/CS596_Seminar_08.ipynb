{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **4 November 2020**\n",
    "\n",
    "\n",
    "Title: **Seminar 8**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Sources:\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Time Series Prediction with LSTM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem we are going to look at in this post is the **International Airline Passengers** prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a **year** and a **month**, the task is to predict the **number** of international airline **passengers** in units of 1,000. \n",
    "\n",
    "\n",
    "- The **data** ranges from **January 1949** to **December 1960**, or **12 years**, with **144 observations**.\n",
    "\n",
    "\n",
    "- We can load this dataset easily using the **Pandas** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import google colab library for loading dataset files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "dataframe = pandas.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "plt.plot(dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, there is an **upward trend** in the dataset over time.\n",
    "\n",
    "\n",
    "- You can also see some **periodicity** to the dataset that probably **corresponds** to the **Northern Hemisphere vacation** period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Long Short-Term Memory Network</h1>\n",
    "\n",
    "- The **Long Short-Term Memory** network, is a **recurrent neural network** that is trained using **Backpropagation** Through Time and **overcomes** the **vanishing gradient problem**.\n",
    "\n",
    "\n",
    "- As such, it can be used to create **large recurrent networks** that in turn can be used to **address difficult sequence problems** in machine learning and achieve state-of-the-art results.\n",
    "\n",
    "\n",
    "- Instead of neurons, **LSTM networks** have **memory blocks** that are connected through layers.\n",
    "\n",
    "\n",
    "- A **block** has components that make it smarter than a classical neuron and a memory for recent sequences. \n",
    "\n",
    "\n",
    "\n",
    "- A **block contains gates** that manage the block’s state and output. \n",
    "  \n",
    "\n",
    "- A block operates upon an input sequence and **each gate** within a block **uses** the **sigmoid activation units** to **control whether they are triggered or not**, making the change of state and addition of information flowing through the block conditional.\n",
    "\n",
    "\n",
    "- There are **three types of gates** within a **unit**:\n",
    "\n",
    "  - **Forget Gate**: conditionally decides what information to throw away from the block.\n",
    " \n",
    "  - **Input Gate**: conditionally decides which values from the input to update the memory state.\n",
    "  \n",
    "  - **Output Gate**: conditionally decides what to output based on input and the memory of the block.\n",
    "  \n",
    "  <center><img src=\"images/L9_LSTM.png\" width=\"800\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- We will see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM Network for Regression</h1>\n",
    "\n",
    "- We can phrase the problem as a **regression problem**.\n",
    "\n",
    "\n",
    "- That is, **given** the **number of passengers this month**, what is the **number of passengers next month**?\n",
    "\n",
    "\n",
    "- We can write a simple **function** to **convert our single column of data** into a **two-column dataset**:\n",
    "\n",
    "  - the **first column** containing **this month’s** ($t$) passenger count;\n",
    "  - the **second column** containing **next month’s** ($t+1$) passenger count.\n",
    "\n",
    "- Before we get started, let’s first import all of the functions and classes we intend to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's fix the **random number seed** to ensure our **results are reproducible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we **extract** the **NumPy array** from the **dataframe** and **convert** the integer values to **floating point values**, which are **more suitable** for modeling with a **neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LSTMs** are **sensitive** to the **scale of the input data**, specifically when the **sigmoid** or **tanh activation functions** are used.\n",
    "\n",
    "\n",
    "- We can easily normalize the dataset using the **MinMaxScaler** preprocessing class from the **scikit-learn** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After we model our data and estimate the skill of our model on the training dataset, we need to get an idea of the skill of the model on new unseen data. \n",
    "\n",
    "\n",
    "- For a **normal classification** or **regression problem**, we would do this using **cross validation**.\n",
    "\n",
    "\n",
    "- With **time series data**, the **sequence of values is important!** \n",
    "\n",
    "\n",
    "- A **simple method** that we can use is to **split the ordered dataset** into **train** and **test datasets**. \n",
    "\n",
    "\n",
    "- The **code** below **calculates** the **index of the split point** and **separates the data** into the **training datasets** with $67\\%$ **of the observations** that we can use to train our model, leaving the **remaining** $33\\%$ for **testing the model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size  = int(len(dataset) * 0.67)\n",
    "test_size   = len(dataset) - train_size\n",
    "\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can define a function to **create a new dataset**.\n",
    "\n",
    "\n",
    "- The **function takes two arguments**: \n",
    "\n",
    "  - the **dataset**, which is a **NumPy array** that we want to convert into a dataset\n",
    "  \n",
    "  - the **look_back**, which is the **number of previous time steps to use** as input variables to predict the next time period.\n",
    "  \n",
    "\n",
    "- In our case we choose the default value for **look_back** equal to $1$.\n",
    "  \n",
    "\n",
    "- This default **will create** a **dataset** where $X$ is the **number of passengers** at a **given time** ($t$) and $Y$ is the **number of passengers** at the **next time** ($t + 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Currently, our data is in the form: **[samples, features]** and we are framing the problem as one time step for each sample. \n",
    "\n",
    "\n",
    "- We can **transform** the prepared **train** and **test input data** into the **expected structure** using **numpy.reshape()** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now **ready to design** and fit our **LSTM network** for this problem.\n",
    "\n",
    "\n",
    "- The **network** has:\n",
    "\n",
    "  - a **visible layer** with **1 input**\n",
    "  \n",
    "  - a **hidden layer** with **4 LSTM blocks** or neurons, \n",
    "  - an **output layer** that **makes a single value prediction**. \n",
    "  \n",
    "  \n",
    "- The **default sigmoid activation function** is used for the **LSTM blocks**. \n",
    "\n",
    "\n",
    "- The **network** is **trained** for $100$ **epochs** and a **batch size** of $1$ is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Once the model is fit**, we can **estimate the performance** of the model on the **train** and **test datasets**. \n",
    "\n",
    "\n",
    "- **Note** that we **invert the predictions before calculating error scores** to ensure that **performance is reported in the same units** as the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we can **generate predictions** using the model for both the **train** and **test dataset** to get a visual indication of the skill of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the **model** did an **excellent job** of **fitting both** the **training** and the **test datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the **model** has an **average error** of about:\n",
    "\n",
    "  - $23$ **passengers** (in thousands) on the **training dataset**\n",
    "  - $52$ **passengers** (in thousands) on the **test dataset**. \n",
    "  \n",
    "  Not that bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM for Regression Using the Window Method</h1>\n",
    "\n",
    "- We can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with window regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the error was increased slightly compared to that of the previous section. \n",
    "\n",
    "- The window size and the network architecture were not tuned: this is just a demonstration of how to frame a prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM for Regression with Time Steps</h1>\n",
    "\n",
    "- You may have noticed that the data preparation for the LSTM network includes time steps.\n",
    "\n",
    "\n",
    "- Time steps provide **another way to phrase our time series problem**. \n",
    "\n",
    "\n",
    "- Like above in the window example, we can **take prior time steps** in our time series as **inputs** to **predict** the **output** at the **next time step**.\n",
    "\n",
    "\n",
    "- We can do this **using** the **same data representation** as in the previous window-based example, **except** when we **reshape the data**, we set the **columns to be the time steps dimension** and change the **features dimension back** to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with time step regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "    dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the **results are slightly better** than **previous example**, although the **structure** of the input data **makes a lot more sense**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM with Memory Between Batches</h1>\n",
    "\n",
    "- The **LSTM** network **has memory**, which is capable of **remembering across long sequences**.\n",
    "\n",
    "\n",
    "- Normally, the **state** within the network is **reset after each training batch** when fitting the model, as well as each call to **model.predict()** or **model.evaluate()**.\n",
    "\n",
    "\n",
    "- We can **gain finer control** over when the **internal state** of the LSTM network is cleared in Keras by making the LSTM layer **stateful**. \n",
    "\n",
    "  This means that it can **build state over the entire training sequence** and even maintain that state if needed to make predictions.\n",
    "\n",
    "\n",
    "- It **requires** that the **training data not be shuffled** when fitting the network.\n",
    "\n",
    "\n",
    "- It also **requires explicit resetting** of the network **state** after each **epoch** by calls to **model.reset_states()**. \n",
    "\n",
    "  This means that we must **create our own outer loop** of **epochs** and within **each epoch** call **model.fit()** and **model.reset_states()**. \n",
    "  \n",
    "  For example like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, when the **LSTM layer** is constructed, the **stateful parameter** must be **set True** and instead of specifying the input dimensions, we must **hard code** the **number of samples in a batch**, **number of time steps** in a sample and **number of features** in a time step by **setting** the **batch_input_shape parameter**. \n",
    "\n",
    "  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(4, batch_input_shape=(batch_size, time_steps, features), stateful=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(trainX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can adapt the previous time step example to use a stateful LSTM. \n",
    "\n",
    "- The full code listing is provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "for i in range(100):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do see that **results are worse**. \n",
    "\n",
    "\n",
    "- The **model** may **need more modules** and may need to be **trained for more epochs** to internalize the structure of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Seminar</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
