{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **2 September 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 2: Linear Regression**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Bibliography:\n",
    "<br>\n",
    "[1] Bishop, Christopher M., *Pattern Recognition and Machine Learning*, Springer, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Getting-started Example</h3>\n",
    "\n",
    "- Lets consider the example of recognizing handwritten digits taken from US zip code:\n",
    "\n",
    "<center><img src=\"images/L2_Handwriting.png\" width=\"500\" alt=\"Example\" /></center>\n",
    "\n",
    "- Each digit corresponds to a $28 \\times 28 = 784$ pixel grayscale image and so can be represented by a vector $X = [x_1, x_2, ..., x_{784}]$ real numbers.\n",
    "\n",
    "- The goal is to **build a machine** that will take such a vector $X$ as input and gives the correctly assigned digit $\\mathbf{y} = [0, . . . , 9]$ as the output.\n",
    "\n",
    "\n",
    "- This is a nontrivial problem due to the wide variability of handwriting. **How to solve it**?\n",
    "\n",
    "  - One approach could be tackled using handcrafted rules for distinguishing the digits based on the shapes of the strokes and so on, but this gives poor results.\n",
    "  \n",
    "  \n",
    "  - Far better results can be obtained by **adopting a machine learning approach** in which a large set of $M$ vectors $\\mathbf{X} = [X_1, X_2, ..., X_M]$ called a **training set** is used to tune $N$ parameters of an adaptive model $\\theta =[\\theta_1, \\theta_2, ..., \\theta_N]$.\n",
    "  \n",
    "- The categories of the digits in the **training set** are known in advance, typically by inspecting them individually and hand-labelling them.\n",
    "\n",
    "- We can express the category of a digit using **target vector** $\\mathbf{y}= [0, 1, ..., 9]$, which represents the identity of the corresponding digit.\n",
    "\n",
    "\n",
    "- The **result of running the machine learning algorithm** can be expressed as a **function $f(X)$** which takes a new digit image $X$ **as input** and that generates **$\\hat{y}$ as an output**, encoded in the same way as the target vectors.\n",
    "\n",
    "\n",
    "- The ability to **categorize correctly new examples** that differ from those used for training is known as **generalization**.\n",
    "\n",
    "\n",
    "**Question**: What type of problem is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regression Model</h3>\n",
    "\n",
    "- In general, researchers:\n",
    "  - **select a model** they would like to estimate,\n",
    "  - **use chosen method** to estimate the parameters of that model.\n",
    " \n",
    " \n",
    "- Regression models involve the following components:\n",
    "  - The **unknown parameter**s, often denoted as a scalar or vector $\\boldsymbol{\\theta}$.\n",
    "  - The **independent variables**, which are observed in data and are often denoted as a vectors $X_i$.\n",
    "  - The **dependent variable**, which are observed in data and often denoted as a scalar $y_{i}$.\n",
    "  - The **error terms**, which are not directly observed in data and are often denoted as a scalar $\\varepsilon_i$.\n",
    "\n",
    "\n",
    "- **Regression models** propose that **$y_{i}$ is a function of $X_{i}$** and $\\boldsymbol{\\theta}$, with $\\varepsilon_i$ representing an **additive error term**:\n",
    "\n",
    "$$y_i = f(X_i, \\boldsymbol{\\theta}) + \\varepsilon_i.$$\n",
    "\n",
    "\n",
    "- The researchers' **goal** is to **estimate the function** $f(X_i, \\boldsymbol{\\theta})$ that most closely fits the data $X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regression Problem</h3>\n",
    "\n",
    "- Suppose we want to study a signal observed from a **sinusoidal signal** source such as an **Slternating Current**.\n",
    "\n",
    "- The wave shape of **Alternating Current** is shown below:\n",
    "\n",
    "<center><img src=\"images/L2_Sine_Wave.jpg\" width=\"300\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- Now, assume, that we are given a training set comprising $M$ observations of $x$ so that **data** is generated from the **true function** $y_{true}(x) = \\sin 2\\pi x$, with **random noise** $\\varepsilon$ included in the **target values** $y$.\n",
    "\n",
    "\n",
    "- Below you can see an example of a training data set of $M = 10$  points, shown as **blue circles**. \n",
    "\n",
    "- The green curve shows the true function $\\sin 2\\pi x$ used to generate the data. \n",
    "\n",
    "\n",
    "<center><img src=\"images/L2_sin2x.png\" width=\"500\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- Our **goal** is to predict the **target value** of $\\hat{y}$ for some **new value** of $\\hat{x}$, without knowledge of the green curve. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Curve Fitting using Polynomial Function</h3>\n",
    "\n",
    "- We will try to **fit the data** using a **polynomial function** of the form:\n",
    "\n",
    "  $$y(x, \\mathbf{\\theta}) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\cdots + \\theta_N x^N$$\n",
    "  \n",
    "  where $M$ is the **order of the polynomial**.\n",
    "  \n",
    "  The polynomial coefficients $\\theta_0, ..., \\theta_N$ are collectively denoted by the vector $\\theta$.\n",
    "  \n",
    "  \n",
    "- Note that, although the **polynomial function** $y(x,\\theta)$ is a **nonlinear function** of $x$, it is a linear function of the coefficients $\\theta$, this is called **linear model**.\n",
    "\n",
    "\n",
    "- The values of the coefficients $\\theta$ will be **determined by fitting the polynomial** to the training data.\n",
    "\n",
    "\n",
    "\n",
    "- This can be done by **minimizing an error Function E** that measures the **misfit between the function** $y(x,\\theta)$, for any given value of $\\theta$ using the training set data points $x_1, ..., x_M$:\n",
    "\n",
    "  $$E(\\theta) = \\frac{1}{2} \\sum_{m = 1}^{M} \\left ( y_m - \\hat{y_m} \\right )^2,$$\n",
    "  \n",
    "  where $\\hat{y_m} = y(x_m, \\theta).$\n",
    "  \n",
    "  \n",
    "- In other words, our task is to find $\\hat{\\boldsymbol{\\theta}}$:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}}E(\\boldsymbol{\\theta}).$$\n",
    "\n",
    "\n",
    "- The error function $E$ is shown below for clarity.\n",
    "- Here the **displacements** are shown by the **vertical green bars**.\n",
    "\n",
    "<center><img src=\"images/L2_Error_Function.png\" width=\"500\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multivariate Linear Regression Problem</h3>\n",
    "\n",
    "\n",
    "- **Linear regression** model assumes that the relationship between the **dependent variable** $y_i$ and the \n",
    "**independent variables** (**regressors**) $X_i$ is **linear** (we assume for a while that $\\varepsilon_i = 0$), i.e. the **regresion function** takes the form:\n",
    "\n",
    "$$f(X_i, \\boldsymbol{\\theta}) = \\theta_1 X_1 + \\theta_2 X_2 + \\cdots + \\theta_N X_N.$$\n",
    "\n",
    "\n",
    "- Suppose we have $M$ training examples $(X_i, y_i)$ and $N$ features $X_i = [x_{i1}, x_{i2}, \\cdots, x_{iN} ]^T\\in \\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "- We can represent $X_i$ as a **Design Matrix**:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1N}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{1N}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{M1} &  x_{M2} &\\cdots & x_{MN}\\\\\n",
    "\\end{bmatrix}, \\text{ and } y_i \\text{ as a vector }\\mathbf{y} = \\begin{bmatrix}\n",
    "y_{1}\\\\ \n",
    "y_{2}\\\\ \n",
    "\\vdots\\\\\n",
    "y_{M}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "- Thus we have a **system of linear equations**, known as **Normal equations**:\n",
    "\n",
    "$$ X  \\boldsymbol{\\theta} = \\mathbf{y}.$$\n",
    "\n",
    "\n",
    "- **How do we solve it**? \n",
    "- And if there's no solution, **how do we find the best possible $\\boldsymbol{\\theta}$**?\n",
    "\n",
    "\n",
    "- There are two approach:\n",
    " - solving **Normal Equation**, i.e. analytical solution (**see Appendix A**);\n",
    " - using **Gradient Descent** method, i.e. iterative process (**see Appendix B**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient Descent vs Normal Equation</h3>\n",
    "\n",
    "|                                                      --------------------------Gradient Descent--------------------------                                                      |                                --------------------------Normal Equation--------------------------                                |\n",
    "|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|\n",
    "| An **iterative process**, <br> i.e. many iterations are required  to achieve the required accuracy | **No iterations required**, i.e computed in one step                              |\n",
    "| **No inverse computation is required**                                                                                         | **Computation of the inverse matrix $(X^TX)^{-1}$ is required**      |\n",
    "|Works well with **large number of features** $n$| **Very slow** if number of features $n$ is large: $n \\geq 10^4$|\n",
    "| Time complexity is $O(nC\\log(1/\\varepsilon))$  for **GD**, and <br > Time complexity is $O(C/\\varepsilon)$ for **SGD**| Time complexity is $O(n^3)$ |\n",
    "| An iterative process **may not converge**                                                                                      | **May not have an analytical solution**, i.e. $X^TX$ is not-invertible               |\n",
    "| **Has a hyperparameter** (learning rate $\\alpha$)                                                                              | **Does not have a hyperparameter**                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Residuals and Coefficient of Determination</h3>\n",
    "\n",
    "- In **regression analysis**, the difference between the **observed value** of the dependent variable $\\mathbf{y}$ and the **predicted value** $\\mathbf{\\hat{y}}$ is called the **residual** $\\mathbf{e}$. \n",
    "\n",
    "- Each data point has one residual:\n",
    "\n",
    "$$e_i = y_i - \\hat{y_i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Least Square Error</h3>\n",
    "\n",
    "- When there's **no solution** to the system, we **fit the data as good as possible**.\n",
    "\n",
    "\n",
    "- We try to predict such $\\hat{\\mathbf{y}}$ that minimizes the residuals $\\mathbf{e}$.\n",
    "\n",
    "- One way of measuring the **performance of the model** is to compute the **norm of the residuals**, known also as **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$E(\\boldsymbol{\\theta}) = \\| \\mathbf{e}\\|^2 = \\|\\mathbf{y} - \\hat{\\mathbf{y}} \\|^2.$$\n",
    "\n",
    "- So our task is to find $\\hat{\\boldsymbol{\\theta}}$:\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}}E(\\boldsymbol{\\theta}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">What is Norm?</h3>\n",
    "\n",
    " - A **norm** on a vector space $X$, is a function $\\left \\| \\text{ . } \\right \\|: \\mathcal{X} \\to \\mathbb{R}$, with the following properties:\n",
    "\n",
    "- $\\left \\| x \\right \\| \\geq 0$, for all $x \\in X$;\n",
    "\n",
    "- $\\left \\| x \\right \\| = 0$ implies that $x = 0$.\n",
    "\n",
    "- $\\left \\| \\alpha x \\right \\| = |\\alpha| \\cdot \\left \\| x \\right \\|$, for all $x \\in X$ and $\\alpha \\in \\mathbb{R}$;\n",
    "\n",
    "- $\\left \\| x + y \\right \\| \\leq \\left \\| x \\right \\| + \\left \\| y \\right \\|$, for all $x,y \\in X$;\n",
    "\n",
    "\n",
    "- In other words, the **norm** definition states that:\n",
    "  - norm is **nonnegative**;\n",
    "  - norm is **strictly positive**;\n",
    "  - norm is **homogenous**;\n",
    "  - norm satisfies the **tirangle inequality**.\n",
    "\n",
    "\n",
    " - A **normed vector space** $(X, \\left \\| \\text{ . } \\right \\|)$ is a vector space $X$ equipped with a norm $\\left \\| \\text{ . } \\right \\|$.\n",
    " \n",
    " \n",
    "- Given **different norms**, we get **different MSEs**, so the **performance** of the **machine learning algorithm will differ**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples of Norm</h3>\n",
    "\n",
    "- Let $X = \\mathbb{R}^n$ be a vector space over an ordered field $\\mathbb{R}$, and let $p \\geq 1$ be a real number $p\\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "- The $p$-**norm** of a vector $x = \\{\\xi_1, ..., \\xi_n\\}$, also known as **Hölder norm**,  is defined as:\n",
    "\n",
    "$$\\left \\| \\text{ x } \\right \\|_{p} = \\left ( \\sum_{i=1}^{n} |\\xi_i|^p \\right )^{\\frac{1}{p}}.$$\n",
    "\n",
    "- When $p=1$, we get the $L_1$ norm, also known as **Manhattan Distance** or **Taxicab norm**:\n",
    "\n",
    "$$\\left \\| \\text{ x } \\right \\|_{1} = \\sum_{i=1}^{n} |\\xi_i|.$$\n",
    "- When $p=2$, we get the $L_2$ norm, also known as **Euclidian norm**: \n",
    "\n",
    "$$\\left \\| \\text{ x } \\right \\|_{2} = \\sqrt{ \\sum_{i=1}^{n} |\\xi_i|^2}$$\n",
    "\n",
    "- When $p=+\\infty$, we get the $L_\\infty$, also known as **infinity norm** or **maximum norm**: \n",
    "\n",
    "$$\\left \\| \\text{ x } \\right \\|_{\\infty} = \\max_{i} |\\xi_i|$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Coefficient of Determination</h3>\n",
    "\n",
    "- Conventionally the **expectation value** (or **mean value**) of the residuals is zero: $\\mathbb{E}(\\mathbf{e}) = 0$.\n",
    "\n",
    "- What about **variance** $\\mathbb{Var}(\\mathbf{e})$?\n",
    "  \n",
    "  $$\\mathbb{Var}(\\mathbf{e}) = \\mathbb{Var}(\\mathbf{y})(1 - R^2).$$\n",
    "\n",
    "  where $R^2$ is called **coefficient of determination**:\n",
    "\n",
    "$$R = 1 - \\frac{\\sum_{i} e_i^2}{\\sum_{i}(y_i - \\bar{y})^2}  \\equiv 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "\n",
    "- **Coefficient of determination** helps to determines how much of the **total variation in $\\mathbf{y}$** is explained by the **variation in $\\mathbf{X}$**:\n",
    "  - $R^2=0$: the linear relationship **explains nothing** (so no linear relationship between $\\mathbf{X}$ and $\\mathbf{y}$);\n",
    "  - $R^2=1$: the linear relationship **explains everything** - no left-overs, no uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Model Selection</h3>\n",
    "\n",
    "- There remains the **problem of choosing the order** $N$ of the polynomial, this is called **model selection**.\n",
    "\n",
    "\n",
    "- The results of **fitting** polynomials having **orders** $N = 0, 1, 3$, and $9$ to the training data set is shown below:\n",
    "\n",
    "<center><img src=\"images/L2_Model_Selection.png\" width=\"1000\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- You can notice that:\n",
    "  - First order ($N = 1$) polynomials give rather **poor fits** to the data and consequently rather poor representations of the **true function** $\\sin 2\\pi x$;\n",
    "  - The third order ($N = 3$) polynomial seems to give the **best fit** to the **true function** $\\sin 2\\pi x$;\n",
    "  - The higher order polynomial ($N = 9$), we obtain an **excellent fit** to the **training data**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Overfitting and Underfitting</h3>\n",
    "\n",
    "- The central challenge in machine learning is that we must perform well on **previously unseen (new) inputs**.\n",
    "\n",
    "\n",
    "- The ability to perform well on previously unobserved inputs is called **generalization**.\n",
    "\n",
    "\n",
    "- There are **two type of errors** we can compute:\n",
    "  - error measure on the training set called the **training error**.\n",
    "  - **test error**, or **generalization error**, defined as the expected value of the error on a **new input**.\n",
    "\n",
    "\n",
    "- In our linear regresion example, the **training** and **test error** are:\n",
    "\n",
    "  $$\\text{training error} = \\frac{1}{m^{(train)}} \\left \\| \\mathbf{\\hat{y}}^{(train)} - \\mathbf{y}^{(train)} \\right \\|_2^2$$\n",
    "\n",
    "  $$\\text{test error} = \\frac{1}{m^{(test)}} \\left \\| \\mathbf{\\hat{y}}^{(test)} - \\mathbf{y}^{(test)} \\right \\|_2^2$$\n",
    "\n",
    "\n",
    "  <center><img src=\"images/L2_RMS.png\" width=\"600\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- For $N=9$ the **training set error goes to zero**, as we might expect because this **polynomial contains 10 degrees of freedom** corresponding to the **10 coefficients** $\\theta_0, \\cdots, \\theta_9$ and so can be tuned exactly to the 10 data points in the training set.\n",
    "\n",
    "\n",
    "\n",
    "- The factors determining **how well** a **machine learning** algorithm will **perform** are its ability to:\n",
    "  - make the **training error** small.\n",
    "  - make the gap between **training** and **test error** small.\n",
    "\n",
    "\n",
    "- These factors correspond to the **two central challenges** in machine learning: **underfitting** and **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Capacity</h3>\n",
    "\n",
    "\n",
    "\n",
    "- Model’s **capacity** is its ability to fit a wide variety of functions.\n",
    "\n",
    "\n",
    "- It **can control** whether a model is more likely to **underfit** or **overfit**.\n",
    "\n",
    "\n",
    "- One way to control the **capacity** of a ML algorithm is by choosing its **hypothesis space**, the set of functions that the learning algorithm is allowed to select as being the solution.\n",
    "\n",
    "<center><img src=\"images/L2_Capacity.png\" width=\"800\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}