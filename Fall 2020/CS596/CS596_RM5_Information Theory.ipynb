{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Information Theory</h1>\n",
    "\n",
    "In **Statistics** and **Machine Learning**, some concept of the **distance** between probability distributions is often required. \n",
    "\n",
    "In other words, we want to know **how similar two different probability distributions are**, and, moreover, we want to **quantify this similarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Measure of Information Content</h3>\n",
    "\n",
    "Let $(\\Omega, \\Sigma, P)$ be a probability space and let $A \\in \\Sigma$ is some event. \n",
    "\n",
    "$\\textbf{Definition}$. **Information content**, or **self-information** of an event $A$ is defined as:\n",
    "\n",
    "$$I(A)= - logP(A).$$\n",
    "\n",
    "**Information content** satisfies the following **three** properties:\n",
    "- Likely events have low information content, and events that are guaranteed to happen have no information content whatsoever.\n",
    "- Less likely events have higher information content.\n",
    "- Independent events have additive information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "Let's assume we toss fair dice and observe the outcome. \n",
    "\n",
    "If events are defined as follows:\n",
    "<br> &emsp; $\\bullet$ $A = \\{1, 2, 3, 4, 5, 6\\}$;\n",
    "<br> &emsp; $\\bullet$ $B = \\{2, 4, 6\\}$;\n",
    "<br> &emsp; $\\bullet$ $C = \\{6\\}$,\n",
    "\n",
    "then the **information content** of these events are:\n",
    "- $I(A)=-log(1)=0$,  i.e. event $A$ has **no surprise** because we already knew that $A$ is guaranteed to happen.\n",
    "- $I(B)=-log(0.5)=1$, i.e. event $B$ has some **information content** of 1. We get some information - we can rule out that observation is even.</li>\n",
    "- $I(C)=-log(\\frac{1}{6})\\approx2.58$, i.e. event $C$ has smallest probability so carries the most **surprise**.\n",
    "\n",
    "**Conclusion**: Rare events have high information content.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Shannon entropy</h3>\n",
    "\n",
    "We can quantify the amount of uncertainty in an entire probability distribution using the **Shannon entropy**.\n",
    "<br>\n",
    "In other words, it's the expected amount of information in an event drawn from that distribution.\n",
    "\n",
    "Let $p(X)$ be a **probability mass function** for some random variable $X$.\n",
    "\n",
    "$\\textbf{Definition}$.  **Entropy** of a random variable $X$ is defined as an expectation of the information content of it's outcomes:\n",
    "\n",
    "$$H(X)=\\mathbb{E}_p[I_p(X)]=-\\sum_{i=1}^{n}p(x_i)logp(x_i),$$ \n",
    "\n",
    "where $x_i$ are possible values of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Cross entropy</h3>\n",
    "\n",
    "Let $p(X)$ and $q(X)$ be two **probability mass functions** over a same set of events.\n",
    "\n",
    "$\\textbf{Definition}$. **Cross Entropy** between distributions $p$ and $q$ is defined as following expectation:\n",
    "\n",
    "$$H(p, q)=\\mathbb{E}_p[I_q(X)]=-\\sum_{i=1}^{n}p(x_i)logq(x_i),$$\n",
    "\n",
    "where $x_i$ are possible values of $X$.\n",
    "\n",
    "If $p=q$, then $H(p, q)=H(p)=H(q)$, i.e. cross-entropy becomes just entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Kullback-Leibler (KL) divergence</h3>\n",
    "\n",
    "Let $p(X)$ and $q(X)$ be two **probability mass functions** over a same set of events.\n",
    "\n",
    "$\\textbf{Definition}$. **Kullback-Leibler divergence** between distributions $p$ and $q$ is defined as following expectation:\n",
    "\n",
    "$$KL(p, q)=\\mathbb{E}_p[log\\frac{p(X)}{q(X)}]=\\sum_{i=1}^{n}p(x_i)log\\frac{p(x_i)}{q(x_i)},$$\n",
    "\n",
    "where $x_i$ are possible values of $X$. Formula is valid when $q(x)\\neq0$ or both $p(x)=q(x)=0$.\n",
    "\n",
    "- There is a relation between **Entropy**, **Cross-Entropy** and **KL divergence**: \n",
    "$$H(p, q)=H(P)+KL(p, q).$$\n",
    "- Similarly to Cross-Entropy, KL divergence is used in Statistics and Machine Learning to measure similarity between probability distributions.\n",
    "- KL Divergence is used as a loss function in approximate variational inference - powerful unsupervised technique in ML to learn complex distributions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
