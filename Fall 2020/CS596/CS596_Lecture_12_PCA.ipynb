{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **30 November 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 12**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Dimensionality Reduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Principal Component Analysis (PCA)</h3>\n",
    "\n",
    "- **Principal Component Analysis (PCA)** is a dimensionality reduction technique that enables you to identify correlations and patterns in a data set so that it can be transformed into a data set of significantly lower dimension without loss of any important information.\n",
    "\n",
    "\n",
    "- The standard context for PCA as an exploratory data analysis tool involves a dataset with observations on $m$ numerical $vectors$, for each of $n$ $features$. \n",
    "\n",
    "\n",
    "- These $m$ numerical values define $n$-dimensional vectors $\\{x_1, ..., x_m\\}$, where $x_i = \\{x_{1i}, ..., x_{ni}\\}$ for $i = \\overline{1, m}$. ($x_{ij} \\in \\mathbb{R}$ for $i=\\overline{1,m}$ and $j=\\overline{1,n}$).\n",
    "\n",
    "\n",
    "- Or, equivalently, data is defined as $m \\times n$ data matrix $X =(x_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Application of matrices for solving a system of linear equations</h3>\n",
    "\n",
    "- A general system of $m$ linear equations with $n$ unknowns can be written as:\n",
    "  \n",
    "  $$\\begin{matrix}\n",
    "\\alpha_{11} x_1 + \\alpha_{12} x_2 + \\cdots + \\alpha_{1n} x_n = b_1\\\\ \n",
    "\\alpha_{21} x_1 + \\alpha_{22} x_2 + \\cdots + \\alpha_{2n} x_n = b_2\\\\  \n",
    "\\vdots \\\\\n",
    "\\alpha_{m1} x_1 + \\alpha_{12} x_2 + \\cdots + \\alpha_{mn} x_n = b_m\n",
    "\\end{matrix}$$\n",
    "\n",
    "  where $x_1, ..., x_n$ are the unknowns, $\\alpha_{11}, ..., \\alpha_{mn}$ are the coefficients of the system, and $b_1, ..., b_m$ are the constant terms.\n",
    "\n",
    "\n",
    "- We can write this system of linear equations in the equivalent matrix form:\n",
    "  \n",
    "  $$Ax = b,$$\n",
    "  \n",
    "  \n",
    "  $$\\text{where } A = \n",
    "\\begin{bmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\cdots  & \\alpha_{1n} \\\\ \n",
    "\\alpha_{21} & \\alpha_{11} & \\cdots  & \\alpha_{2n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots  & \\vdots \\\\ \n",
    "\\alpha_{n1} & \\alpha_{11} & \\cdots  & \\alpha_{nn} \n",
    "\\end{bmatrix}\n",
    "\\text{, }\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\text{ and }\n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Singular Value Decomposition (SVD)</h3>\n",
    "\n",
    "- The matrix $AA^T$ and $A^TA$ are very special in linear algebra, since they has next useful properties:\n",
    "  - they are symmetrical;\n",
    "  - they are square;\n",
    "  - both matrices have the same positive eigenvalues;\n",
    "  - both have the same rank: $\\rho(AA^T)=\\rho(A^TA)=\\rho(A)$;\n",
    "\n",
    "\n",
    "- Let $u_i$ and $v_i$ be the eigenvectors of $AA^T$ and $A^TA$ respectively: $(AA^T)u_i = \\sigma_i u_i$ and $(A^TA)v_i = \\sigma_i v_i$;\n",
    "\n",
    "\n",
    "- The eigenvectors $u_i$ and $v_i$ are called the $singular$ $vectors$ and the square roots of $\\sigma_i$ eigenvalues are called $singular$ $values$ of the matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Theorem**:\n",
    "\n",
    "  Let $A$ be the matrix of the linear transformation $A$ over the $n$-dimensional vector space $\\mathcal{V}$. \n",
    "  \n",
    "  Then $A$ can be factorized as:\n",
    "\n",
    "  $$A = USV^T,$$\n",
    "  \n",
    "  where $U$ and $V$ are $m \\times r$ and $r \\times n$ orthogonal matrices.\n",
    "  \n",
    "  Matrices are orthogonal if $UU^T = U^TU = I$ and $VV^T = V^TV = I$, with eigenvectors chosen from $AA^T$ and $A^TA$ respectively.\n",
    "\n",
    "  $S$ is an $r \\times r$ diagonal matrix with elements equal to the **root of the positive eigenvalues** of $AA^T$ or $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Proof**:\n",
    "\n",
    "  Lets consider the eigenvectors and eigenvalues of the matrix $AA^T$ and $A^TA$ :\n",
    "\n",
    "  $$AA^T u_i = \\sigma_i u_i \\text{, for } i =\\overline{1,m};$$\n",
    "  \n",
    "  $$A^TA v_i = \\sigma_i v_i \\text{, for } i =\\overline{1,n}.$$\n",
    "\n",
    "  We can write these equations in matrix form:\n",
    "\n",
    "  $$AA^T U = U S^2 \\text{ and } A^TA V = V S^2,$$\n",
    "\n",
    "  where $U = \\{u_1, ..., u_m\\}$, $V = \\{v_1, ..., v_n\\}$ and $S = diag(\\sigma_1, ... , \\sigma_r, 0, ..., 0).$<br>\n",
    "\n",
    "  Remembering, that $UU^T = I$ and $V^TV = I$, we have:\n",
    "\n",
    "  $$AA^T = U S^2 U^T = U S V^T V S U^T = (U S V^T)(U S V^T)^T.$$\n",
    "\n",
    "  Therefore, $A$ can be expressed in the form:\n",
    "\n",
    "  $$A = U S V^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Visualisation of SVD</h3>\n",
    "\n",
    "- PCA can be thought of as **fitting** a $D$-**dimensional ellipsoid** to the data, where **each axis** of the ellipsoid represents a **principal component**. \n",
    "\n",
    "- If some **axis** of the ellipsoid is **small**, then the **variance** along that axis is **also small**.\n",
    "\n",
    "<img src=\"images/L12_SVD.jpg\" width=\"1500\" height=\"300\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step by step computation of PCA</h3>\n",
    "\n",
    "The below steps need to be followed to perform dimensionality reduction using PCA:\n",
    "\n",
    "1. Standardization of the data set;\n",
    "\n",
    "2. Calculation of the covariance matrix;\n",
    "\n",
    "3. Calculation of the singular values and singular vectors and factorization of the covariance matrix;\n",
    "\n",
    "4. Calculation of the Prinipal omponents and reduction of the data set size;\n",
    "\n",
    "5. Data reconstruction from a reduced data set;\n",
    "\n",
    "6. Validation of the reconstricted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 1: Standardization of the data</h3>\n",
    "\n",
    "- The properties of PCA have some **undesirable features** when variables are measures in **different units**;\n",
    "\n",
    "\n",
    "- To **overcome** this undesirable feature, it is common practice to begin by **standardizing** the variables;\n",
    "\n",
    "\n",
    "- Standardization is carried out by replacing initial data matrix $X$ with the standardized data matrix $Y$;\n",
    "\n",
    "\n",
    "- Each data value $x_{ij}$ is both centered and divided by the standard deviation $s_j$ of the $n$ observations of the variable $j$:\n",
    "\n",
    "  $$y_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j},$$\n",
    "\n",
    "  $$\\text{where } \\bar{x}_j = \\frac{1}{m}\\sum_{i=1}^{m} x_{ij} \\text{ and } s_j^2 = \\frac{1}{m-1} \\sum_{i=1}^{m}(x_{ij} - \\bar{x}_j)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 2: Computing the covariance matrix $\\Sigma$</h3>\n",
    "\n",
    "- A covariance matrix $\\Sigma$ expresses the **correlation between each two different features** in the data set:\n",
    "\n",
    "  $$\\Sigma =  \n",
    "\\begin{bmatrix}\n",
    "cov[y_1, y_1] & cov[y_1, y_2] & \\cdots & cov[y_1, y_n]\\\\\n",
    "cov[y_2, y_1] & cov[y_2, y_2] & \\cdots & cov[y_2, y_n]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "cov[y_n, y_1] & cov[y_m, y_2] & \\cdots & cov[y_n, y_n]\\\\ \n",
    "\\end{bmatrix}\n",
    ",$$\n",
    "\n",
    "  where where each element represents the **covariance between two features** (remember that $y_i$ are centered variables):\n",
    "$$cov[y_i, y_j] = \\frac{1}{m-1} \\sum_{k=1}^{m} (y_i)(y_j)^T.$$\n",
    "\n",
    "\n",
    "- If the covariance value is **negative**, then the respective features are **indirectly proportional** to each other;\n",
    "\n",
    "\n",
    "- A **positive** covariance denotes that the respective features are **directly proportional** to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 3: Calculating the singular values and singular vectors</h3>\n",
    "\n",
    "- The next step is to factorize the matrix $\\Sigma$ using the SVD:\n",
    "\n",
    "  $$\\Sigma = U S V^T,$$\n",
    "\n",
    "\n",
    "- $U$ and $V$ are $n \\times n$ orthogonal matrices, i.e. $UU^T = U^TU = I$ and $VV^T = V^TV = I$,\n",
    "with singular vectors chosen from $\\Sigma\\Sigma^T$ and $\\Sigma^T\\Sigma$ respectively. \n",
    "\n",
    "\n",
    "-  The $S$ is an $n \\times n$ diagonal matrix with elements equal to the $\\sqrt{\\sigma_i}$ of the singular values of $\\Sigma\\Sigma^T$ or $\\Sigma^T\\Sigma$.\n",
    "\n",
    "\n",
    "-  Singular values are sorted in descending order: $\\sigma_1 \\geq \\sigma_2, .... \\geq \\sigma_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 4: Computing the Principal Components (PC)</h3>\n",
    "\n",
    "- **Principal Components (PC)** are the new set of variables that are obtained from the initial set of variables;\n",
    "\n",
    "\n",
    "- Once we have computed the **singular values** and **singular vectors**, we order them in the **descending order**, where the singular vector with the **highest singular value** is the most significant and forms the first principal components.\n",
    "\n",
    "\n",
    "- The principal components of **lesser significances** can thus be **removed** in order to **reduce the dimensions** of the data.\n",
    "\n",
    "\n",
    "- Thus we take first $k \\leq n$ columns of $U$ and consider new matrix $U_k$:\n",
    "\n",
    "  $$U_k = \\{u_1, ..., u_k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 5: Reducing the dimensions of the data set</h3>\n",
    "\n",
    "- The last step in performing PCA is to re-arrange the original data with the final principal components which represent the maximum and the most significant information of the data set. \n",
    "\n",
    "\n",
    "- In order to replace the original data axis with the newly formed Principal Components, you simply multiply the transpose of the original data set by the transpose of the obtained feature vector.\n",
    "\n",
    "\n",
    "- Thus newly formed Principal Components are:\n",
    "\n",
    "  $$z_i = U_k^T y_i.$$\n",
    "\n",
    "\n",
    "- Since $U_k^T \\in \\mathbb{R}^{k\\times n}$ and $y_i \\in \\mathbb{R}^{n}$, thus $z_i \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Step 6: Validation of the reconstricted data</h3>\n",
    "\n",
    "- We can approximate the reconstruction of the original value by ${y_{i}}' = U_{k}z_{i}$ and compare it with the original value $y_i$:\n",
    "\n",
    "  $$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left \\| y_{i} - {y_{i}}' \\right \\|^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left \\| y_{i} \\right \\|^{2}\n",
    "} \\leq \\epsilon$$\n",
    "\n",
    "  where $\\epsilon$ is the $proportion \\text{ } of \\text{ } total \\text{ } variance$.\n",
    "\n",
    "  Using the next inequality:\n",
    "\n",
    "  $$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\\left \\| y_{i} - {y_{i}}' \\right \\|^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\\left \\| y_{i} \\right \\|^{2}\n",
    "} \\leq 1 -\n",
    "\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "},$$\n",
    "\n",
    "  we can write:\n",
    "\n",
    "  $$\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "} \\geq \\epsilon$$\n",
    "\n",
    "\n",
    "- It is common practice $\\epsilon = 70\\%$ is used to decide how many PCs should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 50 artists>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
