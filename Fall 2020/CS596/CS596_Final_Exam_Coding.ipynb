{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **16 November 2020 from 18:00 to 21:00**\n",
    "<br>\n",
    "Title: **Final Exam**\n",
    "\n",
    "You will receive a maximum of **30 points** distributed as follows:\n",
    "\n",
    "- **Quiz (10 Points)**: \n",
    "  - Problem A1: **1 point**.\n",
    "  - Problem A2: **1 point**.\n",
    "  - Problem A3: **1 point**.\n",
    "  - Problem A4: **1 point**.\n",
    "  - Problem A5: **1 point**.\n",
    "  - Problem A6: **1 point**.\n",
    "  - Problem A7: **1 point**.\n",
    "  - Problem A8: **1 point**.\n",
    "  - Problem A9: **1 point**.\n",
    "  - Problem A10: **1 point**. \n",
    "  \n",
    "  \n",
    "- **Problem Solving (10 Points)**:\n",
    "  - Problem B1: **2 point**.\n",
    "  - Problem B2: **2 point**.\n",
    "  - Problem B3: **2 point**.\n",
    "  - Problem B4: **2 point**.\n",
    "  - Problem B5: **2 point**.\n",
    " \n",
    "\n",
    "- **Coding (10 Points)**:\n",
    "  - Problem C: **10 point**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Problems C: PCA + Logistic Regression using Wines Dataset</h3>\n",
    "\n",
    "- Given a **wine dataset**: https://drive.google.com/file/d/1t0mAAdsRn5TJRw_hUvI7cri3qA28Tq9b/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title of Database: Wine recognition data\n",
      "\tUpdated Sept 21, 1998 by C.Blake : Added attribute information\n",
      "\n",
      "2. Sources:\n",
      "   (a) Forina, M. et al, PARVUS - An Extendible Package for Data\n",
      "       Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
      "       and Food Analysis and Technologies, Via Brigata Salerno, \n",
      "       16147 Genoa, Italy.\n",
      "\n",
      "   (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\n",
      "   (c) July 1991\n",
      "3. Past Usage:\n",
      "\n",
      "   (1)\n",
      "   S. Aeberhard, D. Coomans and O. de Vel,\n",
      "   Comparison of Classifiers in High Dimensional Settings,\n",
      "   Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
      "   Mathematics and Statistics, James Cook University of North Queensland.\n",
      "   (Also submitted to Technometrics).\n",
      "\n",
      "   The data was used with many others for comparing various \n",
      "   classifiers. The classes are separable, though only RDA \n",
      "   has achieved 100% correct classification.\n",
      "   (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
      "   (All results using the leave-one-out technique)\n",
      "\n",
      "   In a classification context, this is a well posed problem \n",
      "   with \"well behaved\" class structures. A good data set \n",
      "   for first testing of a new classifier, but not very \n",
      "   challenging.\n",
      "\n",
      "   (2) \n",
      "   S. Aeberhard, D. Coomans and O. de Vel,\n",
      "   \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
      "   Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
      "   Mathematics and Statistics, James Cook University of North Queensland.\n",
      "   (Also submitted to Journal of Chemometrics).\n",
      "\n",
      "   Here, the data was used to illustrate the superior performance of\n",
      "   the use of a new appreciation function with RDA. \n",
      "\n",
      "4. Relevant Information:\n",
      "\n",
      "   -- These data are the results of a chemical analysis of\n",
      "      wines grown in the same region in Italy but derived from three\n",
      "      different cultivars.\n",
      "      The analysis determined the quantities of 13 constituents\n",
      "      found in each of the three types of wines. \n",
      "\n",
      "   -- I think that the initial data set had around 30 variables, but \n",
      "      for some reason I only have the 13 dimensional version. \n",
      "      I had a list of what the 30 or so variables were, but a.) \n",
      "      I lost it, and b.), I would not know which 13 variables\n",
      "      are included in the set.\n",
      "\n",
      "   -- The attributes are (dontated by Riccardo Leardi, \n",
      "\triclea@anchem.unige.it )\n",
      " \t1) Alcohol\n",
      " \t2) Malic acid\n",
      " \t3) Ash\n",
      "\t4) Alcalinity of ash  \n",
      " \t5) Magnesium\n",
      "\t6) Total phenols\n",
      " \t7) Flavanoids\n",
      " \t8) Nonflavanoid phenols\n",
      " \t9) Proanthocyanins\n",
      "\t10)Color intensity\n",
      " \t11)Hue\n",
      " \t12)OD280/OD315 of diluted wines\n",
      " \t13)Proline            \n",
      "\n",
      "5. Number of Instances\n",
      "\n",
      "      \tclass 1 59\n",
      "\tclass 2 71\n",
      "\tclass 3 48\n",
      "\n",
      "6. Number of Attributes \n",
      "\t\n",
      "\t13\n",
      "\n",
      "7. For Each Attribute:\n",
      "\n",
      "\tAll attributes are continuous\n",
      "\t\n",
      "\tNo statistics available, but suggest to standardise\n",
      "\tvariables for certain uses (e.g. for us with classifiers\n",
      "\twhich are NOT scale invariant)\n",
      "\n",
      "\tNOTE: 1st attribute is class identifier (1-3)\n",
      "\n",
      "8. Missing Attribute Values:\n",
      "\n",
      "\tNone\n",
      "\n",
      "9. Class Distribution: number of instances per class\n",
      "\n",
      "      \tclass 1 59\n",
      "\tclass 2 71\n",
      "\tclass 3 48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data./wine_description.txt\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "- **Problem Statement**:\n",
    "\n",
    "  1. Importing all required libraries: `numpy`, `matplotlib`, etc.;\n",
    "  \n",
    "  2. Importing the data set using `pandas`;\n",
    "  \n",
    "  3. Splitting the dataset into the Training set and Test set using `train_test_split` of the `sklearn.model_selection`\n",
    "  \n",
    "  4. Apply the data standardization using `StandardScaler` method of the `sklearn.preprocessing`; \n",
    "  \n",
    "  5. Apply PCA considering only first **two highest components** using `PCA` method of the `sklearn.decomposition`; \n",
    "  \n",
    "  6. Fit the data with Logistic Regression Model using `LogisticRegression` method of the `sklearn.linear_model`;\n",
    "  \n",
    "  7. Predict the test result and make the confusion matrix using `confusion_matrix` method of the `sklearn.metrics`;\n",
    "  \n",
    "  8. Visualise the Training set result using the `matplotlib` library;\n",
    "  \n",
    "  9. Visualise the Test set results using the `matplotlib` library.\n",
    "  \n",
    "\n",
    "**Hint**: Revise Seminar $3$ and Seminar $12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Wish you good luck!</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
