{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **23 November 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 11**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Sources:\n",
    "Bibliography: \n",
    "<br>[1] **Chapter 9**. Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Unsupervised Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">What is Unsupervised Learning?</h3>\n",
    "\n",
    "- **Unsupervised learning** is a type of machine learning technique where you **do not need to supervise the model**, i.e. the **training data** consists of a set of input vectors $\\mathbf{x}$ **without any corresponding target values**.\n",
    "\n",
    "\n",
    "- The **goal** in the unsupervised learning problems may be:\n",
    "  - to **discover groups of similar examples** within the data, where it is called **clustering**;\n",
    "  - to **determine the distribution of data** within the input space, known as **density estimation**;\n",
    "  - to **project the data** from a high-dimensional space down to two or three dimensions for the purpose of **visualization**.\n",
    "  \n",
    "  \n",
    "- **Unsupervised learning** algorithms **allows you to perform more complex processing tasks** compared to supervised learning.\n",
    "\n",
    "\n",
    "- Although, **unsupervised learning** can be more **unpredictable** compared with other natural learning methods.\n",
    "\n",
    "<img src=\"images/L11_Unsupervised_learning.png\" width=\"1250\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Clustering</h3>\n",
    "\n",
    "- **What is a Cluster?**\n",
    "\n",
    "  A **cluster** is a **subset** of objects which are **similar**.\n",
    "\n",
    "  A **subset** of objects such that the **distance between any two objects in the cluster** is **less** than the **distance between any object in the cluster** and **any object not located inside it**.\n",
    "  \n",
    "  A **connected region** of a **multidimensional space** containing a relatively **high density of objects**.\n",
    "\n",
    "\n",
    "\n",
    "- **What Is Clustering?**\n",
    "\n",
    "  A **loose definition** of **clustering** is the **process of organizing objects into groups whose members are similar in some way**.\n",
    "\n",
    "  <img src=\"images/L11_Clustering.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "- In fact, there are **more than 100 clustering algorithms** known, but few of the algorithms are used popularly.\n",
    "\n",
    "\n",
    "- **Connectivity models**:\n",
    "\n",
    "  As the name suggests, these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. \n",
    "\n",
    "  These models can **follow two approaches**:\n",
    "\n",
    "  - In the **first approach**, they start with **classifying** all data points into separate clusters and then **aggregating** them as the distance decreases. \n",
    "\n",
    "  - In the **second approach**, all data points are **classified** as a single cluster and then **partitioned** as the distance increases. The choice of distance function is subjective.   \n",
    "  \n",
    "  These models are very easy to interpret but **lacks scalability for handling big datasets**. \n",
    "\n",
    "  Examples of these models are **hierarchical clustering algorithm**.\n",
    "\n",
    "\n",
    "- **Centroid models**:\n",
    "\n",
    "  These are **iterative clustering algorithms** in which the **notion of similarity is derived** by the closeness of a data point to the **centroid of the clusters**.\n",
    "\n",
    "  **$K$-Means clustering algorithm** is a popular algorithm that falls into this category. \n",
    "  \n",
    "  In these models,  the number of clusters required at the end have to be mentioned beforehand, which makes it important to have prior knowledge of the dataset.\n",
    "\n",
    "\n",
    "- **Distribution models**: \n",
    "\n",
    "  These clustering models are based on the notion of **how probable** is it that all data points in the cluster **belong to the same distribution**. \n",
    "\n",
    "  A popular example of these models is **Expectation-Maximization (EM) algorithm** which uses multivariate normal distributions.\n",
    "\n",
    "  These models often **suffer from overfitting**. \n",
    " \n",
    "\n",
    "- **Density Models**: \n",
    "\n",
    "  These models search the data space for areas of varied density of data points in the data space. \n",
    "\n",
    "  It isolates various different density regions and assign the data points within these regions in the same cluster. \n",
    "\n",
    "  Popular examples of density models are DBSCAN and OPTICS.\n",
    "   \n",
    " \n",
    "- Let's start by considering the $ùêæ$**-means algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">$K$-means Algorithm</h3>\n",
    "\n",
    "- Lets suppose that we have a data set $\\{ \\mathbf{x_1, x_2, ..., x}_N\\}$ consisting of $N$ observations of a random $D$-dimensional Euclidean variable $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "- Our **goal** is to **partition** the data set into some number **$K$ of clusters** ( suppose for the moment that the value of $K$ is given), in which **each observation** belongs to the cluster with the **nearest mean** (**cluster centers**), serving as a **prototype of the cluster**.\n",
    "\n",
    "\n",
    "- Lets formalize this notion by first introducing a set of $D$-dimensional vectors $\\mathbf{\\mu}_k$, where $k = 1, ... , N$.\n",
    "\n",
    "\n",
    "- Here $\\mathbf{\\mu}_k$ is a **prototype** associated with the $k^{\\mathrm{th}}$ cluster, i.e. $\\mathbf{\\mu}_k$ is representing the **centres of the clusters**.\n",
    "\n",
    "\n",
    "- The **goal** is then to **find an assignment of data points to clusters**, as well as a set of vectors $\\{\\mathbf{\\mu}_k\\}$, such that the **sum of the squares of the distances** of each data point to its closest vector $\\mathbf{\\mu}_k$, is a **minimum**.\n",
    "\n",
    "\n",
    "- Let's **define some notation** to describe the **assignment of data points to clusters**:\n",
    "\n",
    "  For each data point $\\mathbf{x}_n$, we introduce a corresponding set of binary indicator variables $r_{nk} \\in \\{0,1 \\}$, where $k = 1, . . . , K $ describing which of the $K$ clusters the data point $\\mathbf{x}_n$ is assigned to. I.e. if data point $\\mathbf{x}_n$ is assigned to cluster $k$ then $r_{nk} = 1$, and $r_{nj} = 0$ for $j\\neq k$.\n",
    "\n",
    "\n",
    "- Our objective function, called a **distortion measure**, can be defined as:\n",
    "\n",
    "  $$J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\| \\mathbf{x}_n - \\mathbf{\\mu}_k\\|^2$$\n",
    "\n",
    "  which represents the **sum of the squares of the distances** of each data point to its assigned vector $\\mathbf{\\mu}_k$.\n",
    "\n",
    "\n",
    "- We need to to **find values** for the $\\{r_{nk}\\}$ and the $\\{\\mathbf{\\mu}_k\\}$ so as to **minimize** $J$.\n",
    "\n",
    "\n",
    "- For this, we **first choose some initial values** for the $\\mathbf{\\mu}_k$ and then **perform an iterative procedure**.\n",
    "\n",
    "  Each iteration **involves two successive steps** corresponding to successive optimizations with respect to the $r_{nk}$ and the $\\{\\mathbf{\\mu}_k\\}$:\n",
    "  - In the first phase we **minimize $J$** with **respect to the $r_{nk}$**, **keeping the $\\{\\mathbf{\\mu}_k\\}$ fixed**.\n",
    "  - In the second phase we **minimize $J$** with **respect to the $\\{\\mathbf{\\mu}_k\\}$**, **keeping $r_{nk}$ fixed**.\n",
    "  \n",
    "  **Repeated** this two-stage optimization in turn **until there is no further change in the assignments** (**convergence**).\n",
    "\n",
    "  Because each phase reduces the value of the objective function $J$, **convergence** of the algorithm **is assured**.\n",
    "  \n",
    "  \n",
    "- The first step is usually called **E (expectation) step** and second step is called **M (maximization) step** in the context of the $K$-means algorithm.\n",
    "  \n",
    "  \n",
    "- **E step:**\n",
    "\n",
    "  The objective function $J$ is a **linear function of $r_{nk}$**, this optimization can be performed easily to give a closed form solution.\n",
    "  \n",
    "  The **terms involving different $n$ are independent** and so we can **optimize for each $n$ separately** by choosing $r_{nk}$ to be $1$ for whichever value of $k$ gives the minimum value of $\\| \\mathbf{x}_n - \\mathbf{\\mu}_k\\|^2$:\n",
    "  \n",
    "  $$r_{nk} =\n",
    "    \\left\\{\n",
    "    \\begin{matrix}\n",
    "    1 & \\textrm{if } k = \\arg \\min_{j} \\| \\mathbf{x}_n - \\mathbf{\\mu}_k\\|^2 \\\\\n",
    "    0 & \\textrm{otherwise}\n",
    "    \\end{matrix}\n",
    "    \\right.\n",
    "  $$\n",
    "\n",
    "\n",
    "- **M step:**\n",
    "\n",
    "  The objective function $J$ is a **quadratic function of $\\mathbf{\\mu}_k$**, and it can be **minimized by setting** its **derivative** with respect to $\\mathbf{\\mu}_k$ **to zero** giving:\n",
    "  \n",
    "  $$2 \\sum_{n=1}^{N} r_{nk} (\\mathbf{x}_n - \\mathbf{\\mu}_k) = 0$$\n",
    "  \n",
    "  which we can easily solve for $\\mathbf{\\mu}_k$ to give:\n",
    "  \n",
    "  $$\\mathbf{\\mu}_k  = \\frac{\\sum_{n} r_{nk} \\mathbf{x}_n}{\\sum_{n} r_{nk}}.$$\n",
    "  \n",
    "  **Note**, that the **denominator** in this expression **is equal to the number of points** assigned to cluster $k$, and so this result has a simple interpretation, namely **set $\\mathbf{\\mu}_k$ equal to the mean** of all of the data points $\\mathbf{x}_n$ assigned to cluster $k$. For this reason, the procedure is known as the **$K$-means algorithm**.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Visualization of the $K$-means Algorithm</h3>\n",
    "\n",
    "- The $K$-means algorithm is illustrated using the **Old Faithful** data set:\n",
    "https://www.kaggle.com/janithwanni/old-faithful\n",
    "\n",
    "\n",
    "\n",
    "- For the purposes of vizualization, a **standardization is apploed to the data**, such that **each of the variables** has **zero mean** and **unit standard deviation**.\n",
    "\n",
    "\n",
    "- Description of the plot:\n",
    "  - Here $K = 2$ is chosen.\n",
    "  - **Green points** denote the **data set in a two-dimensional Euclidean space**.\n",
    "  - The **initial choices for centres** $\\mathbf{\\mu}_1$ and $\\mathbf{\\mu}_2$ are shown by the **red** and **blue** crosses, respectively.\n",
    "  - On the **left** are shown the **E steps**.\n",
    "  - On the **right** are shown the **M steps**.\n",
    "\n",
    "<img src=\"images/L11_K-mean_algorithm_a.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "- Also, the plot of the cost function $J$ given by after each **E step** (**blue points**) and **M step** (**red points**) of the $K$-means algorithm is shown\n",
    "\n",
    "\n",
    "- As we see, the algorithm **has converged after the third M step**,\n",
    "\n",
    "\n",
    "<img src=\"images/L11_K-mean_algorithm_b.png\" width=\"400\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discussion of the $K$-means Algorithm</h3>\n",
    "\n",
    "- Note, that in the previouse example we have deliberately chosen **poor initial values** for the **cluster centres** so that the **algorithm takes several steps before convergence**.\n",
    "\n",
    "  In practice, a **better initialization procedure** would be to **choose the cluster centres** $\\mathbf{\\mu}_k$ to be **equal** to a **random subset** of $K$ data points.\n",
    "\n",
    "\n",
    "\n",
    "- A **direct implementation** of the $K$-means algorithm can be **relatively slow**, because in each **E step** it is necessary to compute the Euclidean distance between every prototype vector and every data point. \n",
    "  \n",
    "  **Various schemes** have been proposed for **speeding up** the **$K$-means algorithm**, some of which are based on **precomputing a data structure** such as a **tree** such that nearby points are in the same subtree.\n",
    "  \n",
    "  **Other approaches** make **use** of the **triangle inequality for distances**, thereby **avoiding** unnecessary **distance calculations**.\n",
    "\n",
    "\n",
    "- So far, we considered a **batch version** of $K$-means algorithm in which the **whole data set is used** together to update the prototype vectors. \n",
    "  \n",
    "  We can also derive an **on-line stochastic algorithm** by applying the **Robbins-Monro procedure** to the problem of finding the roots of the regression function given by the derivatives of $J$ with respect to $\\mathbf{\\mu}_k$. \n",
    "  \n",
    "  This leads to a equential update in which, for each data point $\\mathbf{x}_n$ in turn, we update the nearest prototype $\\mathbf{\\mu}-K$ using:\n",
    "\n",
    "  $$\\mathbf{\\mu}_k^{\\textrm{new}} = \\mathbf{\\mu}_k^{\\textrm{old}}  + \\eta_n (\\mathbf{x}_n - \\mathbf{\\mu}_k^{\\textrm{old}}),$$\n",
    "\n",
    "  where $\\eta_n$ is the **learning rate parameter**, which is typically made to decrease monotonically as more data points are considered.\n",
    "\n",
    "\n",
    "- The $K$-means algorithm **is based** on the use of **squared Euclidean distance** as the **measure of dissimilarity** between a **data poin**t and a **prototype vector**, which can **make the determination** of the **cluster means** **nonrobust to outliers**.\n",
    "\n",
    "  One can **generalize the $K$-means algorithm** by introducing a more **general dissimilarity measure** $\\mathcal{V}(\\mathbf{x}, \\mathbf{x}')$ between two vectors $\\mathbf{x}$ and $\\mathbf{x}'$ and then minimizing the following distortion measure:\n",
    "  \n",
    "  $$\\hat{J} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\mathcal{V}( \\mathbf{x}_n, \\mathbf{\\mu}_k),$$\n",
    "  \n",
    "  which gives the **$K$-medoids algorithm**.\n",
    "  \n",
    "  \n",
    "- One notable feature of the **$K$-means algorithm** is that at each iteration, **every data point** is **assigned uniquely** to **one of the clusters**. This is known as **strong assignments** of data points to clusters.\n",
    "\n",
    "  If the **data points** that **lie roughly midway** between **cluster centres**, then it is not clear that the hard assignment to the nearest cluster is the most appropriate. \n",
    "  \n",
    "  By adopting a probabilistic approach, one obtains **soft assignments** of data points to clusters in a way that reflects the level of uncertainty over the most appropriate assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Image Segmentation and Compression</h3> \n",
    "\n",
    "- The **goal of segmentation** is to **partition an image into regions** each of which has a **reasonably homogeneous visual appearance** or which corresponds to objects or parts of objects (Forsyth and Ponce, 2003).\n",
    "\n",
    "  Below, **two examples** of the application of the **$K$-means clustering algorithm** to **image segmentation** are presented,  showing the **initial images** together with their **$K$-means segmentations** obtained using **various values of $K$**.\n",
    " \n",
    "  It should be emphasized that this use of **$K$-means is not a particularly sophisticated approach to image segmentation**, not least because **it takes no account of the spatial proximity of different pixels**.\n",
    "\n",
    "  The **image segmentation problem** is in general **extremely difficult** and **remains the subject of active research**.\n",
    "\n",
    "<img src=\"images/L11_Image_Segmentation.png\" width=\"1000\" alt=\"Example\" />\n",
    "\n",
    "- We can also use the **result of a clustering algorithm** to perform **data compression**:\n",
    "\n",
    "  It is important to distinguish between:\n",
    "  - **lossless data compression**, in which the **goal** is to **be able to reconstruct the original data** exactly from the compressed representation;\n",
    "  - **lossy data compression**, in which we **accept some errors** in the reconstruction in return for higher levels of compression than can be achieved in the lossless case.\n",
    "  \n",
    "  We can **apply** the **$K$-means algorithm** to the problem of **lossy data compression** as follows:\n",
    "  - For each of the $N$ data points, we store only the identity $k$ of the cluster to which it is assigned. \n",
    "  - We also store the values of the $K$ cluster centres $\\mathbf{\\mu}_k$, which typically requires significantly less data, provided we choose $K \\ll N$.\n",
    "  - Each **data point** is then **approximated** by its **nearest centre** $\\mathbf{\\mu}_k$.\n",
    "  \n",
    "  - **New data points** can similarly **be compressed** by first finding the nearest $\\mathbf{\\mu}_k$ and then storing the label $k$ instead of the original data vector. This framework is often called **vector quantization**, and the vectors $\\mathbf{\\mu}_k$ are called **code-book vectors**.\n",
    "  \n",
    "  Lets consider the example of the image segmentation problem discussed above:\n",
    "  - Suppose the original image has $N = 240 \\times 180 = 43, 200$ pixels comprising $\\{R, G,B\\}$ values each of which is stored with $8$ bits of precision.\n",
    "  - To transmit the whole image directly would cost $24N = 1, 036, 800$ bits.\n",
    "  - Now suppose we first run $K$-means on the image data, and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector $\\mathbf{\\mu}_k$.\n",
    "  - **$K$ vectors $\\mathbf{\\mu}_k$** requires $\\log_2 K$ bits per pixel.\n",
    "  - **$K$ code book vectors $\\mathbf{\\mu}_k$** requires 24K bits.\n",
    "  - The **total number of bits required** to transmit the image is $24K + N \\cdot \\log_2 K$, which is equal to $43, 248$ bits ($K = 2$), $86, 472$ bits ($K = 3$), and $173, 040$ bits ($K = 10$), respectively.\n",
    "  - These represent compression ratios compared to the original image of $4.2\\%$, $8.3\\%$, and $16.7\\%$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Elbow Method</h3>\n",
    "\n",
    "- The **Elbow Method** is a **heuristic** used in determining the **number of clusters** in a data set. \n",
    "\n",
    "\n",
    "- The **idea** is to **run $K$-means clustering** for a **range of clusters** $k$ (let‚Äôs say from $1$ to $10$) and for **each value**, we are calculating the **sum of squared distances** from each point to its assigned center (**distortions**).\n",
    "\n",
    "\n",
    "- When the **distortions are plotted** and the **plot looks like an arm** then the **Elbow** (the point of inflection on the curve) is the **best value of** $k$.\n",
    "  \n",
    "  <img src=\"images/L11_Elbow_Method.png\" width=\"800\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Hierarchical Clustering algorithm</h3>\n",
    "\n",
    "- **Hierarchical clustering algorithm** is like $K$-**means clustering** but the difference is that it works in **bottom-up order**.\n",
    "\n",
    "\n",
    "- It works with a **distance matrix**.\n",
    "\n",
    "  In case of **raw data** is provided, the **software** will **automatically compute** a **distance matrix**.\n",
    "  \n",
    "  As an example, the **distance matrix** below shows the distance between **six objects**:\n",
    "\n",
    "  <img src=\"images/L11_Distance_Matrix.png\" width=\"400\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- Hierarchical clustering **starts** by treating **each observation** as a separate **cluster**.\n",
    "  \n",
    "  Then, it repeatedly executes the following two steps: \n",
    "  \n",
    "  1. **identify** the two clusters that are closest together,\n",
    "    \n",
    "  2. **Merge** the two most similar clusters.\n",
    "    \n",
    "  This **iterative process** continues until **all the clusters are merged** together. \n",
    "  \n",
    "  <img src=\"images/L11_Hierarchical_algorithm.png\" width=\"400\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- As a result, hierarchical clustering creates a **dendrogram** that shows the hierarchical relationships between clusters:\n",
    "\n",
    "  <img src=\"images/L11_Dednogram.png\" width=\"800\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Discussion of the Hierarchical Clustering algorithm</h3>\n",
    "\n",
    "- **Measures of distance (similarity)**:\n",
    "\n",
    "  In the example above, the **distance** between two clusters has been computed as the Euclidean distance from one cluster to another. \n",
    "\n",
    "  **Many other** distance metrics have been developed. \n",
    "\n",
    "  The **choice of distance metric** should be made based on theoretical concerns from the domain of study. \n",
    "\n",
    "  That is, a distance metric needs to define similarity in a way that is sensible for the field of study. \n",
    "  \n",
    "  For example, if clustering crime sites in a city, city block distance may be appropriate. Or, better yet, the time taken to travel between each location.\n",
    "  \n",
    "  Where there is **no theoretical justification** for an alternative, the **Euclidean distance** should **generally be preferred**, as it is usually the appropriate measure of distance in the physical world.\n",
    "\n",
    "\n",
    "- **Linkage Criteria**:\n",
    "\n",
    "  After selecting a distance metric, it is necessary to determine **from where distance is computed**.\n",
    "  \n",
    "  For example, it can be computed between the two most similar parts of a cluster (**single-linkage**), the two least similar bits of a cluster (**complete-linkage**), the center of the clusters (**mean or average-linkage**), or some other criterion. \n",
    "  \n",
    "  **Many linkage criteria** have been developed.\n",
    "\n",
    "  As with distance metrics, the choice of linkage criteria should be made based on theoretical considerations from the domain of application. \n",
    "  \n",
    "  A key theoretical issue is what causes variation. \n",
    "  \n",
    "  For example, in archeology, we expect variation to occur through innovation and natural resources, so working out if two groups of artifacts are similar may make sense based on identifying the most similar members of the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
