{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **9 September 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 3**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**\n",
    "\n",
    "Bibliography:\n",
    "<br>\n",
    "[1] Bishop, Christopher M., *Pattern Recognition and Machine Learning*, Springer, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Linear Regression (Part II)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">ML Steps</h3>\n",
    "\n",
    "While we go into details, I would like to list the **steps** that ML go through in case of linear regression:\n",
    "\n",
    "1. **Loading the Data**, i.e. $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "\n",
    "2. **Exploring the Data**, i.e. perform **Exploratory Data Analysis (EDA)** for $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "\n",
    "3. **Slicing the Data**, i.e. split the data into **Train**, **Validation** and **Test** sets.\n",
    "\n",
    "\n",
    "4. **Choosing the Model**, i.e. identifying the form of the function $f(\\mathbf{x}, \\mathbf{w})$.\n",
    "\n",
    " \n",
    "5. **Defining the Cost function**, i.e. the way you measure the distance $E$ between observed $y$ and predicted $\\hat{y}$.\n",
    "\n",
    "\n",
    "6. **Finding the Extrema** of the Coss function, i.e. optimal values $\\hat{\\theta} = \\arg \\min_{\\theta} E(\\theta)$.\n",
    "\n",
    "\n",
    "7. **Evaluating the Accuracy**, i.e. run the model with optimal values of $\\hat{\\theta}$ on **Validation** set.\n",
    "\n",
    "\n",
    "8. **Revisiting Hyperparameters**, i.e. **optimize the hyperparameters** of the model and **re-run steps 4-7**.\n",
    "\n",
    "\n",
    "9. **Checking the Final Result**, i.e. run the obtained model on **Test** set and validate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Linear Basis Function Models</h3>\n",
    "\n",
    "- The simplest linear model for regression is one that involves a **linear combination** of the input variables:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^{D} w_i x_i = w_0 + w_1 x_1 + ... + w_D x_D,$$\n",
    "\n",
    "  where $\\mathbf{x} = [x_1, ..., x_D]^{\\mathbf{T}}$. \n",
    "\n",
    "\n",
    "- The **key property** of this model is that it is a **linear function** of the parameters $w_0, ..., w_D$.\n",
    "\n",
    "\n",
    "- We can **extend** the class of models by considering **linear combinations of fixed nonlinear functions** of the input variables, of the form:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^{D} w_i \\phi_i(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}} \\mathbf{\\phi}(\\mathbf{x}),$$\n",
    "\n",
    "  where $\\phi_i(\\mathbf{x})$ are known as **basis functions**:\n",
    "  \n",
    "  $$\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ \\vdots \\\\w_{D}\n",
    "\\end{bmatrix}\n",
    " \\text{ and }\n",
    "\\mathbf{\\phi} = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0 \\\\ \\vdots \\\\ \\phi_{D}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Fixed Basis Functions</h3>\n",
    "\n",
    "- All of the algorithms are **equally applicable** if we first make a **fixed nonlinear transformation** of the inputs using a vector of basis functions $\\phi(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "- **Suitable choices** of nonlinearity can **make the process** of modelling **easier**.\n",
    "\n",
    "<img src=\"images/L3_Fixed_Basis_Functions.png\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples of Basis Functions</h3>\n",
    "\n",
    "There are many other possible options for basic functions:\n",
    "\n",
    "\n",
    "- **Polinomial** basis function: \n",
    "  \n",
    "  $$\\phi_i(x) = x^i.$$\n",
    "\n",
    "\n",
    "- **Gaussian** basis function: \n",
    "  \n",
    "  $$\\phi_i(x) = \\textrm{exp} \\left (-\\frac{(x-\\mu_i)^2}{2s^2} \\right ).$$\n",
    " \n",
    " \n",
    "- **Sigmoidal** basis function: \n",
    "  \n",
    "  $$\\phi_i(x) = \\sigma \\left ( \\frac{x-\\mu_i}{s} \\right ),$$\n",
    "  \n",
    "  where $\\sigma(a)$ is the **Logistic Sigmoid Function** defined by:\n",
    "    \n",
    "  $$\\sigma(a) = \\frac{1}{1 + e^{-a}}.$$\n",
    "\n",
    "\n",
    "- **Fourier** basis function:  $\\phi_0(x) = w_0 $, $\\phi_{2k+1}(x) = \\cos{n x}$ and $\\phi_{2k}(x) = \\sin{n x}$.\n",
    "\n",
    "  In many signal processing applications, it is of interest to consider basis functions that are localized in both space and frequency, leading to a class of functions known as **wavelets**.\n",
    "\n",
    "\n",
    "- Figure showing **Polynomials** (on the left), **Gaussians** (in the centre), and **Sigmoidal** (on the right) basis functions.\n",
    "\n",
    "<img src=\"images/L3_Basis_Functions.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">The Gaussian Distribution</h3>\n",
    "\n",
    "- In the case of a **single variable** $x$, the **Gaussian distribution**, also known as the **Normal distribution**, can be written in the form:\n",
    "\n",
    "  $$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{{\\left ( 2\\pi \\sigma^2 \\right )}^{1/2}}\\textrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2} \\left( x-\\mu\\right)^2\\right\\},$$\n",
    "\n",
    "  where $\\mu$ is the **mean** and $\\sigma^2$ is the **variance**.\n",
    "\n",
    "\n",
    "- For a $D$-**dimensional vector** $\\textbf{x}$, the **multivariate Gaussian distribution** takes the form:\n",
    "\n",
    "  $$\\mathcal{N}(\\textbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{{\\left ( 2\\pi\\right )}^{D/2}} \\frac{1}{{\\left | \\Sigma \\right |}^{1/2}} \\textrm{exp}\\left\\{ -\\frac{1}{2}\\left( \\textbf{x} - \\mu \\right)^{\\textbf{T}} \\Sigma^{-1} \\left( \\textbf{x} - \\mu \\right)\\right\\},$$\n",
    "\n",
    "  where $\\mu$ is a $D$-dimensional **mean** vector, $\\mathbf{\\Sigma}$ is a $D \\times D$ **covariance matrix**, and $|\\mathbf{\\Sigma}|$ denotes the **determinant** of $\\mathbf{\\Sigma}$.\n",
    "  \n",
    "  \n",
    "  <center><img src=\"images/L3_Normal.svg\" width=\"700\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Sigmoid Function</h3>\n",
    "\n",
    "- **Historical reference**. \n",
    "\n",
    "  - A **Sigmoid function** is a mathematical function whose name comes from the Greek letter **Sigma**.\n",
    "  \n",
    "  - A **Sigmoid function** has a characteristic **$S$-shaped curve** or **Sigmoid curve**.\n",
    "  \n",
    "  - A **Sigmoid function** is a type of **logistic function** and purely refers to any function that retains the $S$-shape. \n",
    "  \n",
    "  - Traditionaly sigmoidal function exists between $0$ and $1$.\n",
    "\n",
    "\n",
    "- A **sigmoid functions** are frequently used in **Machine Learning**, specifically in the testing of **Artificial Neural Networks (NN)**, as a way of understanding the output of a node or **Neuron**. \n",
    "  \n",
    "  \n",
    "  <center><img src=\"images/L3_Sigmoid_Function.png\" width=\"700\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- **Sigmoid functions** have several useful properties:\n",
    "\n",
    "  1. $\\sigma(x)$ is **between 0 and 1**, i.e. $0 \\leq \\sigma(x) \\leq 1$ for any $x \\in (-\\infty , +\\infty )$;\n",
    "\n",
    "  2. $\\sigma(x)$ is **not linear**, i.e. $\\sigma \\left ( \\alpha x + \\beta y \\right ) \\neq \\alpha \\sigma \\left ( x \\right ) + \\beta \\sigma \\left ( y \\right )$;\n",
    "\n",
    "  3. $\\sigma(x)$ is **monotonic**, i.e. if $x < y$ then $\\sigma(x) < \\sigma(y)$;\n",
    "\n",
    "  4. **Derivative** of ${\\sigma}'(x)= \\frac{d}{dx}\\sigma(x)$ is **continuous** and it **can be expressed through** $\\sigma(x)$.\n",
    "\n",
    "\n",
    "- Different **Sigmoid** functions with it's **derivative** are listed below:\n",
    "\n",
    "  - **Logistic** function:\n",
    "    \n",
    "    $$f(x) = \\frac{1}{1+ e^{-x}} \\text { and } \\frac{df}{dx}(x) = f(x)(1-f(x)).$$\n",
    "    \n",
    "  - **Hyperbolic tangent** function:\n",
    "  \n",
    "    $$f(x) = \\tanh x = \\frac{e^x - e^{-x}}{e^x+ e^{-x}} \\text{ and } \\frac{df}{dx}(x) = 1-f(x).$$\n",
    "    \n",
    "  - **Arctangent** function:\n",
    "  \n",
    "    $$f(x) = \\arctan x  \\text{ and } \\frac{df}{dx}(x) = \\frac{1}{1+x^2}.$$\n",
    "\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Fourier Transform</h3>\n",
    "\n",
    "- The **Fourier Transform** is used in ML in a wide range of applications, such as **image analysis**, **image filtering**, **image reconstruction** and **image compression**.\n",
    "\n",
    " \n",
    "  <center><img src=\"images/L3_Fourier_Transform.gif\" width=\"500\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Maximum Likelihood and Least Square Methods</h3>\n",
    "\n",
    "- Lets show that the **least squares approach** could be motivated as the **maximum likelihood solution** under an assumed **Gaussian noise** model.\n",
    "\n",
    "\n",
    "- Let assume that for **some input value** $\\mathbf{x}$ the **target variable** $t$ is given by by a deterministic function with additive **Gaussian noise** so that:\n",
    "\n",
    "  $$t = y( \\mathbf{x}, \\mathbf{w}) + \\varepsilon,$$\n",
    "  \n",
    "  where $\\varepsilon$ is a **zero mean Gaussian random variable** with precision $\\beta = \\frac{1}{\\sigma}$.\n",
    "\n",
    "\n",
    "- Thus we can write **likelihood function** of of the **target variable** $t$ observing the $\\mathbf{x}, \\mathbf{w}$ and $\\beta^{-1}$:\n",
    "\n",
    "  $$p(t | \\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N} (t | y( \\mathbf{x}, \\mathbf{w}), \\beta^{-1}).$$\n",
    "\n",
    "\n",
    "\n",
    "- Now lets consider a **data set of inputs** $\\mathbf{X} = \\{\\mathbf{x_1}, ..., \\mathbf{x_N}\\}$ with corresponding **target values** $\\mathbf{t} = \\{t_1, ..., t_N\\}$.\n",
    "\n",
    "  Making the assumption that these data points are **drawn independently** we obtain the following expression for the **likelihood function**:\n",
    "\n",
    "$$p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \\prod_{n=1}^{N} \\mathcal{N} (t_n | \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}).$$\n",
    "\n",
    "\n",
    "- Let's rewrite the **likelihood function** using the **standard form** for the **Normal distirbution**:\n",
    "\n",
    "  $$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{{\\left ( 2\\pi \\sigma^2 \\right )}^{1/2}}\\textrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2} \\left( x-\\mu\\right)^2\\right\\} \\Rightarrow $$\n",
    "  \n",
    "  $$\\mathcal{N}(t_n | \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}) = \n",
    "    \\frac{\\beta}{\\sqrt{2\\pi}}\\textrm{exp}\\left\\{ -\\frac{\\beta^2}{2} \\left( t_n - \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}) \\right)^2\\right\\} = \n",
    "    \\frac{\\beta}{\\sqrt{2\\pi}}\\textrm{exp}\\left\\{ -\\frac{\\beta^2}{2} E_D(\\mathbf{w}) \\right\\},$$\n",
    "    \n",
    "  where the **sum-of-squares error** function is defined by:\n",
    "\n",
    "  $$E_D(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N}\\left\\{  t_n - \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n})\\right\\}^2.$$\n",
    "\n",
    "- Taking the **logarithm of the likelihood function**, we have:\n",
    "\n",
    "  $$\\ln p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \n",
    "\\sum_{n=1}^{N} \\ln \\mathcal{N} (t_n | \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}) = \n",
    "\\frac{N}{2} \\ln \\beta  - \\frac{N}{2} \\ln (2\\pi)  - \\beta E_D(\\mathbf{w}),$$\n",
    " \n",
    "\n",
    "- We see that **maximization of the likelihood** function **under a conditional Gaussian noise** distribution for a linear model is equivalent to **minimizing a sum-of-squares error** function given by $E_D(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Finding the Extrema of the Cost Function</h3>\n",
    "\n",
    "- To **find the extrema** of the function we need to **calculate the gradient** of this functionand **set it to zero**!\n",
    "\n",
    "\n",
    "- The **gradient** of the **sum-of-squares error** function $E_D(\\mathbf{w})$ takes the form:\n",
    "\n",
    "  $$\\nabla E_D(\\mathbf{w}) = \n",
    "\\sum_{n=1}^{N}\\left\\{  t_n - \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n})\\right\\} \\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}}.$$\n",
    "\n",
    "- **Setting** the **gradient** to **zero** gives:\n",
    "\n",
    "  $$ 0 = \\sum_{n=1}^{N} t_n \\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}} - \\mathbf{w}^{\\mathbf{T}} \\left ( \\sum_{n=1}^{N} \\mathbf{\\phi}(\\mathbf{x_n})\\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}} \\right ).$$\n",
    "\n",
    "\n",
    "- Solving for $\\mathbf{w}$ we obtaing:\n",
    "\n",
    "  $$\\mathbf{w}_{ML}  = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{t} = \\Phi^{\\dagger}  \\mathbf{t}$$\n",
    "\n",
    "  which are known as the **normal equations** for the **least squares problem**.\n",
    "\n",
    "\n",
    "- Here $\\Phi$ is an $N \\times M$ matrix, called the **design matrix**, whose elements are given by $\\Phi_{ni} = \\phi_i(\\mathbf{x_n})$, so that:\n",
    "\n",
    "  $$\\Phi = \\begin{bmatrix}\n",
    " \\phi_0(\\mathbf{x_1}) & \\phi_1(\\mathbf{x_1}) & \\cdots & \\phi_{M-1}(\\mathbf{x_1})  \\\\ \n",
    " \\phi_0(\\mathbf{x_2}) & \\phi_1(\\mathbf{x_2}) & \\cdots & \\phi_{M-1}(\\mathbf{x_2})  \\\\ \n",
    "\\vdots  & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "\\phi_0(\\mathbf{x_N}) & \\phi_1(\\mathbf{x_N}) & \\cdots & \\phi_{M-1}(\\mathbf{x_N})\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "  And the quantity:\n",
    "\n",
    "  $$\\Phi^{\\dagger} \\equiv \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}}$$\n",
    "\n",
    "  is known as the **Moore-Penrose pseudo-inverse** of the matrix $\\Phi$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multiple Outputs</h3>\n",
    "\n",
    "- So far, we have considered the case of a **single target variable** $t$.\n",
    "\n",
    "\n",
    "- In some applications, we may wish to predict $K > 1$ target variables.\n",
    "\n",
    "\n",
    "- This could be done by introducing a **different set of basis functions** for each component of $t$, leading to **multiple, independent regression problems** $\\rightarrow$ **Not interesting**.\n",
    "\n",
    "\n",
    "- More **common approach** is to **use** the **same set of basis functions**:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = \\mathbf{W}^{\\mathbf{T}}\\phi(\\mathbf{x}),$$\n",
    "\n",
    "  where $y$ is a $K$-dimensional column vector, $\\mathbf{W}$ an $M \\times K$ matrix of parameters,\n",
    "and $\\phi(\\mathbf{x})$ is an $M$-dimensional column vector with elements $\\phi_j(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "- Suppose we take the conditional distribution of the target vector to be an isotropic Gaussian of the form:\n",
    "\n",
    "  $$p(\\mathbf{t} | \\mathbf{x}, \\mathbf{W}, \\beta) = \\mathcal{N} \\left ( \\mathbf{t} | \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x}), \\beta^{-1}\\mathbf{I} \\right ).$$\n",
    "\n",
    "\n",
    "- If we have a set of observations ${\\mathbf{t_1}, ..., \\mathbf{t_N}}$, we can combine these into a matrix $\\mathbf{T}$ of size $N \\times K$.\n",
    "\n",
    "  Similarly, we can combine the input vectors ${\\mathbf{x_1}, ..., \\mathbf{x_N}}$ into a matrix $\\mathbf{X}$ of size $N \\times M$.\n",
    "\n",
    "\n",
    "- The **log likelihood function** is then given by:\n",
    "\n",
    "  $$\\ln p(\\mathbf{T | \\mathbf{X}, \\mathbf{W}, \\beta}) = \n",
    "\\sum_{n=1}^{N} \\ln \\mathcal{N} \\left ( \\mathbf{t_n} | \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x_n}), \\beta^{-1}\\mathbf{I}\\right ) = \\frac{NK}{2} \\ln \\left( \\frac{\\beta}{ 2\\pi}\\right)  - \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left \\| \\mathbf{t_n} - \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x_n}) \\right \\|^2.$$\n",
    "\n",
    "\n",
    "- As before, we can maximize this function with respect to $\\mathbf{W}$, giving:\n",
    "\n",
    "  $$\\mathbf{W}_{ML} = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{T}.$$\n",
    "\n",
    "\n",
    "- If we examine this result for each target variable $t_k$, we have:\n",
    "\n",
    "  $$\\mathbf{w}_k = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{t}_k = \\Phi^{\\dagger} \\mathbf{t}_k.$$\n",
    "\n",
    "- Thus the solution to the regression problem **decouples** between the different target variables, and **we need only compute a single pseudo-inverse matrix** $\\Phi^{\\dagger}$, which is shared by all of the vectors $\\mathbf{w}_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Batch Techniques and Sequential Learning</h3>\n",
    "\n",
    "- **Batch techniques** involve processing the entire training set in **one go**.<br>\n",
    "\n",
    "  For example, **Normal Equation solution** for **sum-of-squares error** function. \n",
    "  \n",
    "  \n",
    "- **Sequential algorithms** consider the data points one at a time and the model parameters updated after each such presentation.\n",
    "\n",
    "  For example, **Stochastic Gradient Descent (SGD)**\n",
    "  \n",
    "  $$\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E_n, $$\n",
    "  \n",
    "  where $\\tau$ denotes the **iteration number**, and $\\eta$ is a **learning rate parameter**.\n",
    "\n",
    "  <center><img src=\"images/L3_Learning_Rate.png\" width=\"500\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "- In the case of the **sum-of-squares error** function, this gives:\n",
    "\n",
    "  $$\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} + \\eta (t_n  - {\\mathbf{w}^{(\\tau)}}^{\\mathbf{T}}\\phi(\\mathbf{x_n}))\\phi(\\mathbf{x_n}),$$\n",
    "  \n",
    "  which is known as **least-mean-squares** or the **LMS algorithm**.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Evaluating the Accuracy</h3>\n",
    "\n",
    "- As we discussed before, to **Bias** and **Variance** are very usefull in **evaluating the accuracy** of our fitted model.\n",
    "\n",
    "\n",
    "-  **Bias** of an estimator is the **expected difference** between its **estimates** and the **true values** in the data:\n",
    "\n",
    "$$\\operatorname{Bias}(\\hat{f}(x_0))=f(x_0)-\\mathbf{E}\\left [\\hat{f}(x_0) \\right ]$$\n",
    "\n",
    " - **Variance** of an estimator is defined as the **expected value** of the **squared difference** between the **estimate** of a model and the **expected value** of the **estimate**:\n",
    "\n",
    "$$\\operatorname{Var}(\\hat{f}(x_0))=\\mathbf{E}\\left [ \\left(\\hat{f}(x_0)-\\mathbf{E} \\left[\\hat{f}(x_0)\\right] \\right)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "- Let's consider the dataset generated by the **true function** $f(x) = 4x^2 + 3x + 2$.\n",
    "\n",
    "\n",
    "- On the **left side** plot you can see the result of the fit of this function with the **four different models**.\n",
    "\n",
    "- On the **right side** the visualization of the **bias** and **variance** is shown.\n",
    "\n",
    "- True value is calculated at $x_0 = 5$.\n",
    "\n",
    "<center><img src=\"images/L3_Polynomials.png\" width=\"1000\" alt=\"Example\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Another Example</h3>\n",
    "\n",
    "\n",
    "<center><img src=\"images/L3_Number_of_Degree.gif\" width=\"800\" alt=\"Example\" /></center>\n",
    "\n",
    "<center><img src=\"images/L3_Generated_Samples.gif\" width=\"800\" alt=\"Example\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, the **degree of the polynomial**, i.e. the **number of features**, is the **hyperparameter**.\n",
    "\n",
    "\n",
    "- **Questions**:\n",
    " - How to choose the hyperparameter correctly?  $\\rightarrow$ **Bias-Variance Trade-Off**\n",
    " - Is there any other way to solve the problem of **overfitting**? $\\rightarrow$ **Regularization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bias–Variance Decomposition</h3>\n",
    "\n",
    "- **Total expected error** a point $x_0$ is defined as follows:\n",
    "\n",
    "$$\\mathbf{E} \\left [ \\left ( f(x_0) - \\hat{f}(x_0)\\right )^2\\right ].$$\n",
    "\n",
    "- **Whichever model**  $\\hat{f}$ we choose, its expected error can be further **decomposed** as:\n",
    "\n",
    "$$\\mathbf{E} \\left [\\left(f(x_0) - \\hat{f}(x_0)\\right)^2\\right]\n",
    "= \\left(\\operatorname{Bias}\\left[\\hat{f}(x_0)\\right] \\right) ^2 + \\operatorname{Var}\\left[\\hat{f}(x_0)\\right] + \\sigma^2,$$\n",
    "\n",
    "&emsp; &emsp; &ensp;where $\\sigma^2$ is an **irreducible error** which we can't get rid off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bias-Variance Trade-Off</h3>\n",
    "\n",
    "$\\bullet$ Usually in **Machine Learning** there is a **trade-off** between model's **Bias** and **Variance**.\n",
    "\n",
    "$\\bullet$ As we gradually **grow model's capacity**, we gradually **reduce bias** and **increase variance**.\n",
    "\n",
    "$\\bullet$ The **goal** is to **find a sweet spot** where both bias and variance are acceptable.\n",
    "\n",
    "<center><img src=\"images/L3_Bias_vs_Variance.png\" width=\"500\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regularization</h3>\n",
    "\n",
    "- **Regularization** is a technique used to **address overfitting**;\n",
    "\n",
    "\n",
    "- **Regularization**, significantly **reduces the variance** of the model, **without** substantial **increase in its bias**.\n",
    "\n",
    "\n",
    "- **Main idea** of regularization is to **keep all the features**, but **reduce magnitude of parameters** $\\boldsymbol{\\theta}$;\n",
    "\n",
    "\n",
    "- A **regularization term** (or **regularizer**) $R(f)$ is added to a **loss function**:\n",
    "\n",
    "  $$E_{reg}(\\boldsymbol{\\theta}) = E(\\boldsymbol{\\theta}) + R(f).$$\n",
    "\n",
    "\n",
    "- Mainly, there are two types of forms of regularization:\n",
    "  - **Ridge regularization**, or **Tikhonov regularization**, uses he $L_2$ **norm** of the vector $\\boldsymbol{\\theta}$:\n",
    "\n",
    "    $$R(f) = \\lambda \\cdot \\| \\mathbf{w} \\|^2_2 = \\lambda \\cdot \\mathbf{w}^T\\mathbf{w} = \\lambda \\cdot \\sum_{i=1}^n w_i^2$$\n",
    "  \n",
    "  - **Lasso regularization** uses he $L_1$ **norm** of the vector $\\mathbf{w}$:\n",
    "\n",
    "    $$R(f) = \\lambda \\cdot \\| \\mathbf{w} \\|_1 = \\lambda \\cdot \\sum_{i=1}^n |w_i|$$\n",
    "\n",
    "  - $\\lambda$ is a **hyperparameter**, called **regularization parameter**.\n",
    "\n",
    "\n",
    "- Plot of the contours of the **unregularized error function (blue)** along with the **constraint region** for \n",
    "\n",
    "  the **Ridge regularer** $q = 2$ on the **left** and the **Lasso regularizer** $q = 1$ on the **right**,\n",
    "  \n",
    "  in which the **optimum value** for the parameter **vector $\\mathbf{w}$** is **denoted by $\\mathbf{w}^{\\star}$**.\n",
    "\n",
    "<center><img src=\"images/L3_Regularization_2.png\" width=\"800\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "\n",
    "- A more general regularizer is sometimes used, for which the **regularized error** takes the form:\n",
    "\n",
    "  $$R(f) = \\lambda \\cdot \\sum_{i=1}^n  \\| w_i \\|^q$$\n",
    "\n",
    "\n",
    "- Contours of the **regularization term** for various values of the **parameter $q$** are shown below:\n",
    "\n",
    "<center><img src=\"images/L3_Regularization.png\" width=\"800\" alt=\"Example\" /></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The result of the **fitting** of the **true function** $f(x) = \\sin 2\\pi x$ to the **polynomial function of $10$-th degree** with **regularization** for different **learning rate** $\\lambda$ are presented below:\n",
    "\n",
    "<center><img src=\"images/L3_Learning_Rate.gif\" width=\"800\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
