{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CS596 - Machine Learning**\n",
    "<br>\n",
    "Date: **26 October 2020**\n",
    "\n",
    "\n",
    "Title: **Lecture 8**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Teaching Assistant: **Levan Sanadiradze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Convolutional Neural Networks (CNN)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous lecture: \n",
    "  - They are made up of neurons that have learnable weights and biases. \n",
    "  - Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. \n",
    "\n",
    "\n",
    "- The whole network still expresses a single differentiable score function: \n",
    "  - From the raw image pixels on one end to class scores at the other. \n",
    "  - And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer.\n",
    "  - The tips/tricks we developed for learning regular Neural Networks still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">So What Changes?</h3>\n",
    "\n",
    "- CNN architectures make the explicit assumption that the **inputs are images**, which allows us to encode certain properties into the architecture. \n",
    "\n",
    "- These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Image Classification Problem using CIFAR-10</h3>\n",
    "\n",
    "- For demonstration purposes, we will use the **CIFAR-10 dataset**:\n",
    "\n",
    "  <a href=\"https://www.kaggle.com/c/cifar-10\">CIFAR-10</a>  is an established computer-vision dataset used for object recognition collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton: \n",
    "\n",
    "  It is a **subset** of the **80 million tiny images dataset** and consists of **50,000 32x32 color images** containing one of **10 object classes**, with **5000 images per class**.\n",
    "\n",
    "  Classes in the dataset, as well as 10 random images from each, are presented below:\n",
    "\n",
    "  <img src=\"images/L8_Cifar10.png\" width=\"600\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Why not Rergural Neural Networs?</h3>\n",
    "\n",
    "- **Recall**: **regular Neural Networks**.\n",
    "\n",
    "  Neural Networks receive an **input** (a **single vector**), and transform it through a series of **hidden layers**. \n",
    "  \n",
    "  Each **hidden layer** is made up of a set of neurons, where **each neuron** is **fully connected** to all **neurons** in the **previous layer**.\n",
    "  \n",
    "  **Neurons** in a **single layer** function completely **independently** and have **no common connections**.\n",
    "  \n",
    "  The **last fully-connected layer** is called the **output layer** and in classification settings it represents the **class scores**.\n",
    "\n",
    "\n",
    "  <br>\n",
    "  <img src=\"images/L8_RNN.jpg\" width=\"500\" alt=\"Example\" />\n",
    "  <br>\n",
    "\n",
    "\n",
    "- **Regular Neural Networks don’t scale well to full images**. \n",
    "\n",
    "  For example, in CIFAR-10 dataset, images are only of size $32 \\times 32 \\times 3$ (**$32$ wide**, **$32$ high**, **$3$ color channels**), so a single fully-connected neuron in a **first hidden layer** of a Regular Neural Network would have $32 \\times 32 \\times 3 = 3072$ **weights**. \n",
    "  \n",
    "  This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. \n",
    "  \n",
    "  For example, an **image of more respectable size**, e.g. $200 \\times 200 \\times 3$, would lead to neurons that have $200 \\times 200 \\times 3 = 120,000$ **weights**. \n",
    "  \n",
    "  Moreover, we would almost certainly want to have several such neurons, so the **parameters would add up quickly**!\n",
    "  \n",
    "  Clearly, this **full connectivity is wasteful** and the **huge number of parameters** would quickly **lead** to **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Architecture Overview</h3>\n",
    "\n",
    "- Convolutional Neural Networks take advantage of the fact that the **input consists of images** and they constrain the architecture in a more sensible way.\n",
    "\n",
    "\n",
    "- The **Layers** of a CNN have neurons arranged in **3 dimensions**: **width, height, depth**.\n",
    "\n",
    "\n",
    "<img src=\"images/L8_CNN3.jpeg\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "- The **neurons in a layer** will only be **connected** to a **small region** of the **layer before** it, instead of all of the neurons in a fully-connected manner.\n",
    "\n",
    "\n",
    "- The **final output layer** would for **CIFAR-10** have dimensions $1 \\times 1 \\times 10$, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. \n",
    "\n",
    "\n",
    "<img src=\"images/L8_CNN.jpeg\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Layers used to build CNNs</h3>\n",
    "\n",
    "On CIFAR-10 example, we can highlight **several type** of layers to build CNN **architectures**:\n",
    "\n",
    "- **Input Layer**: \n",
    "\n",
    "  Holds the **raw pixel values** of the **image**, in our case an image of $32 \\times 32$ and with three **color channels** (**R, G, B**).\n",
    "  \n",
    "  \n",
    "- **Convolutional Layer**: \n",
    "\n",
    "  Computes the **output of neurons** that are **connected** to **local regions** in the **previouse layer**.\n",
    "  \n",
    "  Uses technique called **convolution** that uses **filters** and may result in volume such as $32 \\times 32 \\times 12$ if we decided to use $12$ filters.\n",
    "  \n",
    "  Performs transformations that are a function of **not only the activations** in the input volume, **but also** of the **parameters**.\n",
    "  \n",
    "  **Does most of the computations**!\n",
    "  \n",
    "\n",
    "- **RELU Layer**:\n",
    "\n",
    "  Applies an **elementwise activation function**, such as the $max(0,x)$ thresholding at zero. \n",
    "  \n",
    "  Leaves the size of the volume unchanged: $32 \\times 32 \\times 12$.\n",
    "  \n",
    "  Performs transformations that are a **fixed function** of the activations in the input volume and are **independent of parameters**.\n",
    "  \n",
    "\n",
    "- **Pooling Layer**:\n",
    "\n",
    "  Performs a **downsampling** operation **along the spatial dimensions** (**width**, **height**), resulting in volume such as $16 \\times 16 \\times 12$.\n",
    "  \n",
    "  Performs transformations that are a **fixed function** of the activations in the input volume and are **independent of parameters**.\n",
    "  \n",
    "\n",
    "- **Fully-Connected Layer**:\n",
    "\n",
    "  **Computes the class scores**, resulting in volume of size $1 \\times 1 \\times 10$, where each of the $10$ **numbers** **correspond** to the **10 categories** of **CIFAR-10**.\n",
    "  \n",
    "  As with ordinary Neural Networks and as the name implies, each **neuron in this layer** will be **connected** to **all the numbers** in the **previous volume**.\n",
    "  \n",
    "  Performs transformations that are a function of **not only the activations** in the input volume, **but also** of the **parameters**.\n",
    "  \n",
    "  \n",
    "  <img src=\"images/L8_CNN2.jpeg\" width=\"900\" alt=\"Example\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Convolutional Layer</h3>\n",
    "\n",
    "- The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.\n",
    "\n",
    "\n",
    "- Conv Layers have **filters** which **detect** the **patterns**. \n",
    "\n",
    "\n",
    "- Different patterns in an image are **Multiple edges**, **Shapes**, **Textures**, **Objects**, etc:\n",
    "\n",
    " \n",
    " <img src=\"images/L8_Patterns.png\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "- Every **filter** is **small spatially** (along width and height), but **extends through the full depth** of the input volume. \n",
    "\n",
    "\n",
    "- For example, a typical filter on a first layer of a ConvNet might have size $5 \\times 5 \\times 3$, i.e. $5$ pixels **width** and **height**, and $3$ because images have **depth** $3$, the **color channels**. \n",
    "\n",
    "\n",
    "- **During the forward pass**, we **slide** (or more precisely, **convolve**) each **filter** **across** the **width** and **height** of the input volume and **compute dot products** between the entries of the filter and the input at any position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Example 1**:\n",
    "\n",
    "  For example, an **RGB CIFAR-10** image has size $32 \\times 32 \\times 3$.\n",
    "  \n",
    "  If the **filter size** is $5 \\times 5$, then each neuron in the Conv Layer will have **total** $5 \\times 5 \\times 3 = 75$ **weights** (and $+1$ **bias parameter**). \n",
    "  \n",
    "  **Notice** that the **extent of the connectivity along the depth axis must be 3**, since this is the depth of the input volume.\n",
    "\n",
    "\n",
    "- **Example 2**: \n",
    "\n",
    "  Suppose an input volume had size $16 \\times 16 \\times 20$.\n",
    "  \n",
    "  Then using the **filter size** of $3 \\times 3$, every neuron in the Conv Layer would now have a total of $3 \\times 3 \\times 20 = 180$ **connections to the input volume**. \n",
    "  \n",
    "  **Notice** that, again, the **connectivity is local in space** (e.g. $3 \\times 3$), but **full along the input depth**, i.e. $20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Spatial arrangement.</h3>\n",
    "\n",
    "- **Three hyperparameters** control the size of the output volume: the **depth**, **stride** and **zero-padding**:\n",
    "\n",
    "\n",
    "1. The **depth** of the **output volume**: \n",
    "\n",
    "   It corresponds to the number of filters we would like to use, each learning to look for something different in the input. \n",
    "   \n",
    "   For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. \n",
    "   \n",
    "   We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre).\n",
    "\n",
    "\n",
    "2. The **stride** with **which** we **slide the filter**. \n",
    "\n",
    "   When the **stride is** $1$ then we move the filters **one pixel at a time**.\n",
    "   \n",
    "   When the **stride is** $2$ then the filters **jump 2 pixels at a time** as we slide them around. \n",
    "   \n",
    "   **This** will **produce smaller output volumes** spatially.\n",
    "\n",
    "\n",
    "3. **Zero-padding** is used somemetimes to pad the input volume with zeros around the border. \n",
    "\n",
    "   The **size** of this **zero-padding** is a **hyperparameter**. \n",
    "   \n",
    "   The nice feature of zero padding is that it will **allow us** to **control the spatial size** of the **output volumes**.\n",
    "   \n",
    "   \n",
    "- Thus, we can **compute** the **spatial size** of the **output volume** as a function of the **input volume size** ($W$), the **filter size** of the Conv Layer neurons ($F$), the **stride** with which they are applied ($S$), and the amount of **zero padding** used ($P$) on the border:\n",
    "\n",
    "  $$\\frac{W−F+2P}{S+1}.$$\n",
    "  \n",
    "  \n",
    "\n",
    "- For example for a $7 \\times 7$ **input** and a $3 \\times 3$ **filter** with **stride** $1$ and **pad** $0$ we would get a $5 \\times 5$ **output**. \n",
    "\n",
    "  With **stride** $2$ we would get a $3 \\times 3$ **output**. Lets also see one more graphical example:\n",
    "  \n",
    "  \n",
    "  \n",
    "- **Note** that the spatial arrangement hyperparameters have mutual constraints. \n",
    "\n",
    "  For example, when the **input has size** $W=10$, **no zero-padding** is used $P=0$, and the **filter size** is $F=3$, then it would be **impossible to use stride** $S=2$, since $(W−F+2P)/S+1=(10−3+0)/2+1=4.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Real-World Example</h3>\n",
    "\n",
    "- The **Krizhevsky et al**. architecture that **won** the **ImageNet challenge in 2012** accepted images of size $227 \\times 227 \\times 3$. \n",
    "\n",
    "\n",
    "- On the **first Convolutional Layer**, it used neurons with receptive **field size** $F=11$, **stride** $S=4$ and **no zero padding** $P=0$. \n",
    "\n",
    "  Since $(227 - 11)/4 + 1 = 55$, and since the **Conv layer had a depth** of $K=96$, the Conv layer output volume had size $55 \\times 55 \\times 96$. \n",
    "  \n",
    "- **Each** of the $55 \\times 55 \\times 96$ **neurons** in this volume **was connected to a region of size** $11 \\times 11 \\times 3$ in the **input volume**. \n",
    "  \n",
    "  Moreover, **all $96$ neurons** in each depth column are **connected** to the same $11 \\times 11 \\times 3$ region of the input, but of course with different weights. \n",
    "  \n",
    "  As a fun aside, if you read the actual paper it claims that the input images were $224 \\times 224$, which is surely incorrect because $(224 - 11)/4 + 1$ is quite clearly not an integer. \n",
    "  \n",
    "  **This has confused many people** in the history of ConvNets and little is known about what happened. \n",
    "  \n",
    "  One guess is that **Alex** used **zero-padding** of $P=3$ extra pixels that **he does not mention in the paper**.\n",
    "  \n",
    "    <img src=\"images/L8_Filter.jpeg\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Parameter Sharing</h3>\n",
    "\n",
    "- Using the **real-world example** above, we see that there are $55 \\times 55 \\times 96 = 290,400$ neurons in the **first Conv Layer**, and each has $11 \\times 11 \\times 3 = 363$ **weights** and $1$ **bias**. \n",
    "\n",
    "  Together, this **adds up to** $290400 \\times 364 = 105,705,600$ **parameters** on the **first layer of the ConvNet alone**.\n",
    "  \n",
    "  Clearly, this number is very high.\n",
    "  \n",
    "  \n",
    "- Thus **parameter sharing scheme** is used in Convolutional Layers to **control the number of parameters**.\n",
    "\n",
    "\n",
    "- It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: \n",
    "\n",
    "  That if **one feature** is **useful** to **compute at some spatial position** $(x,y)$, then it **should also be useful** to **compute** at a **different position** $(x',y')$.\n",
    "  \n",
    "  \n",
    "- **We will discuss this more in details during the Seminar!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>Convolution demo</title>\n",
       "\n",
       "<script type=\"text/javascript\" src=\"external/d3.min.js\"></script>\n",
       "<script type=\"text/javascript\" src=\"utils.js\"></script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "body {\n",
       "  margin: 0;\n",
       "  padding: 0;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var W1 = 7;\n",
       "var H1 = 7;\n",
       "var D1 = 3;\n",
       "\n",
       "var K = 2;\n",
       "var F = 3;\n",
       "var S = 2; // stride\n",
       "\n",
       "var cs = 25; // cell size\n",
       "\n",
       "var X = new U.Vol(W1, H1, D1); // input volume\n",
       "for(var q=0;q<X.w.length;q++) {\n",
       "  X.w[q] = Math.floor(Math.random()*3);\n",
       "  // 0 pad with P = 1\n",
       "  for(var d=0;d<X.depth;d++) {\n",
       "    for(var x=0;x<X.sx;x++) {\n",
       "      for(var y=0;y<X.sy;y++) {\n",
       "        if(x === 0 || x === (X.sx - 1) || y === 0 || y === (X.sy - 1)) {\n",
       "          X.set(x,y,d,0);\n",
       "        }\n",
       "      }  \n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "var Ws = [];\n",
       "var bs = [];\n",
       "for(var k=0;k<K;k++) {\n",
       "  var W = new U.Vol(F, F, D1);\n",
       "  for(var q=0;q<W.w.length;q++) {\n",
       "    W.w[q] = Math.floor(Math.random()*3) - 1;\n",
       "  }\n",
       "  Ws.push(W);\n",
       "  var b = new U.Vol(1,1,1);\n",
       "  b.w[0] = 1 - k;\n",
       "  bs.push(b);\n",
       "}\n",
       "\n",
       "var conv_forward = function(V, Ws, bs, stride) {\n",
       "  // optimized code by @mdda that achieves 2x speedup over previous version\n",
       "  var out_sy = ((V.sy-W.sy)/stride +1);\n",
       "  var out_sx = ((V.sx-W.sx)/stride +1);\n",
       "  var A = new U.Vol(out_sx |0, out_sy |0, Ws.length |0, 0.0);\n",
       "  \n",
       "  var V_sx = V.sx |0;\n",
       "  var V_sy = V.sy |0;\n",
       "  var xy_stride = stride |0;\n",
       "\n",
       "  for(var d=0;d<Ws.length;d++) {\n",
       "    var f = Ws[d];\n",
       "    var x = 0;\n",
       "    var y = 0;\n",
       "    for(var ay=0; ay<out_sy; y+=xy_stride,ay++) {  // xy_stride\n",
       "      x = 0;\n",
       "      for(var ax=0; ax<out_sx; x+=xy_stride,ax++) {  // xy_stride\n",
       "\n",
       "        // convolve centered at this particular location\n",
       "        var a = 0.0;\n",
       "        for(var fy=0;fy<f.sy;fy++) {\n",
       "          var oy = y+fy; // coordinates in the original input array coordinates\n",
       "          for(var fx=0;fx<f.sx;fx++) {\n",
       "            var ox = x+fx;\n",
       "            if(oy>=0 && oy<V_sy && ox>=0 && ox<V_sx) {\n",
       "              for(var fd=0;fd<f.depth;fd++) {\n",
       "                // avoid function call overhead (x2) for efficiency, compromise modularity :(\n",
       "                a += f.w[((f.sx * fy)+fx)*f.depth+fd] * V.w[((V_sx * oy)+ox)*V.depth+fd];\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "        a += bs[d].w[0];\n",
       "        A.set(ax, ay, d, a);\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  return A;\n",
       "}\n",
       "\n",
       "function renderVol(svg, V, xoff, yoff, col, title, vid) {\n",
       "\n",
       "  var pad = 3;\n",
       "  var dpad = 20;\n",
       "\n",
       "  var gyoff = 20;\n",
       "\n",
       "  var txt = title + ' (' + V.sx + 'x' + V.sy + 'x' + V.depth + ')';\n",
       "  // 1 padding exception\n",
       "  //if(vid === 'x') { txt = title + ' (' + (V.sx-2) + 'x' + (V.sy-2) + 'x' + V.depth + ')'; }\n",
       "\n",
       "  svg.append('text')\n",
       "    .attr('x', xoff)\n",
       "    .attr('y', yoff - 5)\n",
       "    .attr('font-size', 16)\n",
       "    .attr('fill', 'black')\n",
       "    .text(txt);\n",
       "\n",
       "  for(var d = 0; d < V.depth; d++) {\n",
       "\n",
       "    svg.append('text')\n",
       "      .attr('x', xoff)\n",
       "      .attr('y', yoff + d * (V.sy * (cs + pad) + dpad) + gyoff - 5)\n",
       "      .attr('font-size', 16)\n",
       "      .attr('fill', 'black')\n",
       "      .attr('style', 'font-family: courier;')\n",
       "      .text(vid + '[:,:,'+d+']');\n",
       "\n",
       "    for(var x = 0; x < V.sx; x++) {\n",
       "      for(var y = 0; y < V.sy; y++) {\n",
       "\n",
       "        var xcoord = xoff + x * (cs + pad);\n",
       "        var ycoord = yoff + y * (cs + pad) + d * (V.sy * (cs + pad) + dpad) + gyoff;\n",
       "\n",
       "        var thecol = col;\n",
       "        if(vid === 'x' && (x === 0 || y === 0 || x === V.sx - 1 || y === V.sy - 1)) {thecol = '#DDD';}\n",
       "\n",
       "        svg.append('rect')\n",
       "          .attr('x', xcoord)\n",
       "          .attr('y', ycoord)\n",
       "          .attr('height', cs)\n",
       "          .attr('width', cs)\n",
       "          .attr('fill', thecol)\n",
       "          .attr('stroke', 'none')\n",
       "          .attr('stroke-width', '2')\n",
       "          .attr('id', vid+'_'+x+'_'+y+'_'+d)\n",
       "          .attr('class', vid);\n",
       "\n",
       "        svg.append('text')\n",
       "          .attr('x', xcoord + 5)\n",
       "          .attr('y', ycoord + 15)\n",
       "          .attr('font-size', 16)\n",
       "          .attr('fill', 'black')\n",
       "          .text(V.get(x,y,d).toFixed(0));\n",
       "\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "function draw() {\n",
       "  var d3elt = d3.select('#draw');\n",
       "  svg = d3elt.append('svg').attr('width', '100%').attr('height', '100%')\n",
       "        .append('g').attr('transform', 'scale(1)');\n",
       "\n",
       "  var yoff = 20;\n",
       "  // render input volume\n",
       "  renderVol(svg, X, 10, yoff, '#DDF', 'Input Volume (+pad 1)', 'x');\n",
       "\n",
       "  for(var i=0;i<Ws.length;i++) {\n",
       "    // render weights\n",
       "    renderVol(svg, Ws[i], 270 + i*170, yoff, '#FDD', 'Filter W'+i, 'w'+i);\n",
       "    // render biases\n",
       "    renderVol(svg, bs[i], 270 + i*170, 350 + yoff, '#FDD', 'Bias b'+i, 'b'+i);\n",
       "  }\n",
       "\n",
       "  // render output\n",
       "  renderVol(svg, O, 600, yoff, '#DFD', 'Output Volume', 'o');\n",
       "\n",
       "  // render controls\n",
       "  \n",
       "  svg.append('text')\n",
       "    .attr('x', 520)\n",
       "    .attr('y', 470)\n",
       "    .attr('font-size', 16)\n",
       "    .attr('fill', 'black')\n",
       "    .text('toggle movement');\n",
       "  svg.append('rect')\n",
       "    .attr('x', 500)\n",
       "    .attr('y', 450)\n",
       "    .attr('height', 30)\n",
       "    .attr('width', 150)\n",
       "    .attr('fill', \"rgba(200, 200, 200, 0.1)\")\n",
       "    .attr('stroke', 'black')\n",
       "    .attr('stroke-width', '2')\n",
       "    .attr('style', 'cursor:pointer;')\n",
       "    .on('click', function() {\n",
       "      // toggle \n",
       "      if(iid === -1) {\n",
       "        iid = setInterval(focusCell, 1000);\n",
       "      } else {\n",
       "        clearInterval(iid);\n",
       "        iid = -1;\n",
       "      }\n",
       "    });\n",
       "}\n",
       "\n",
       "var fxg = 0;\n",
       "var fyg = 0;\n",
       "var fdg = 0;\n",
       "function focusCell() {\n",
       "  \n",
       "  // first unfocus all\n",
       "  for(var i=0;i<Ws.length;i++) {\n",
       "    d3.selectAll('.w'+i).attr('stroke', 'none');\n",
       "    d3.selectAll('.b'+i).attr('stroke', 'none');\n",
       "  }\n",
       "  d3.selectAll('.x').attr('stroke', 'none');\n",
       "  d3.selectAll('.o').attr('stroke', 'none');\n",
       "\n",
       "  var fx = fxg;\n",
       "  var fy = fyg;\n",
       "  var fd = fdg;\n",
       "\n",
       "  // highlight the output cell\n",
       "  var csel = d3.select('#o'+'_'+fx+'_'+fy+'_'+fd);\n",
       "  csel.attr('stroke', '#0A0');\n",
       "\n",
       "  // highlight the weights\n",
       "  d3.selectAll('.w'+fd).attr('stroke', '#A00');\n",
       "  // highlight the bias\n",
       "  d3.selectAll('.b'+fd).attr('stroke', '#A00');\n",
       "\n",
       "  d3.selectAll('.ll').remove();\n",
       "\n",
       "  // highlight the input cell\n",
       "  for(var d=0;d<D1;d++) {\n",
       "    for(var x=0;x<F;x++) {\n",
       "      for(var y=0;y<F;y++) {\n",
       "        var ix = fx * S + x;\n",
       "        var iy = fy * S + y;\n",
       "        var id = d;\n",
       "        var csel = d3.select('#x'+'_'+ix+'_'+iy+'_'+id);\n",
       "        csel.attr('stroke', '#00A');\n",
       "\n",
       "        // connect with line\n",
       "        if(x === 0 && y === 0) {\n",
       "          var wsel = d3.select('#w'+fd+'_'+x+'_'+y+'_'+d);\n",
       "          svg.append('line')\n",
       "            .attr('x1', csel.attr('x'))\n",
       "            .attr('y1', csel.attr('y'))\n",
       "            .attr('x2', wsel.attr('x'))\n",
       "            .attr('y2', wsel.attr('y'))\n",
       "            .attr('stroke', 'black')\n",
       "            .attr('stroke-width', '1')\n",
       "            .attr('class', 'll');\n",
       "        }\n",
       "        if(x === 0 && y === (F-1)) {\n",
       "          var wsel = d3.select('#w'+fd+'_'+x+'_'+y+'_'+d);\n",
       "          svg.append('line')\n",
       "            .attr('x1', csel.attr('x'))\n",
       "            .attr('y1', parseFloat(csel.attr('y')) + cs)\n",
       "            .attr('x2', wsel.attr('x'))\n",
       "            .attr('y2', parseFloat(wsel.attr('y')) + cs)\n",
       "            .attr('stroke', 'black')\n",
       "            .attr('stroke-width', '1')\n",
       "            .attr('class', 'll');\n",
       "        }\n",
       "        if(x === (F-1) && y === 0) {\n",
       "          var wsel = d3.select('#w'+fd+'_'+x+'_'+y+'_'+d);\n",
       "          svg.append('line')\n",
       "            .attr('x1', parseFloat(csel.attr('x')) + cs)\n",
       "            .attr('y1', csel.attr('y'))\n",
       "            .attr('x2', parseFloat(wsel.attr('x')) + cs)\n",
       "            .attr('y2', wsel.attr('y'))\n",
       "            .attr('stroke', 'black')\n",
       "            .attr('stroke-width', '1')\n",
       "            .attr('class', 'll');\n",
       "        }\n",
       "        if(x === (F-1) && y === (F-1)) {\n",
       "          var wsel = d3.select('#w'+fd+'_'+x+'_'+y+'_'+d);\n",
       "          svg.append('line')\n",
       "            .attr('x1', parseFloat(csel.attr('x')) + cs)\n",
       "            .attr('y1', parseFloat(csel.attr('y')) + cs)\n",
       "            .attr('x2', parseFloat(wsel.attr('x')) + cs)\n",
       "            .attr('y2', parseFloat(wsel.attr('y')) + cs)\n",
       "            .attr('stroke', 'black')\n",
       "            .attr('stroke-width', '1')\n",
       "            .attr('class', 'll');\n",
       "        }\n",
       "        \n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // output focus cycle\n",
       "  fxg++;\n",
       "  if(fxg >= O.sx) {\n",
       "    fxg = 0;\n",
       "    fyg++;\n",
       "    if(fyg >=O.sy) {\n",
       "      fyg = 0;\n",
       "      fdg++;\n",
       "      if(fdg >= O.depth) {\n",
       "        fdg = 0;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "}\n",
       "\n",
       "iid = -1;\n",
       "function start() {\n",
       "  O = conv_forward(X, Ws, bs, S);\n",
       "  draw();\n",
       "  iid = setInterval(focusCell, 1000);\n",
       "}\n",
       "\n",
       "</script>\n",
       "\n",
       "\n",
       "\n",
       "</head>\n",
       "\n",
       "<body onload=\"start()\">\n",
       "\n",
       "<div id=\"draw\">\n",
       "</div>\n",
       "\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "HTML(filename=\"materials/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"images/L8_Filters.png\" width=\"900\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Convolutional Layer Summary</h3>\n",
    "\n",
    "- Accepts a volume of size $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "\n",
    "- Requires four hyperparameters:\n",
    "  - Number of **filters** $K$;\n",
    "  - Their **spatial** extent $F$;\n",
    "  - The **stride** $S$;\n",
    "  - The amount of **zero padding** $P$.\n",
    "\n",
    "\n",
    "- Produces a volume of size $W_2 \\times H_2 \\times D_2$ where:\n",
    "\n",
    "  $$W_2=\\frac{W_1−F+2P}{S+1},$$\n",
    "  \n",
    "  $$H_2=\\frac{H_1−F+2P}{S+1},$$\n",
    "  \n",
    "  $$D_2=K.$$\n",
    "  \n",
    "\n",
    "- With **parameter sharing**, it introduces $F \\cdot F \\cdot D_1$ **weights per filter**, for a total of $(F \\cdot F \\cdot D_1) \\cdot K$ **weights** and $K$ **biases**.\n",
    "\n",
    "\n",
    "- A common setting of the hyperparameters is $F=3$, $S=1$, $P=1$. \n",
    "\n",
    "  However, there are common conventions and rules of thumb that motivate these hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">You Remember RELU, Dont You?</h3>\n",
    "\n",
    "- The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain.\n",
    "    \n",
    "- ReLU stands for Rectified Linear Unit. \n",
    "    \n",
    "- The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 100 linearly spaced numbers\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "#define the function\n",
    "def relu_function(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "#define the f(x) values\n",
    "y = [relu_function(i) for i in x]\n",
    "\n",
    "# plot the function\n",
    "plt.plot(x,y, 'g')\n",
    "\n",
    "# setting the axes at the centre\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Pooling Layer</h3>\n",
    "\n",
    "- It is common to periodically insert a **Pooling layer** in-between successive Convolutional layers in a CNN architecture.\n",
    "\n",
    "\n",
    "- Its function is to **progressively reduce** the **spatial size** of the representation to **reduce** the **amount of parameters** and **computation in the network**, and hence to also **control overfitting**. \n",
    "\n",
    "\n",
    "- The **Pooling Layer** **operates independently** on **every depth slice** of the input and **resizes** it spatially, using the **MAX operation**.\n",
    "\n",
    "\n",
    "- The **Pooling layer**:\n",
    "\n",
    "  - **Accepts a volume of size** $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "  - Requires **two hyperparameters**:\n",
    "    \n",
    "    - Their **spatial** extent $F$;\n",
    "    - The **stride** $S$.\n",
    "    \n",
    "  - **Produces a volume of size** $W_2 \\times H_2 \\times D_2$ where:\n",
    "\n",
    "    $$W_2=\\frac{PW_1−F}{S+1},$$\n",
    "    \n",
    "    $$H_2=\\frac{H_1−F}{S+1},$$\n",
    "    \n",
    "    $$D_2= D+1$$\n",
    "    \n",
    "  - **Introduces zero parameters** since it computes a **fixed function of the input** For **Pooling layers**\n",
    "  \n",
    "  - It is **not common** to pad the input using **zero-padding**.\n",
    "  \n",
    "\n",
    "  <img src=\"images/L8_Pooling.jpeg\" width=\"800\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "  - However, **many people dislike** the pooling operation and think that we can get away without it. \n",
    "  \n",
    "    For example, **to reduce the size** of the representation one can **use larger stride** in CONV layer once in a while. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Fully-Connected Layer</h3>\n",
    "\n",
    "- Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. \n",
    "\n",
    "- Their **activations** can hence be **computed** with a **matrix multiplication** followed by a **bias offset**.\n",
    "\n",
    "\n",
    "- It's worth to emphesize that the **only difference** between **FC** and **CONV layers** is that the **neurons** in the **CONV layer are connected only to a local region** in the input, and that **many of the neurons** in a **CONV volume share parameters**.\n",
    "\n",
    "\n",
    "- Therefore, it turns out that it’s **possible** to **convert between FC and CONV layers**:\n",
    "\n",
    "\n",
    "- For any **CONV layer** there is an **FC layer** that **implements the same forward function**. \n",
    "  \n",
    "  The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).\n",
    "\n",
    "\n",
    "- Conversely, any **FC layer can be converted to a CONV layer**. \n",
    "\n",
    "  For example, an **FC layer** with $K=4096$ that is looking at some input volume of size $7 \\times 7 \\times 512$ can be equivalently expressed as a CONV layer with $F=7$, $P=0$, $S=1$, $K=4096$. \n",
    "    \n",
    "  **In other words**, we are **setting the filter size** to **be exactly the size of the input volume**.\n",
    "  \n",
    "  Hence the output will simply be $1 \\times 1 \\times 4096$ since only a single depth column **fits** across the input volume, giving identical result as the initial FC layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Commonly used CNN Architectures</h3>\n",
    "\n",
    "There are several architectures in the field of Convolutional Networks that have a name. \n",
    "\n",
    "\n",
    "The most common are:\n",
    "  \n",
    "- **LeNet**. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. \n",
    "\n",
    "  Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.\n",
    "  \n",
    "  \n",
    "- **AlexNet**. The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. \n",
    "\n",
    "  The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). \n",
    "  \n",
    "    The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n",
    "  \n",
    "  \n",
    "- **ZF Net**. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. \n",
    "\n",
    "  It became known as the ZFNet (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
    "  \n",
    "  \n",
    "- **GoogLeNet**. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. \n",
    "\n",
    "  Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). \n",
    "  \n",
    "  Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. \n",
    "    \n",
    "  There are also several followup versions to the GoogLeNet, most recently Inception-v4.\n",
    "\n",
    "\n",
    "- **VGGNet**. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. \n",
    "\n",
    "  Its main contribution was in showing that the depth of the network is a critical component for good performance. \n",
    "  \n",
    "  Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. \n",
    "  \n",
    "  Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). \n",
    "  \n",
    "  Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
    "\n",
    "\n",
    "- **ResNet**. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. \n",
    "\n",
    "  It features special skip connections and a heavy use of batch normalization. \n",
    "  \n",
    "  The architecture is also missing fully connected layers at the end of the network. \n",
    "  \n",
    "  The reader is also referred to Kaiming’s presentation (video, slides), and some recent experiments that reproduce these networks in Torch. \n",
    "  \n",
    "  ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). \n",
    "  \n",
    "  In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
